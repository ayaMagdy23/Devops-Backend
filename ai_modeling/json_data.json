[
{
  "Question": "How to resolve the error no module named pandas when one node (in Airflow's DAG) is successful in using it(pandas) and the other is not?\nI am unable to deduce as to why I am getting an error no module named pandas.\nI have checked via pip3 freeze and yes, the desired pandas version does show up.\nI have deployed this using docker on a kubernetes cluster.\n",
  "Answer": "Pandas is generally required, and sometimes used in some hooks to return dataframes. Well, it's possible that Airflow was installed with pip and not pip3 possibly being added as a Python 2 module and not a Python 3 module (though, using pip should have installed Pandas when one looks at the setup.py).\nWhich Operator in your DAG is giving this error?\nDo you have any PythonVirtualEnvironmentOperators or BashOperators running python from the command line (and thus possibly not sharing the same environment that you're checking has pandas)?\n"
},
{
  "Question": "I've been using the kubectl to upload Airflow workflows to the kubernetes (/usr/local/airflow/dags) manually. It is possible to do this without using the kubectl? by using a python script? or something else? If it's possible would you be able to share your source? or your python script? Thanks, Appreciate\n",
  "Answer": "This totally depends on your setup. E.G. We use AWS and so we have the DAGs syncing from an S3 bucket path every 5 minutes. We just put dags into S3. I see that some Kubernetes setups use a kind of shared volume defined by a git repository, that might also work. Airflow itself (the webserver(s), worker(s), nor scheduler) does not offer any hook to upload into the DAG directory.\n"
},
{
  "Question": "Is there currently a way to serve websockets from an application deployed on Okteto cloud given the okteto-specific limitations around Ingresses and Services ?\n\nI've read that this would only be possible using a Service or Ingress of type LoadBalancer, so that is what I've tried.\nBut, according to the Okteto docs, Services of type LoadBalancer (or NodePort) are managed. In practice they seem to get transformed automatically into a ClusterIP Service, + exposed to the internet on an automatic URL.\nDo these handle only HTTP requests ? Or is there a way to make them handle other kinds of connections based on TCP or UDP (like websockets) ?\n",
  "Answer": "You don't need a LoadBalancer to use WebSockets, they can be served from an Ingress with a ClusterIP as well (this is what Okteto Cloud uses for our endpoints). This setup supports HTTPS, WebSockets and even GRPC-based endpoints.\nThis sample shows you how to use WebSockets on a Node app deployed in Okteto Cloud, hope it helps! (it uses okteto-generated Kubernetes manifests, but you can also bring your own).\n"
},
{
  "Question": "Recently, I tried to setup Jenkins X on a kubernetes cluster. However there exists some problem during installation. \nThere are several options in jx create cluster such as aks(create with AKS), aws(create with AWS), minikube(create with Minikube) and etc.\nHowever there is no option which create a cluster with local kubernetes cluster. I want to setup Jenkins X with my own cluster. \nCan I get some advice?\nThanks.\n",
  "Answer": "when you have your cluster setup such that you can run kubectl commands against it, you can run jx boot to setup your jx installation. You don't need to use jx create cluster as your cluster already exists. \n"
},
{
  "Question": "The data needs to be loaded periodically (like once in a day), and it should be stored in SQL format so that the API can run SQL queries.\nWe are thinking of loading it from HDFS. Currently we are thinking of using Apache Nifi using PutIgniteCache. \nI was thinking probably I can launch a remote Ignite client node and then use IgniteDataStreamer to stream the data, but I was not able to find proper documentation for that. Are there any better ways to do this?\n",
  "Answer": "The documentation for Nifi says that it uses the data streamer API, so unless you need more control it doesn\u2019t seem like a bad option (with the caveat that I\u2019d never heard of Nifi before much less used it!).\n"
},
{
  "Question": "Trying Ignite with Kubertenes deployment options. \n\nIs it zone-aware? Cannot find any docs & configuration about this.\nCan I connect the Kubernetes cluster via an external client? I'd like to connect via C# client.\n\nThank you!\n",
  "Answer": "\nBroadly speaking, Ignite assumes all your nodes are close to one another. You could make it aware of multiple zones by using custom affinity functions, but it's not straight-forward. There are also third-party solutions (I work for GridGain who provide one) that support data centre replication, etc. that would work in this situation\nYes. I would suggest that thick clients should be part of your Kubernetes infrastructure (since they become part of the Ignite cluster topology), but you can connect a thin client just by opening the right ports. You can use Kubernetes load balancing / round robin to connect to any node\n\n"
},
{
  "Question": "Are  there any know issues with running the org.apache.ignite.spi.discovery.tcp.ipfinder.kubernetes.TcpDiscoveryKubernetesIpFinder a purely IPv6 environment? I looked here and it mentions there may be issues with clusters becoming detached but does not offer any specifics.  Any information would be appreciated, thanks.\n",
  "Answer": "I'm not aware of any IPv6 problems per se, so if your network is configured correctly I would expect it to work.\nThe problem we typically see when IPv6 is enabled is that it's possible to route to the IPv4 address but not the IPv6 address -- which is why setting preferIPv4Stack works.\n"
},
{
  "Question": "I have a Kubernetes setup of 7 Apache Ignite servers and over 100 clients.\nWith my current Apache Ignite configuration, I am seeing the following line of log for the servers:\njava.lang.OutOfMemoryError: Java heap space\n\nBelow is the memory configuration Apache Ignite server:\n\nPod memory limit: 2Gb\nXmx: 768m\n\nI would like to know what should be the optimum Memory configuration for the Apache Ignite cluster\n",
  "Answer": "It depends on what you're trying to do -- persistence and SQL tend to use more heap space for example -- but both 2Gb and 768Mb are much smaller than I'd expect for an in-memory database.\nThe tuning guide suggests 10Gb as a starting point:\n-server\n-Xms10g\n-Xmx10g\n-XX:+AlwaysPreTouch\n-XX:+UseG1GC\n-XX:+ScavengeBeforeFullGC\n-XX:+DisableExplicitGC\n\n"
},
{
  "Question": "Is there a way to connect C# Thick Client running in the Windows Machine outside of the Kubernetes with Apache Ignite Cluster nodes are present in the Kubernetes.\nBelow specified article says it is not possible but this article was written in 2020. We are looking for Scenario-3 from below article\nhttps://dzone.com/articles/apache-ignite-on-kubernetes-things-to-know-about\nI hope there might some enhancements for Scenario-3.\nWe dont want convert our C# Thick client to Thin Client as we are using Data Streamer to insert data in bulk and same functionality is not available with Thin Client.\n",
  "Answer": "The recommendation here would be to use the thin-client. The .net thin-client does have the data streamer API.\nThere is no straight-forward way to connect a thick-client node from outside Kubernetes to a cluster inside it.\n"
},
{
  "Question": "I'm working locally (within Docker for Mac) on a Kubernetes cluster that will eventually be deployed to the cloud. We plan to use a database service in that environment. To simulate that, I'd like to have the services in the cluster connect to a database running outside the cluster on my laptop.\nCan I do that? Here's what I thought I'd try.\n\nDefine a Service with type: ExternalName and externalName: somedb.local\nAdd 127.0.0.1 somedb.local to /etc/hosts on the laptop\n\nIs that correct? Is there a better way?\n",
  "Answer": "After talking with some colleagues, I found a solution.\nIn Docker for Mac, host.docker.internal points to the host machine, and that lets me connect to the db running there, even from containers running in the K8s cluster.\n"
},
{
  "Question": "I have a hard requirement to use a single ELB Classic (CLB) load balancer. Can a single ELB Classic (CLB) distribute traffic between two different Auto Scaling Groups, both running the same application code with no special path based routing needed from an ALB (Application Load Balancer).\nFor example, in a high availability (HA) cluster set-up with KOPS, how does KOPS make it possible to use a single ELB Classic load balancer (as an entry point to the API server) to serve traffic to two different Auto Scaling Groups in different Availability Zones (AZs) each with their own master instances?\nThanks in advance.\n",
  "Answer": "A single classis ELB cannot have multiple ASGs associated with it, but the newer Application Load Balancer can do this.\n"
},
{
  "Question": "I am currently using Kubernetes Python SDK to fetch relevant information from my k8s cluster. I am running this from outside the cluster.\nI have a requirement of fetching the images of all the POD's running within a namespace. I did look at Docker python SDK but that requires me to be running the script on the cluster itself which i want to avoid.\nIs there a way to get this done ?\nTIA\n",
  "Answer": "\nthat requires me to be running the script on the cluster itself\n\nNo, it should not: the kubernetes-client python performs operations similar to kubectl calls (as detailed here).\nAnd kubectl calls can be done from any client with a properly set .kube/config file.\nOnce you get the image name from a kubectl describe po/mypod, you might need to docker pull that image locally if you want more (like a docker history).\nThe OP Raks adds in the comments:\n\nI wanted to know if there is a python client API that actually gives me an option to do docker pull/save/load of an image as such\n\nThe docker-py library can pull/load/save images.\n"
},
{
  "Question": "I've placed a docker compose file project.yaml at the location /etc/project/project.yaml\nthe file and well as the project directory have the same file permission, i.e. -rxwrxxrwx\nbut when I run docker-compose\nsudo docker-compose -f ./project.yaml up -d\n\nif errors out with the following\nCannot find the file ./project.yaml\nI have checked various times and it seems there is no permission issue. Can anyone tell why we have this problem and what would be the solution\n",
  "Answer": "Beside using the full path, as commented by quoc9x, double-check your current working directory when you call a command with a relative path ./project.yaml\nIf you are not in the right folder, that would explain the error message.\n"
},
{
  "Question": "I am not able to attach to a container in a pod. Receiving below message\nError from server (Forbidden): pods \"sleep-76df4f989c-mqvnb\" is forbidden: cannot exec into or attach to a privileged container\nCould someone please let me what i am missing?\n",
  "Answer": "This seems to be a permission (possibly RBAC) issue.\nSee Kubernetes pod security-policy.\nFor instance gluster/gluster-kubernetes issue 432 points to Azure PR 1961, which disable the  cluster-admin rights (although you can customize/override the admission-controller flags passed to the API server).\nSo it depends on the nature of your Kubernetes environment.\n"
},
{
  "Question": "I have downloaded the kubernetes from GitHub and now I want to run it from the downloaded file not from GitHub. Could you please help me? \n",
  "Answer": "You can follow:\n\n\"Assign Memory Resources to Containers and Pods\"\n\"Assign CPU Resources to Containers and Pods\"\n\nThat is:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: cpu-demo\n  namespace: cpu-example\nspec:\n  containers:\n  - name: demo-ctr\n    image: vish/stress\n    resources:\n      limits:\n        cpu: \"1\"\n        memory: \"200Mi\"\n      requests:\n        cpu: \"0.5\"\n        memory: \"100Mi\"\n    args:\n    - -cpus\n    - \"2\"\n\nAt the pod level: \"Configure a Pod Quota for a Namespace\".\n"
},
{
  "Question": "I am trying to run a kubernetes cluster on mac os for some prototyping using docker (not vagrant or virtualbox). \nI found the instructions online at https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/getting-started-guides/docker.md but the instructions are 3 years old (Oct 2015).\nThe instructions refer to boot2docker but the present version of docker on mac (Docker Community Edition v 18.06.1-ce-mac73) doesn't have boot2docker. \nCan you point me to the latest instructions?\n",
  "Answer": "Since 2015, everything has been move to the Kubernetes website GitHub repo.\nThe full installation/process page is now at kubernetes.io/docs/tasks/.\nAnd since issue 7307, a Kubernetes installation on MacOs would no longer use xHyve, but, as stated in the documentation:\n\nmacOS: VirtualBox or VMware Fusion, or HyperKit.\n\n"
},
{
  "Question": "I am serving jupyter notebook through a Kubernetes cluster. And I've set resources.limits to prevent someone from draining all of the host servers memory. \nWhile one problem is that the jupyter notebook kernels after crash and automatic restart they do not throw any OOM errors after the container exceeds the memory, which will make the user very confused. \nSo how can I make the jupyter notebook raise the OOM error when running with Kubernetes? \n",
  "Answer": "If you have only one specific pod, you can monitor the events/logs, as in here:\nkubectl get events --watch \nkubectl logs -f podname\n\nThat being said, not all events in a pod lifecycle are properly reported, as shown in kubernetes/kubernetes issue 38532 and the (abandoned) PR 45682.\nBut you should still see  OOMKilled:true when docker inspect'ing the pod.\n"
},
{
  "Question": "I know continuous deployment can be done via spinnaker like Blue/Green, Canary Deployment etc \nbut my question how does it differ from other GitOps tools like WeaveWorks Flux ? \nDoes both solve the same problem ? or Do we need both tools in conjunction ? \n",
  "Answer": "As mentioned in \"The GitOps Pipeline - Part 2\"\n\nWeave Flux is a tool that automates the staging and release of containers to Kubernetes, as well as services, deployments, network policies and even Istio routing rules. Flux is therefore \u2018glue\u2019 that maps all the relationships between code, services and running clusters.\nIt can be seen as a leaner, Kubernetes-native alternative to Spinnaker.\n\n"
},
{
  "Question": "If there is an update in the docker image, rolling update strategy will update all the pods one by one in a daemonset, similarly is it possible to restart the pods gracefully without any changes the daemonset config or can it be triggered explicitly?\nCurrently, I am doing it manually by\nkubectl delete pod <pod-name>\nOne by one until each pod gets into running state.\n",
  "Answer": "You could try and use Node maintenance operations:\n\nUse kubectl drain to gracefully terminate all pods on the node while marking the node as unschedulable (with --ignore-daemonsets, from Konstantin Vustin's comment):\n\nkubectl drain $NODENAME --ignore-daemonsets\n\n\nThis keeps new pods from landing on the node while you are trying to get them off.\n\nThen:\n\nMake the node schedulable again:\n\nkubectl uncordon $NODENAME\n\n"
},
{
  "Question": "I am able to install kubernetes using kubeadm method successfully. My environment is behind a proxy. I applied proxy to system, docker and I am able to pull images from Docker Hub without any issues. But at the last step where we have to install the pod network (like weave or flannel), its not able to connect via proxy. It gives a time out error. I am just checking to know if there is any command like curl -x http:// command for kubectl apply -f? Until I perform this step it says the master is NotReady.\n",
  "Answer": "When you do work with a proxy for internet access, do not forget to configure the NO_PROXY environment variable, in addition of HTTP(S)_PROXY.\nSee this example:\n\nNO_PROXY accepts a comma-separated list of hosts, IP addresses, or IP ranges in CIDR format:\nFor master hosts\n\nNode host name\nMaster IP or host name\n\nFor node hosts\n\nMaster IP or host name\n\nFor the Docker service\n\nRegistry service IP and host name\n\n\nSee also for instance weaveworks/scope issue 2246.\n"
},
{
  "Question": "Here's the full error Unable to connect to the server: dial tcp [::1]:8080: connectex: No connection could be made because the target machine actively refused it.\nHere's my Kubectl config view\napiVersion: v1\nclusters: []\ncontexts:\n- context:\n    cluster: \"\"\n    user: \"\"\n  name: dev\ncurrent-context: dev\nkind: Config\npreferences: {}\nusers: []\n\nI'm running Minikube start. It's stuck on Starting VM...\nIn Hyper-V Manager, I have minikube VM running. \n",
  "Answer": "Check out \"Minikube on Windows 10 with Hyper-V\" by Jock Reed\nThe command to run, from a Windows CMD console as Administrator, is:\nminikube start --vm-driver hyperv --hyperv-virtual-switch \"Primary Virtual Switch\"\n\nWith \"Primary Virtual Switch\" being the name of the new \"External\" \"Virtual network switch\" you have created first.\nDon't forget to turn off Dynamic Memory for the minikube VM (minikube issue 2326)\nAnd possibly, disable IPv6  on Network Adapter Windows 10 (issue 754\nMake sure to use the v0.28.0/minikube-windows-amd64 executable, as mentioned in issue 1943.\n"
},
{
  "Question": "I need an Akka cluster to run multiple CPU intensive jobs. I cannot predict how much CPU power I need. Sometimes load is high, while at other times, there isn't much load. I guess autoscaling is a good option, which means, example: I should be able to specify that I need minimum 2 and maximum 10 Actors. The cluster should scale up or down along with a cool off period as load goes up or down. Is there a way to do that? \nI am guessing, maybe one can make an Docker image of the codebase, and autoscale it using Kubernetes. Is it possible? Is there a native Akka solution?\nThanks\n",
  "Answer": "If you consider a project like hseeberger/constructr and its issue 179, a native Akka solution should be based on akka/akka-management:\n\nThis repository contains interfaces to inspect, interact and manage various Parts of Akka, primarily Akka Cluster. Future additions may extend these concepts to other parts of Akka.\n\nThere is a demo for kubernetes.\n"
},
{
  "Question": "I want my pods to be gracefully recycled from my deployments after certain period of time such as every week or month. I know I can add a cron job for that if I know the Kubernetes command. \nThe question is what is the best approach to do this in Kubernetes. Which command will let me achieve this goal?\nThank you very much for helping me out on this.\n",
  "Answer": "As the OP rayhan has found out, and as commented in kubernetes/kubernetes issue 13488, a kubectl patch of an environment variable is enough.\nBut... K8s 1.15 will bring kubectl rollout restart... that is when PR 77423 is accepted and merged.\n\nkubectl rollout restart now works for daemonsets and statefulsets.\n\n"
},
{
  "Question": "I'm looking to find a way to write a scaler for my application which is running on Minikube to scale it up and down based on Time Stamps. Any idea?\n",
  "Answer": "That would be an Horizontal Pod Autoscaler (see its Walkthrough here), which would  automatically scales the number of pods in a replication controller, deployment or replica set based on observed CPU utilization (or, with custom metrics support, on some other application-provided metrics)\nIn your case, the custom metric would be the time.\nYou can then follow \"Kubernetes: Horizontal Pod Autoscaler using Minikube\" from Tommy Elmesewdy as a practical example to implement one such autoscaler on Minikube.\nIn your case, you should ensure custom metrics are enabled:\nminikube start --extra-config kubelet.EnableCustomMetrics=true\n\n"
},
{
  "Question": "I checked the pods in the kube-system namespace and noticed that some pods share the same ip address.The pods that share the same ip address appear to be on the same node.\n \nIn the Kubernetes documenatation it said that \"Evert pod gets its own ip address.\" (https://kubernetes.io/docs/concepts/cluster-administration/networking/). I'm confused as to how same ip for some pods came about.\n",
  "Answer": "This was reported in issue 51322 and can depend on the network plugin you are using.\nThe issue was seen when using the basic kubenet network plugin on Linux.\nSometime, a reset/reboot can help\n\nI suspect nodes have been configured with overlapped podCIDRs for such cases.\n  The pod CIDR could be checked by kubectl get node -o jsonpath='{.items[*].spec.podCIDR}'\n\n"
},
{
  "Question": "so basically i changed my website to a new kubernetes cluster. Now it is necessary to enable RBAC. The pipeline runs without any errors but unfortunately the certmanager SSH doesn't work anymore. I installed certmanager with gitlab so now I'm wondering if this could have anything to do with the change to RBAC? Unfortunetally i'm new with RBAC so I don't really understand if it could be related or not. Can anybody help?\nHere is a picture from the error when I run whynopadlock: \n\n\nEdit: This is the output when I run:\nkubectl auth can-i --list --as=system:serviceaccount:gitlab-managed-apps:default\n\n\n",
  "Answer": "Deploy cert-manager to managed cluster for SSL certificates is an issue requested for the past two years (issue 40635)\nIssue 29650 shows the all process is not yet well documented.\nBut you still have an ingress working with cert manager, meaning having the annotations expected by a cert manager:\n\nkubernetes.io/ingress.class: nginx\nkubernetes.io/tls-acme: \"true\" \n\nThat could help with RBAC, even though issue 33186 shows this setup as not fully working.\n"
},
{
  "Question": "I want to create a deployment with GitHub URL using\nkubectl create -f github.com/deployment.yaml\nThat deployment.yaml file is located in  my private GitHub repository.\nHow can I authenticate my kubectl to use my GitHub private repo and create that deployment?\n",
  "Answer": "You could simply:\n\ncurl the file from the private repo\npass its content to kubectl create -f\n\nThat is, from this example:\nUSER=\"me\"\nPASSWD=\"mypasswd\"\nOUTPUT_FILEPATH=\"./foo\"\nOWNER=\"mycompany\"\nREPOSITORY=\"boo\"\nRESOURCE_PATH=\"project-x/a/b/c.py\"\nTAG=\"my_unit_test\"\ncurl \\\n    -u \"$USER:$PASSWD\" \\\n    -H 'Accept: application/vnd.github.v4.raw' \\\n    -o \"$OUTPUT_FILEPATH\" \\\n    -L \"https://api.github.com/repos/$OWNER/$REPOSITORY/contents/$RESOURCE_PATH?ref=$TAG\"\n| kubectl create -f /dev/stdin\n\n"
},
{
  "Question": "I am using cloud build and GKE k8s cluster and i have setup CI/CD from github to cloud build.\nI want to know is it good to add CI build file and Dockerfile in the repository or manage config file separately in another repository? \nIs it good to add Ci & k8s config files with business logic repository?\nWhat is best way to implement CI/CD cloud build to GKE with managing CI/k8 yaml files\n",
  "Answer": "Yes, you can add deployment directives, typically in a dedicated folder of your project, which can in turn use a cicd repository\nSee \"kelseyhightower/pipeline-application\" as an example, where:\n\nChanges pushed to any branch except master should trigger the following actions:\n\nbuild a container image tagged with the build ID suitable for deploying to a staging cluster\nclone the pipeline-infrastructure-staging repo\npatch the pipeline deployment configuration file with the staging container image and commit the changes to the pipeline-infrastructure-staging repo\n\nThe pipeline-infrastructure-staging repo will deploy any updates committed to the master branch.\n\n"
},
{
  "Question": "I have an app running with Docker and the .git directory is ignored to reduced the project's size.\nThe problem is that every time an artisan command is ran this message is displayed and stored inside the logs of Kubernetes. Moreover, in some cases is the reason why some kubernetes tasks cannot reach the HEALTHY status.\nI have a Cronjob with kubernetes which reaches just 2/3 and the only message that the logs displayed is this one.\n",
  "Answer": "monicahq/monica PR 950 is an example of a workaround where the Sentry configuration is modified to test for the presence of the Git repository, ensuring php artisan config:cache is run only once.\n// capture release as git sha\n'release' => is_dir(__DIR__.'/../.git') ? trim(exec('git log --pretty=\"%h\" -n1 HEAD')) : null,\n\n"
},
{
  "Question": "I am tasked with building a new relic chart to show gitlab runner job count over time.\nI am trying to determine what type of object is a gitlab runner job. Is is a deployment, a pod or a statefulset?\nAny assistance with being able to visualize gitlab runner pods in new relic would be appreciated.\n",
  "Answer": "As mentioned in \"Kubernetes executor for GitLab Runner\":\n\nThe Kubernetes executor, when used with GitLab CI, connects to the Kubernetes API in the cluster creating a Pod for each GitLab CI Job.\nThis Pod is made up of, at the very least:\n\na build container,\na helper container, and\nan additional container for each service defined in the .gitlab-ci.yml or config.toml files.\n\n\nSince those are pods, you should see them in a NewRelic Kubernetes cluster explorer.\n"
},
{
  "Question": "Are the resources released once the kube job is finished?\nI mean the associated pod resources, let say a job is run to completion and the associated pod is in a completed state, which was allocated 0.5 CPU, is the 0.5cpu released after the job is finished?\nor is it released only after deleting the job?\n",
  "Answer": "A pod phase does not include \"completed\": you only have \"Succeeded\":\n\nAll Containers in the Pod have terminated in success, and will not be restarted.\"\n\nSince this is a terminal state, its reserved resources won't be taken into consideration by the scheduler.\n"
},
{
  "Question": "I am creating k8s cluster from digital ocean but every time I am getting same warning after I create cluster and open that cluster in lens ID.\nHere is the screenshot of warning:\n\ni did every soltion which i found but still can't able to remove the error. \n\n",
  "Answer": "Check first if k3s-io/k3s issue 1857 could help:\n\nI was getting the same error when I installed kubernetes cluster via kubeadm.\nAfter reading all the comments on the subject, I thought that the problem might be caused by containerd and the following two commands solved my problem, maybe it can help:\nsystemctl restart containerd\nsystemctl restart kubelet\n\n\nAnd:\n\nThis will need to be fixed upstream. I suspect it will be fixed when we upgrade to containerd v1.6 with the cri-api v1 changes\n\nSo checking the containerd version can be a clue.\n"
},
{
  "Question": "I want to access my Kubernetes cluster API in Go to run kubectl command to get available namespaces in my k8s cluster which is running on google cloud.\nMy sole purpose is to get namespaces available in my cluster by running kubectl command: kindly let me know if there is any alternative.\n",
  "Answer": "You can start with kubernetes/client-go, the Go client for Kubernetes, made for talking to a kubernetes cluster. (not through kubectl though: directly through the Kubernetes API)\nIt includes a NamespaceLister, which helps list Namespaces.\nSee \"Building stuff with the Kubernetes API \u2014\u200aUsing Go\" from Vladimir Vivien\n\nMichael Hausenblas (Developer Advocate at Red Hat) proposes in the comments documentations with using-client-go.cloudnative.sh\n\nA versioned collection of snippets showing how to use client-go. \n\n"
},
{
  "Question": "I'm trying to start minikube on ubuntu 18.04 inside nginx proxy manager docker network in order to setup some kubernetes services and manage the domain names and the proxy hosts in the nginx proxy manager platform.\nso I have nginxproxymanager_default docker network and when I run minikube start --network=nginxproxymanager_default I get\n\nExiting due to GUEST_PROVISION: Failed to start host: can't create with that IP, address already in use\n\nwhat might I been doing wrong?\n",
  "Answer": "A similar error was reported with kubernetes/minikube issue 12894\n\nplease check whether there are other services using that IP address, and try starting minikube again.\n\nConsidering minikube start man page\n\n--network string\nnetwork to run minikube with.\nNow it is used by docker/podman and KVM drivers.\nIf left empty, minikube will create a new network.\n\nUsing an existing NGiNX network (as opposed to docker/podman) might not be supported.\nI have seen NGiNX set up as ingress, not directly as \"network\".\n"
},
{
  "Question": "When I try to run a container in the cluster, I get a message \"deployment test created\" but when I look at the dashboard I can see that its in an error state (Failed to pull image..., it was not able to pull the image from the local minikube docker env due to authorization issues\nMy steps were:\n\nStart minikube using hyperv and setting the --insecure-registry switch to 10.0.0.0/8, also tried 0.0.0.0/0 - Kubernetes version 1.9.0 also tried 1.8.0\nSet the docker env to the minikube docker via minikube docker-env | Invoke-Expression\nbuild docker image - image builds and exists in minikube local docker\nkubectl run test --image test-service --port 1101\n\nThis is the result:\n\nWhat am I missing?\n",
  "Answer": "As discussed in the comments, openfaas/faas-netes issue 135 illustrates a similar issue, and mention as a possible solution:\n\nimagePullPolicy if not mentioned should have defaulted to Never instead of Always.\n\nThe OP Tim Jarvis realized then:\n\nI realized it was not an auth issue, but that it was always wanting to pull from an external repo.\n  The fix for me was to use the imagePullPolicy of IfNotPresent.\n\n"
},
{
  "Question": "I have installed a K8S cluster on laptop using Kubeadm and VirtualBox. It seems a bit odd that the cluster has to be up and running to see the documentation as shown below.\npraveensripati@praveen-ubuntu:~$ kubectl explain pods\nUnable to connect to the server: dial tcp 192.168.0.31:6443: connect: no route to host\n\nAny workaround for this?\n",
  "Answer": "See \"kubectl explain\u200a\u2014\u200a#HeptioProTip\"\n\nBehind the scenes, kubectl just made an API request to my Kubernetes cluster, grabbed the current Swagger documentation of the API version running in the cluster, and output the documentation and object types.\n\nTry kubectl help as an offline alternative, but that won't be as complete (limite to kubectl itself).\n"
},
{
  "Question": "I am using launcher.gcr.io/google/jenkins2 to run jenkins in gcp kubernetes engine.\nEverything seems ok except that I get Could not initialize class org.jfree.chart.JFreeChart error for every chart that jenkins attempt to draw. I googled the error and almost everyone solves that with adding -Djava.awt.headless=true. As you can guess I already tried that and it does not work.\nIdeas?\n",
  "Answer": "One other possible solution/workaround is seen in JENKINS issue 39636:\n\nI installed libxext6 libxrender1 fontconfig libfontconfig but it didn't help. After that I also installed libjfreechart-java but I still have the same problem.\nCommenting out the assistive_technologies line in /etc/java-8-openjdk/accessibility.properties solved it.\n\nYou can see that recommendation in tianon's  comment of that answer:\n\nIn my case it ended up being bugs.debian.org/798794 (from \"Assistive technology not found error while building aprof-plot\").\nAdding \"RUN sed -i 's/^assistive_technologies=/#&/' /etc/java-8-openjdk/accessibility.properties\" to my Dockerfile fixed it. :)\n\n"
},
{
  "Question": "For example, a defer function is implemented within a webhook's logic (the webhook is written in Golang). The webhook times out after taking too long to complete. In this case, will the defer function still get executed?\nThe request timeout is set via context.WithTimeout for the webhook.\n",
  "Answer": "It's usually a good idea to clearly mention what programming language you're talking about, although I can figure it out.\nIn Go, the defer functor (whatever you want to call it) will be executed when the method returns, no matter how it completes, unless you manage to kill the executing process in a drastic manner, and perhaps even then.\n"
},
{
  "Question": "I understand that blue-green deployment means having two versions of production code deployed. It allows us to test in production.\nAssume that we have an e-commerce application, and both deployments are sharing a database.\nThen someone testing the green deployment makes a purchase. This purchase will be recorded in the production database even though it was just a test.\nHow do you deal with this scenario? How do blue-green deployments handle database interactions in general?\n",
  "Answer": "I'm not sure what resources you're looking at, but at least in our organization, using Kubernetes, blue-green deployments are not used for \"canary testing\", which is what you're referring to.  Blue-green deployments are used to facilitate smooth deployment switches.  During the process of deployment, the existing pods in the \"current color\" are still running while the pods in the \"target color\" are starting up and reaching readiness state.  The service doesn't switch over to the target color until the new pods in the target color are all ready.\nRegarding how you would do that sort of thing with \"canary testing\", I'd say that is completely application-specific.\n"
},
{
  "Question": "Has someone experience in debugging .NET 6 F# code running in a service-less deployment/pod inside a kubernetes cluster in AKS with Visual Studio (ideally 2022)?\nBridge to Kubernetes is not available for VS 2022, and the VS2019 (and VS code) seems to require a service and an http access. In my case, I have microservices that only use the internal cluster networking between them with a messaging engine, without associated services.\nLogs are helpful, but being able to debug would be great.\nIdeas?\nP.S.\nThis is a way, but it looks way too invasive\n",
  "Answer": "My experience with this sort of thing is with Java applications, but I assume it would be similar for this platform. This would typically be done with a \"port-forward\", described on this page: https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/ .\nBasically, this will provision a local port on your desktop, such that if you connect to that port on your desktop, it will actually be connecting to a mapped port on the pod in the cluster.\nOnce the port-forward is working, you can connect to that port from vscode.\nIn Java, this port would be the \"debugger port\", sometimes referred to as the JPDA port.\n"
},
{
  "Question": "How do I get the current namespace of a deployment/service using the kubernetes client-go API? It doesn't seem to be in the client object or in the config.\n",
  "Answer": "Using\nioutil.ReadFile(\"/var/run/secrets/kubernetes.io/serviceaccount/namespace\")\n\nworks but is ugly, when the desired implementation is present in the Namespace() method of inClusterClientConfig. But how would you get that object starting from  rest.InClusterConfig()? It is only instantiable from outside the package via NewNonInteractiveDeferredLoadingClientConfig.\nI see kubernetes #63707 which looks related but was abandoned.\n"
},
{
  "Question": "I need to move my filebeat to other namespace, but I must keep registry , I mean that:\n # data folder stores a registry of read status for all files, so we don't send everything again on a Filebeat pod restart\n      - name: data\n        hostPath:\n          path: /var/lib/filebeat-data\n          type: DirectoryOrCreate\n\nCan you tell me how can I copy that in kubernetes\n",
  "Answer": "Just to check my assumptions:\n\nfilebeat is a a DaemonSet\nWhen you start it up in the new Namespace, you want to keep the registry\nYou're happy to keep the on-disk path the same\n\nBecause the data folder is mounted from the host directly - if you apply the same DaemonSet in a new Namespace, it will mount the same location into the container. So there's no need to copy any files around.\n"
},
{
  "Question": "I have a microservice deployed in a Tomcat container/pod. There are four different files generated in the container - access.log, tomcat.log, catalina.out and application.log (log4j output). What is the best approach to send these logs to Elasticsearch (or similar platform). \nI read through the information on this page Logging Architecture - Kubernetes 5. Is \u201cSidecar container with a logging agent\u201d the best option for my use case?\nIs it possible to fetch pod labels (e.g.: version) and add it to each line? If it is doable, use a logging agent like fluentd? (I just want to know the direction I should take).\n",
  "Answer": "Yes, the best option for your use case is to have to have one tail -f sidecar per log file and then install either a fluentd or a fluent-bit daemonset that will handle shipping and enriching the log events.  \nThe fluentd elasticsearch cluster addon is available at that link.  It will install a fluentd daemonset and a minimal ES cluster.  The ES cluster is not production ready so please see the README for details on what must be changed.\n"
},
{
  "Question": "In my Kubernetes Service, running on OpenShift, I have an annotation like this:\n  annotations:\n    service.beta.openshift.io/serving-cert-secret-name: \"...\"\n\nwhich works fine on OpenShift 4.x.\nHowever I also want to support OpenShift 3.11, which requires the similar annotation (note alpha):\nservice.alpha.openshift.io/serving-cert-secret-name: \"...\"\n\nCan I just include both annotations in my yaml file in order to support both versions? In other words will OpenShift 4.x ignore the alpha annotation; and will OpenShift 3.11 ignore the beta annotation?\n",
  "Answer": "Yes\nThis is a common pattern for alpha/beta annotation migrations in the Kubernetes ecosystem, the controllers will only be looking for their specific annotation, any the controller doesn't recognise will be ignored.\nIf a controller is written to be backwards-compatible, they will normally look for the new beta annotation, and only if not finding it respect the alpha one.\n"
},
{
  "Question": "I'm setting up a Kubeflow cluster on AWS EKS, is there a native way in Kubeflow that allows us to automatically schedule jobs i.e. (Run the workflow every X hours, get data every X hours, etc.)\nI have tried to look for other things like Airflow, but i'm not really sure if it will integrate well with the Kubeflow environment.\n",
  "Answer": "That should be what a recurring run is for.\nThat would be using a run trigger, which does have a cron field, for specifying cron semantics for scheduling runs.\n"
},
{
  "Question": "I've create a private repo on docker hub and trying to pull that image into my kubernetes cluster. I can see the documentations suggest to do this\n\nkubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=<path/to/.docker/config.json> \\\n    --type=kubernetes.io/dockerconfigjson\n\nI am already logged in, and i change path to ~/.docker/config.json but it keeps giving me \nerror: error reading ~./docker/config.json: no such file or directory \ndespite the fact if i type cat ~/.docker/config.json it displays the content, meaning there is a file.\nSo in other words how to properly authenticate and be able to push private images into kube cluster?\n",
  "Answer": "\nerror: error reading ~./docker/config.json: no such file or directory\n                     ^^^^ ?\n\n\n~./docker/config.json does not seem valid:\n~/.docker/config.json would\nTo remove any doubt, try the full path instead of ~:\nkubectl create secret generic regcred \\\n    --from-file=.dockerconfigjson=/home/auser/.docker/config.json \\\n    --type=kubernetes.io/dockerconfigjson\n\n"
},
{
  "Question": "I would like to install Istio into my Kubernetes cluster.  The installation page says to:\n\ndownload the installation file for your OS\n\nMy developer machine is a Windows 10 machine.  The nodes in my Kubernetes cluster run CentOS.\nWhen it says \"Your OS\" does it mean my machine that I will download it to and run it from, or does it mean the OS that my cluster runs on?  (or do I need to run it from a node in the cluster?)\n",
  "Answer": "The download basically has istioctl and some samples in it.\nSo you want to download for the OS that you are running the command from (in my case Windows 10).\n"
},
{
  "Question": "I have an existing POD containing a DB. I have a script containing executable queries in that container. I need to schedule the execution of the script. How do I go about doing this?\n",
  "Answer": "OpenShift has a \"cronjob\" resource type which can schedule a job to run at specific intervals. You can read more about it here.\nYou can create a custom image which contains the client to connect to your DB and supply it with the credentials mapped as secrets. This can run your executable queries at the interval you've set for the job.\n"
},
{
  "Question": "I have k8s cluster with three workers and when I explicitly creates the pod, corresponding docker images get downloaded to worker. When I explicitly deletes the pods corresponding docker images are still present on worker nodes. Is this expected case?  \n",
  "Answer": "\nIs this expected case?\n\nPossibly, considering the Kubernetes Garbage collection policy for images:\n\nKubernetes manages lifecycle of all images through imageManager, with the cooperation of cadvisor.\nThe policy for garbage collecting images takes two factors into consideration: HighThresholdPercent and LowThresholdPercent.\nDisk usage above the high threshold will trigger garbage collection.\nThe garbage collection will delete least recently used images until the low threshold has been met.\n\n"
},
{
  "Question": "I have a use case in which I need to run kubectl command in crontab. I am getting empty response from all kubectl commands\nI have tried this\n#!/bin/sh\n\n/usr/bin/kubectl get po >> /cron.txt\n\nHelp will be appreciated!\n",
  "Answer": "I don't know why you're not getting any output, but I think I know why it's not telling you why it's not getting output.\nYour command line is only going to get stdout, and drop stderr on the floor. It's likely that stderr will tell you why it's not working.\nTry changing the command line to:\n/usr/bin/kubectl get po 2>&1 >> /cron.txt\n\n"
},
{
  "Question": "I see there are many Github pages for gradle kubernetes plugin like \nhttps://github.com/bmuschko/gradle-kubernetes-plugin\n\nhttps://github.com/kolleroot/gradle-kubernetes-plugin\n\nhttps://github.com/qaware/gradle-cloud-deployer\nNone of these having any concrete example how to connect to kubernetes from gradle and create a new deployment and service  I tried all about git link in gradle but no luck...\n",
  "Answer": "Since I also faced a lack of plugins that deal with Kubernetes I started working on a Gradle plugin to make deploying resources to a Kubernetes cluster easier: https://github.com/kuberig-io/kuberig.\nIn the user manual you will find details about how to connect to a kubernetes cluster here:\nhttps://kuberig-io.github.io/kuberig/#/initializing-an-environment\nIt also includes an example of how to define a deployment here: https://kuberig-io.github.io/kuberig/#/defining-a-deployment\nAnd a service here:\nhttps://kuberig-io.github.io/kuberig/#/defining-a-service\nIt may also be useful to go through the quickstart first https://kuberig-io.github.io/kuberig/#/quick-start.\nHope it can be of use to you.\n"
},
{
  "Question": "I have a pod that needs to clean up an external reference when it is terminated.  (I can do this with a a curl command.)\nI looked into container lifecycle events and they seem just what I need.\nBut the example shows creating a Pod resource directly via yaml.  With Helm, I just made deployments and the pods are auto created.\nHow can I define a PreStop container lifecycle hook in a Kubernetes deployment?\n",
  "Answer": "I should have looked a bit longer.\nThe Writing a Deployment Spec section of the deployment documentation says:\n\nThe .spec.template is a Pod template. It has exactly the same schema as a Pod, except it is nested and does not have an apiVersion or kind.\n\nSo I can just add my hook in there as if it were the Pod yaml.\n"
},
{
  "Question": "I have a deployment (starterservice) that deploys a single pod with a persistent volume claim. This works. However restart fails:\nkubectl rollout restart deploy starterservice\n\nThe new pod is started before the old one has terminated and it cannot attach the volume (Multi-Attach error for volume \"pvc-...\"). I can work around this by scaling to zero and then back up to 1 instead:\nkubectl scale --replicas=0 deployment/starterservice\nkubectl scale --replicas=1 deployment/starterservice\n\nI was wondering if there was a way to get kubectl rollout restart to wait for the old pod to terminate before starting a new one? Tx.\n",
  "Answer": "You need to set deployment strategy = recreate.\nspec:\n  strategy:\n    type: Recreate\n\nThe difference between the Recreate strategy compared to RollingUpdate (default) is that Recreate will terminate the old pod before creating new one while RollingUpdate will create new pod before terminating the old one.\nhttps://kubernetes.io/docs/concepts/workloads/controllers/deployment/#recreate-deployment\n"
},
{
  "Question": "Suppose that I have a Deployment which loads the env variables from a ConfigMap:\nspec.template.spec.containers[].envFrom.configMapRef\n\nNow suppose that I change the data inside the ConfigMap.\nWhen exactly are the Pod env variables updated? (i.e. when the app running in a pod sees the new env variables)\nFor example (note that we are inside a Deployment):\n\nIf a container inside the pod crashes and it is restarted does it read the new env or the old env?\nIf a pod is deleted (but not its RelicaSet) and it is recreated does it read the new env or the old env?\n\n",
  "Answer": "After some testing (with v1.20) I see that env variables in Pod template are updated immediately (it's just a reference to external values).\nHowever the container does not see the new env variables... You need at least to restart it (or otherwise delete and recreate the pod).\n"
},
{
  "Question": "I need to setup a RabbitMQ cluster with queue  mirroring enabled on all queues in Kubernetes.\nThe RabbitMQ plugin for kubernetes peer discovery only provides a clustering mechanism based on peer discovery , as the plugin name indicates.\nBut how do I enable queue mirroring and achieve HA , so that if pods a restarted for any reason or if I need to scale the Rabbitmq nodes , I can do it without any loss of messages.\n",
  "Answer": "Add a definitions.json file into your ConfigMap and ensure that your pods mount the file (in /etc/rabbitmq). In that file, specify all exchanges/queues, and have a policy defined for mirroring that would be applied to those exchanges/queues.\nIt may be easier to manually set this up and export the definitions file from a running RabbitMQ node.\nThis way - your cluster is all configured when started.\n"
},
{
  "Question": "I was testing Skaffod and It is a great tool for microservices development.\nBut I do not find any tutorial on how to use it with Java. Is there any support to Maven builds?\n",
  "Answer": "Skaffold now supports JIB out of the box which will be more efficient than multistage Dockerfile building! Check out the JIB Maven example in Skaffold.\n"
},
{
  "Question": "Some of our services in our K8s (EKS) envirnment use config files to drive functionality so we don't have to redeploy the whole image each time.  Using kubectl cp command allows us to copy new config files to the pod.  So the command kubectl cp settings.json myapi-76dc75f47c-lkvdm:/app/settings.json copies the new settings.json file to the pod.\nFor fun I deleted the pod and k8s recreated it successfully with the old settings.json file.  Anyone know a way of keeping the new settings.json file if the pod gets destroyed?  Is there a way to update the deployment without redeploying the image?\nThanks, Tim\n",
  "Answer": "Store the config file inside a ConfigMap and mount the ConfigMap to Deployment's pod template. When the file needs updating, either:\n\nRe-create the ConfigMap (kubectl delete then kubectl create --from-file)\nOr use the \"dry-run kubectl create piped into kubectl replace\" technique from https://stackoverflow.com/a/38216458/34586\n\n"
},
{
  "Question": "Just downloaded Lens 5.25. Windows 10, Docker Desktop 4/v20.10.8, Kubernetes v1.21.4.\nI run Docker Desktop, wait until Kubernetes is ready and then open Lens.\nIt shows docker-desktop as being disconnected.\nThere is no way to get past this screen. How do I open the docker-desktop cluster?\n",
  "Answer": "Click the cluster to open its information panel, then select the connect (chainlink) icon in the toolbar.\n\nYou can then click the cluster's icon in the same panel to open it.\n"
},
{
  "Question": "We deployed Gridgain cluster in Google Kubernetes cluster and it working properly with persistence enable. We need to auto scale enable. At scale up no any errors, but at the scale down given \"Partition loss\". We need to recover this loss partitions using control.sh script. But it is not possible at every time.\nWhat is the solution for this? Is scale down not working for Gridgain nodes?\n",
  "Answer": "Usually you should have backup factor sufficient to offset lost nodes (such as, if you have backups=2, you can lose at most 2 nodes at the same time).\nCoupled with baselineAutoAdjust set to reasonable value it should provide scale down feature.\nScale down with data loss and persistence enabled will indeed require resetting lost partitions.\n"
},
{
  "Question": "I've created a label with:\nkubectl label pods <pod id> color=green\nbut removing it using:\nkubectl label pods bar -color\ngives:\nunknown shorthand flag: 'c' in -color\nAny suggestions?\n",
  "Answer": "The dash goes at the end of the label name to remove it, per kubectl help label:\n# Update pod 'foo' by removing a label named 'bar' if it exists.\n# Does not require the --overwrite flag.\nkubectl label pods foo bar-\n\nSo try kubectl label pods bar color-.\n"
},
{
  "Question": "How do I automatically restart Kubernetes pods and pods associated with deployments when their configmap is changed/updated?\n\nI know there's been talk about the ability to automatically restart pods when a config maps changes but to my knowledge this is not yet available in Kubernetes 1.2. \nSo what (I think) I'd like to do is a \"rolling restart\" of the deployment resource associated with the pods consuming the config map. Is it possible, and if so how, to force a rolling restart of a deployment in Kubernetes without changing anything in the actual template? Is this currently the best way to do it or is there a better option?\n",
  "Answer": "The current best solution to this problem (referenced deep in https://github.com/kubernetes/kubernetes/issues/22368 linked in the sibling answer) is to use Deployments, and consider your ConfigMaps to be immutable.\nWhen you want to change your config, create a new ConfigMap with the changes you want to make, and point your deployment at the new ConfigMap. If the new config is broken, the Deployment will refuse to scale down your working ReplicaSet. If the new config works, then your old ReplicaSet will be scaled to 0 replicas and deleted, and new pods will be started with the new config.\nNot quite as quick as just editing the ConfigMap in place, but much safer.\n"
},
{
  "Question": "I'm working on a python script for update the configmaps programmatically.\nExample script at shown as below. \nimport requests\n\nheaders = {\"Content-Type\": \"application/json-patch+json\"}\nconfigData = {\n\"apiVersion\": \"v1\",\n\"kind\": \"ConfigMap\",\n\"data\": {\n    \"test2.load\": \"testimtest\"\n},\n\"metadata\": {\n    \"name\": \"nginx2\"\n}\n}\n\nr = requests.patch(\"http://localhost:8080/api/v1/namespaces/default/configmaps/nginx2\", json=configData)\n\nThe interesting side of this problem is that I have no problem with POST and GET methods but when I want to update kubernetes configmaps with PATCH method of HTTP I'm getting \n \"reason\":\"UnsupportedMediaType\" //STATUS_CODE 415\n\nHow I can handle this problem. \n",
  "Answer": "I suggest you use a Kubernetes client library, instead of making the raw HTTP calls yourself. Then you don't need to figure out the low-level connection stuff, as the library will abstract that away for you.\nI've been using Pykube, which provides a nice pythonic API, though it does appear to be abandoned now.\nYou can also use the official client-python, which is actively maintained. The library is a bit more clunky, as it's based on an autogenerated OpenAPI client, but it covers lots of use-cases like streaming results.\n"
},
{
  "Question": "When I run Kubernetes commands, Powershell is wanting me to use the path to the kubectl.exe instead of just using the command kubectl.\nI'm told using an Alias would work but I'm not sure how to do that in this case with Powershell and my attempts have come up fruitless.\nThis is what I tried:\nHow to make an alias for Kubectl in Windows using ENV Variables?\nI tried running this:\nC:\\Aliases> New-Item -ItemType File -Path C:\\Aliases\\\"K.bat\" -Value \"doskey k=kubectl $*\" -Force\nAnd made a system Environment Variable with Aliases as the name and C:\\Aliases as the value.\ntyping K, k, kubectl, etc. was not returning anything that looked like it was being set as an alias.\n",
  "Answer": "Place the following in your $PROFILE file (open it for editing with, e.g., notepad $PROFILE; if it doesn't exist, create it with New-Item -Force $PROFILE first):\nSet-Alias k kubectl.exe\n\nIf kubectl.exe isn't in a directory listed in $env:PATH, specify the full path instead (substitute the real directory path below):\nSet-Alias k 'C:\\path\\to\\kubectl.exe'\n\nThis allows you to invoke kubectl.exe with k in future sessions.\n(The post you link to is for cmd.exe (Command Prompt), not PowerShell.)\n"
},
{
  "Question": "I have two Kubernetes clusters in datacenters and I'm looking to create a third in public cloud. Both of my clusters use Azure AD for authentication by way of OIDC. I start my API server with the following: \n--oidc-issuer-url=https://sts.windows.net/TENAND_ID/\n--oidc-client-id=spn:CLIENT_ID\n--oidc-username-claim=upn\n\nI created a Kubernetes cluster on GKE, and I'm trying to figure out how to use my OIDC provider there. I know that GKE fully manages the control plane.\nIs it possible to customize a GKE cluster to use my own OIDC provider, which is Azure AD in this case?\n",
  "Answer": "This is now supported!  Check out the documentation on how to configure an external OIDC provider.\n"
},
{
  "Question": "\nI need to see the logs of all the pods in a deployment with N worker pods\nWhen I do kubectl logs deployment/name --tail=0 --follow the command syntax makes me assume that it will tail all pods in the deployment\nHowever when I go to process I don't see any output as expected until I manually view the logs for all N pods in the deployment\n\nDoes kubectl log deployment/name get all pods or just one pod?\n",
  "Answer": "only one pod seems to be the answer.\n\ni went here How do I get logs from all pods of a Kubernetes replication controller? and it seems that the command kubectl logs deployment/name only shows one pod of N\nalso when you do execute the kubectl logs on a deployment it does say it only print to console that it is for one pod (not all the pods)\n\n"
},
{
  "Question": "We are finding that our Kubernetes cluster tends to have hot-spots where certain nodes get far more instances of our apps than other nodes.\nIn this case, we are deploying lots of instances of Apache Airflow, and some nodes have 3x more web or scheduler components than others.\nIs it possible to use anti-affinity rules to force a more even spread of pods across the cluster?\nE.g. \"prefer the node with the least pods of label component=airflow-web?\"\nIf anti-affinity does not work, are there other mechanisms we should be looking into as well?\n",
  "Answer": "Try adding this to the Deployment/StatefulSet .spec.template:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: \"component\"\n                  operator: In\n                  values:\n                  - airflow-web\n              topologyKey: \"kubernetes.io/hostname\"\n\n"
},
{
  "Question": "I am having the following questions:\nActually, I am using grafana open source service in Azure Virtual Machine. I would like to see the Kubernetes SLA Metrics on Grafana. When I had googled for it, then I had got the following page: https://grafana.com/docs/grafana-cloud/kubernetes/\nWhen I tried to follow the tutorial, I am not able to find the onboarding or lightning icon. Is this integration only for grafana enterprise? If no, please let me know, how to proceed further.\nThanks for the answers in advance.\nRegards,\nChaitanya\n",
  "Answer": "There is now an Azure Managed Grafana service on Azure. When create an instance you can grant it access to Azure Monitor which will serve up statistics from your AKS clusters.\n"
},
{
  "Question": "I'm using ignite chart in kubernetes, in memory deployment without persistent volumes, how can I configure default tables to be created automatically after restart of all ignite pods?\n",
  "Answer": "You can specify them in your IgniteConfiguration using Java or Spring XML, via Query Entities mechanism:\nhttps://ignite.apache.org/docs/latest/SQL/indexes#configuring-indexes-using-query-entities\nIn this case all the caches and corresponding tables will be recreated when cluster is started.\n"
},
{
  "Question": "While deploying a Kubernetes application, I want to check if a particular PodSecurityPolicy exists, and if it does then skip installing it again.\nI came across the helm lookup function, which allows us to check the existing K8 resources.\nWhile I understand how to use this function to get all the resources of same kind, how do I use this function to check if a PodSecurityPolicy named \"myPodSecurityPolicy\" exists.\nI tried something like this:\n{{- if ne (lookup \"v1\" \"PodSecurityPolicy\" \"\" \"\") \"myPodSecurityPolicy\"}}\n<do my stuff>\n\n{{- end }}\n\nBut it doesn't look like I can compare it this way, seeing an error -\nerror calling ne: invalid type for comparison\n\nAny inputs? Thanks in advance.\n",
  "Answer": "Please check your API version and PSP name. Lookup is returning a map or nil not a string and that's why you are getting that error. The following is working for me. For negative expression, just add not after if.\n{{- if (lookup \"policy/v1beta1\" \"PodSecurityPolicy\" \"\" \"example\") }}\n<found: do your stuff>\n\n{{- end }}\n\nHTH\n"
},
{
  "Question": "I need to be able to assign custom environment variables to each replica of a pod. One variable should be some random uuid, another unique number. How is it possible to achieve? I'd prefer continue using \"Deployment\"s with replicas. If this is not feasible out of the box, how can it be achieved by customizing replication controller/controller manager? Are there hooks available to achieve this?\n",
  "Answer": "You can use the downward API to inject the metadata.uid of the pod as an envvar, which is unique per pod\n"
},
{
  "Question": "I am developing a website that runs a simulation given a user-submitted script. I tried to follow some Online Judge architectures, but in my case, I need to send user input and receive the output in realtime, like a simulation.\nI tried Kubernetes Jobs, but isn't seems so easy to communicate with the container, especially if I need a Kubernetes client on the language that I am working.\nSo, my question is: Given this scenario, what is the best approach to orchestrate multiple containers with interactive I/O programmatically?\n\n*Obs.: I am not worrying about security yet.\n",
  "Answer": "Please take a look at the design of the spark operator:\nhttps://github.com/GoogleCloudPlatform/spark-on-k8s-operator\nThat has a somewhat similar design to what you\u2019re targeting. Similarly, Argo Workflow is another example:\nhttps://github.com/argoproj/argo\n"
},
{
  "Question": "Difficulty running airflow commands when running Airflow on Kubernetes that I installed from the Helm stable/airflow repo. For instance I try to exec into the scheduler pod and run airflow list and I get the following error:\nairflow.exceptions.AirflowConfigException: error: cannot use sqlite with the KubernetesExecutor airlow\n\nOk so I switch to the celery executor.\nSame thing\nairflow.exceptions.AirflowConfigException: error: cannot use sqlite with the CeleryExecutor\n\nSo what is the correct way to run airflow CLI commands when running on K8s?\n",
  "Answer": "Make sure you are using bash. /home/airflow/.bashrc imports the environment variables from /home/airflow/airflow_env.sh to setup the connection. The following are some examples:\nkubectl exec -ti airflow-scheduler-nnn-nnn -- /bin/bash\n$ airflow list_dags\n\nOr with shell you can import the env vars yourself:\nkubectl exec -ti airflow-scheduler-nnn-nnn -- sh -c \". /home/airflow/airflow_env.sh && airflow list_dags\"\n\n"
},
{
  "Question": "Did someone already get this error : pods \"k8s-debian9-charming-but-youthful-merkle\" is forbidden: pod does not have \"kubernetes.io/config.mirror\" annotation, node \"k8s-uk1-node-002\" can only create mirror pods ?\nWhy the node is configured to create only mirror pods ? How to unconfigure this ? Is this RBAC policies ?\nI created the kubernetes cluster with terraform and ansible on an openstack, with kubespray\nAny help is welcome,\nthanks by advance,\nGreg\n",
  "Answer": "The NodeRestriction admission plugin is responsible for enforcing that limitation, to prevent nodes from creating pods that expand their access to resources like serviceaccounts and secrets unrelated to their existing workloads\n"
},
{
  "Question": "I have installed istio 1.6.7 in an AKS cluster using istioctl.\nI have enabled the istio operator using init command.\nWhen I try to enable Grafana and Kiali using a separate yaml on top of the installed istio system with kubectl, the istio ingress gateway pod is recreated and my custom configurations are deleted.\nThe documentation specifies that we can install add-ons with kubectl.\nAdd-on yaml is as follows:\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nspec:\n  values:\n   grafana:\n      enabled: true\n\n",
  "Answer": "I am assuming you are referring to the Standalone Operator Installation guide. When updating the configuration, you have to change the original manifest and not create a new one. Your specified manifest doesn't contain any profile or metadata information. It should look like the following:\napiVersion: install.istio.io/v1alpha1\nkind: IstioOperator\nmetadata:\n  namespace: istio-system\n  name: example-istiocontrolplane\nspec:\n  profile: default\n  addonComponents:\n    grafana:\n      enabled: true\n\n"
},
{
  "Question": "I'm trying to connect my k8s cluster to my ceph cluster with  this manual:\nhttps://akomljen.com/using-existing-ceph-cluster-for-kubernetes-persistent-storage/\nI want to deploy rbd-provision pods into kube-system namespace like this https://paste.ee/p/C1pB4 \nAfter deploying pvc I get errors because my pvc is in default namespace. Can I do with that anything? I read docs and if I'm right I can't use ServiceAccount with 2 ns, or can?\n",
  "Answer": "Service accounts can be granted permissions in another namespace. \nFor example, within the namespace \"acme\", grant the permissions in the view ClusterRole to the service account in the namespace \"acme\" named \"myapp\" :\nkubectl create rolebinding myapp-view-binding \\\n  --clusterrole=view --serviceaccount=acme:myapp \\\n  --namespace=acme\n\n"
},
{
  "Question": "Been working on a disaster recovery plan for my Kubernetes cluster and I am able to make snap shots of my managed disks but im not sure how to bind a recovered manager disk to an existing volumn cliam so I can re hydrate my data after a loss of data.\n",
  "Answer": "You can mount any disk manually as a volume in a POD to recover data. Better approach would be to use Velero to take k8s configuration backup. It will save the disk and PVC information and should restore the volume claims smoothly.\nAdditionally, have you looked at AzureCSI drivers? That's the hot stuff in AKS right now. It does support Volume snapshotting and recovery from within the cluster. Best practice still would be to use Velero for configuration with CSI to backup whenever possible.\n"
},
{
  "Question": "Kubernetes ships with a ConfigMap called coredns that lets you specify DNS settings. I want to modify or patch a small piece of this configuration by adding:\napiVersion: v1\nkind: ConfigMap\ndata:\n  upstreamNameservers: |\n    [\"1.1.1.1\", \"1.0.0.1\"]\n\nI know I can use kubectrl edit to edit the coredns ConfigMap is there some way I can take the above file containing only the settings I want to insert or update and have it merged on top of or patched over the existing ConfigMap?\nThe reason for this is that I want my deployment to be repeatable using CI/CD. So, even if I ran my Helm chart on a brand new Kubernetes cluster, the settings above would be applied.\n",
  "Answer": "This will apply the same patch to that single field:\nkubectl patch configmap/coredns \\\n  -n kube-system \\\n  --type merge \\\n  -p '{\"data\":{\"upstreamNameservers\":\"[\\\"1.1.1.1\\\", \\\"1.0.0.1\\\"]\"}}'\n\n"
},
{
  "Question": "I am trying to add at this --server argument three https endpoint to make it HA. Is that possible?\nkubectl config set-cluster Zarsk \\\n    --certificate-authority=ca.pem \\\n    --embed-certs=true \\\n    --server=https://${k8s_PUBLIC_ADDRESS}:6443 \\\n    --kubeconfig=${instance}.kubeconfig\n\n",
  "Answer": "No, only a single connection URL is supported\n"
},
{
  "Question": "I can't find documentation on how to create user group on Kubernetes with yaml file. I'd like gather some authenticated users in group using their e-mail accounts.\nI'd like to write in yaml something like :\n kind: GoupBinding\n apiVersion: rbac.authorization.k8s.io/v1beta1\n metadata:\n   name: \"frontend-developers\"\n   namespace: development\nsubjects:\n- kind: User\n  name: a@xyz.com,vv@xyz.com\n  apiGroup: \"\"\n\n",
  "Answer": "Groups are determined by the configured authentication method. See https://kubernetes.io/docs/reference/access-authn-authz/authentication/ for details about how each authenticator determines the group membership of the authenticated user.\n"
},
{
  "Question": "is jsonPath supported in kubernetes http api ?\nfor ex; how the following translates to in http API ?\nkubectl get pods -o=jsonpath='{.items[0]}'\n\n",
  "Answer": "It's not supported by the API, you would need to evaluate that jsonpath against the API response. \n"
},
{
  "Question": "I'd like to mount volume if it exists. For example:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n  - name: mypod\n    image: redis\n    volumeMounts:\n    - name: foo\n      mountPath: \"/etc/foo\"\n  volumes:\n  - name: foo\n    secret:\n      secretName: mysecret\n\nis the example from the documentation. However if the secret mysecret doesn't exist I'd like to skip mounting. That is optimistic/optional mount point.\nNow it stalls until the secret is created.\n",
  "Answer": "secret and configmap volumes can be marked optional, and result in empty directories if the associated secret or configmap doesn't exist, rather than blocking pod startup\napiVersion: v1\nkind: Pod\nmetadata:\n  name: mypod\nspec:\n  containers:\n    - name: mypod\n      image: redis\n      volumeMounts:\n        - name: foo\n          mountPath: /etc/foo\n  volumes:\n    - name: foo\n      secret:\n        secretName: mysecret\n        optional: true\n\n"
},
{
  "Question": "In Kubernetes object metadata, there are the concepts of resourceVersion and generation.  I understand the notion of resourceVersion: it is an optimistic concurrency control mechanism\u2014it will change with every update.  What, then, is generation for?\n",
  "Answer": "resourceVersion changes on every write, and is used for optimistic concurrency control\nin some objects, generation is incremented by the server as part of persisting writes affecting the spec of an object.\nsome objects' status fields have an observedGeneration subfield for controllers to persist the generation that was last acted on.\n"
},
{
  "Question": "The kubernetes service is in the default namespace. I want to move it to kube-system namespace. So I did it as follow:\nkubectl get svc kubernetes -o yaml > temp.yaml\n\nThis generates temp.yaml using current kubernetes service information. Then I changed the value of namespace to kube-system in temp.yaml. Lastly, I ran the following command:\nkubectl replace -f temp.yaml\n\nBut I got the error:\nError from server: error when replacing \"temp.yaml\": service \"kubernetes\" not found\n\nI think there is no service named kubernetes in the kube-system namespace.\nWho can tell me how can to do this?\n",
  "Answer": "Name and namespace are immutable on objects. When you try to change the namespace, replace looks for the service in the new namespace in order to overwrite it. You should be able to do create -f ... to create the service in the new namespace\n"
},
{
  "Question": "I have a AKS cluster in which I have deployed three Docker containers in three different namespaces. The first pod needs to check the availability of the other two pods. What I mean is that POD-1 monitors the POD-2 and POD-3. Is there any option to monitor the pod this way? Can POD-1 generate logs or alerts if any error occurs in the other two pods?\nThe first pods is a C# application.\n",
  "Answer": "You can use the Kubernetes API client SDK for C# (https://github.com/kubernetes-client/csharp) to connect with the API Server and get the status of the desired PODS.\nYou may need to create a Service Account and assign it to your POD if you get permission issues.\n\nClient libraries often handle common tasks such as authentication for you. Most client libraries can discover and use the Kubernetes Service Account to authenticate if the API client is running inside the Kubernetes cluster, or can understand the kubeconfig file format to read the credentials and the API Server address.\n\nhttps://kubernetes.io/docs/reference/using-api/client-libraries/\n"
},
{
  "Question": "I have a k8s job setup as a helm hook for pre-install and pre-upgrade stages.\n\"helm.sh/hook\": pre-install,pre-upgrade\nIs there a way to know inside the job/pod which stage it is getting executed - whether it is pre-install or pre-upgrade ?\n",
  "Answer": "You could create separate jobs/pods manifests assigning them different arguments/env variables to keep track of the hook events. I haven't seen anything in the tool itself.\n"
},
{
  "Question": "I'm trying to run mongo commands through my bash script file.\nI have sample.sh -- run with ./sample.sh\nI want to run below command inside my sample.sh file. It is bash script.\nI want to mongo commands run inside the file, not inside the shell.\nkubectl exec $mongoPodName -c mongo-mongodb -- mongo db.createUser({user:\"admin\",pwd:\"123456\",roles:[{role:\"root\",db:\"admin\"}]})\n\n\nI get the error below :\n./generate-service-dbs.sh: line 37: syntax error near unexpected token `('\n./generate-service-dbs.sh: line 37: `kubectl exec $mongoPodName -c mongo-mongodb -- mongo db.createUser({user:\"admin\",pwd:\"123456\",roles:[{role:\"root\",db:\"admin\"}]})'\n\n\n\n",
  "Answer": "Don't you have to run command with --eval?\nkubectl exec $mongoPodName -c mongo-mongodb -- mongo --eval 'db.createUser({user:\"admin\",pwd:\"123456\",roles:[{role:\"root\",db:\"admin\"}]})'\n\n\n"
},
{
  "Question": "I created a namespace xxx; the role for this namespace is to get pods, services, etc. I created a service account yyy and a role binding yyy to the role in namespace xxx.\nWhen I try to check something through the API with a secret token, for example\ncurl -kD - -H \"Authorization: Bearer $TOKEN https://localhost:6443/api/v1/namespaces/xxx/pods\n\nI get a \"403 forbidden\" error.\nSo I a cluster role binding of my service account yyy to cluster role view, and after that of course a user can see pods of my namespace, but can see other pods from other namespaces too.\nHow can I restrict service account yyy tee see pods, services, etc. only from a specific namespace?\n",
  "Answer": "To allow access only in a specific namespace create a rolebinding, not a clusterrolebinding:\nkubectl create rolebinding my-viewer --clusterrole=view --serviceaccount=xxx:yyy -n xxx\n"
},
{
  "Question": "I am using Google Kubernetes Engine and have the Google HTTPS Load Balancer as my ingress.\nRight now the load balancer uses Let's Encrypt certificates. However, is there a simple way to ensure that the certificates are automatically renewed prior to their 90 day expiry?\n",
  "Answer": "You have not specified how you configured Let's Encrypt for your load balancer. Right now Google does not offer this for you, so I assume you mean you set the Let's Encrypt certificate yourself. In this case, Google can't renew your certificate.\nUntil there's an official support you can install a third-party add-on like cert-manager to automate certificate configuration and renewal. There's a GKE tutorial for doing this at https://github.com/ahmetb/gke-letsencrypt.\n"
},
{
  "Question": "I'm writing a shell script which needs to login into the pod and execute a series of commands in a kubernetes pod.\nBelow is my sample_script.sh:\nkubectl exec octavia-api-worker-pod-test -c octavia-api bash\nunset http_proxy https_proxy\nmv /usr/local/etc/octavia/octavia.conf /usr/local/etc/octavia/octavia.conf-orig\n/usr/local/bin/octavia-db-manage --config-file /usr/local/etc/octavia/octavia.conf upgrade head\n\nAfter running this script, I'm not getting any output.\nAny help will be greatly appreciated\n",
  "Answer": "Are you running all these commands as a single line command? First of all, there's no ; or && between those commands. So if you paste it as a multi-line script to your terminal, likely it will get executed locally.\nSecond, to tell bash to execute something, you need: bash -c \"command\".\nTry running this:\n$ kubectl exec POD_NAME -- bash -c \"date && echo 1\"\n\nWed Apr 19 19:29:25 UTC 2017\n1\n\nYou can make it multiline like this:\n$ kubectl exec POD_NAME -- bash -c \"date && \\\n      echo 1 && \\\n      echo 2\"\n\n"
},
{
  "Question": "I have deployed kubernetes v1.8 in my workplace. I have created roles for admin and view access to namespaces 3months ago. In the initial phase RBAC is working as per the access given to the users. Now RBAC is not happening every who has access to the cluster is having clusteradmin access. \nCan you suggest the errors/changes that had to be done?\n",
  "Answer": "Ensure the RBAC authorization mode is still being used (--authorization-mode=\u2026,RBAC is part of the apiserver arguments)\nIf it is, then check for a clusterrolebinding that is granting the cluster-admin role to all authenticated users:\nkubectl get clusterrolebindings -o yaml | grep -C 20 system:authenticated\n"
},
{
  "Question": "I'd like to see the 'config' details as shown by the command of:\nkubectl config view\n\nHowever this shows the entire config details of all contexts, how can I filter it (or perhaps there is another command), to view the config details of the CURRENT context?\n",
  "Answer": "kubectl config view --minify displays only the current context\n"
},
{
  "Question": "I'm encountering a situation where pods are occasionally getting evicted after running out of memory.  Is there any way to set up some kind of alerting where I can be notified when this happens?\nAs it is, Kubernetes keeps doing its job and re-creating pods after the old ones are removed, and it's often hours or days before I'm made aware that a problem exists at all.\n",
  "Answer": "GKE exports Kubernetes Events (kubectl get events) to Stackdriver Logging, to the \"GKE Cluster Operations\" table:\n\nNext, write a query specifically targeting evictions (the query I pasted below might not be accurate):\n\nThen click \"CREATE METRIC\" button.\nThis will create a Log-based Metric. On the left sidebar, click \"Logs-based metrics\" and click the \"Create alert from metric\" option on the context menu of this metric:\n\nNext, you'll be taken to Stackdriver Alerting portal. You can set up alerts there based on thresholds etc.\n"
},
{
  "Question": "I am running geth full node https://github.com/ethereum/go-ethereum/wiki/geth on Google Cloud platform on a VM instance. Currently, I have mounted a SSD and write the chain data to it.\nI want to now run it on multiple VM instances and use a load balancer for serving the requests made by Dapp. I can do this using a normal load balancer and create VMs and autoscale. However, I have the following questions:\n\nSSD seems to be a very important part of blockchain syncing speed. If I simply create VM images and add it for autoscaling, it won't help much because the blockchain will take time to sync.\nIf I want to run these nodes on kubernetes cluster, what's the best way to use the disk?\n\n",
  "Answer": "Take a look at this Kubernetes Engine tutorial which shows you how to run StatefulSets with automatic persistent volume provisioning: https://cloud.google.com/kubernetes-engine/docs/how-to/stateful-apps\nTake a look at this Kubernetes Engine tutorial which shows you how to provision SSD disks https://cloud.google.com/kubernetes-engine/docs/concepts/persistent-volumes#ssd_persistent_disks \nWith these + HorizontalPodAutoscaler, you should be able to create a StatefulSet with auto-scaling and each pod will get its own SSD disk.\n"
},
{
  "Question": "I would like to disable the logging of the health checks produced by my Ingress, on my pods.\nI have a GCE ingress, distributing two pods, and I would like to clear up the logs i get from them.\nDo you have any idea ?\nThanks,\n",
  "Answer": "(It's not clear what do you mean by disabling logs. So I'll make an assumption.)\nIf your application is logging something when it gets a request, you can check the user agent of the request to disable requests from Google Load Balancer health checking.\nWhen you provision a GCE ingress, your app will get a Google Cloud HTTP Load Balancer (L7). This LB will make health requests with header:\nUser-agent: GoogleHC/1.0\n\nI recommend checking for a case-insensitive header (\"user-agent\") and again a case-insenstive check to see if its value is starting with \"googlehc\".\nThis way, you can distinguish Google HTTP (L7) load balancer health requests and leave them out of your logs.\n"
},
{
  "Question": "I'd like to ask if is there possibility to create shared quota for few namespaces, e.g. for team\nScenario:\ncluster:\n100c\n100gb ram\n\nquota for TEAM A 40c/40gb\nquota for TEAM B 60c/60gb\n\nSome namespaces from TEAM An e.g.\nteama-dev\nteama-test\nteama-stage\nteama-int\n\nHave to be all limited to quota 40c/40gb\nSame for TEAM B.\nCase is that I don't want specify quotas directly for namespaces, but for team, or group of namespaces\n",
  "Answer": "That is not possible in Kubernetes today.\nOpenShift supports quota across multiple namespaces via ClusterResourceQuota, and it is possible something like that might make it into Kubernetes in the future, but does not exist yet.\n"
},
{
  "Question": "I cannot find a way to remove GPU (accelerator resource) from Google Kubernetes Engine (GKE) cluster. There is no official documentation on how to make change to it. Can you suggest a proper way to do so? The UI is gray out and it cannot allow me to make change from the console. \nHere is the screenshot when I click to edit cluster.\n\nThank you\n",
  "Answer": "You cannot edit settings of a Node Pool once it is created.\nYou should create a new node pool with the settings you want (GPU, machine type etc) and delete the old node pool.\nThere's a tutorial on how to migrate to a new node pool smoothly here: https://cloud.google.com/kubernetes-engine/docs/tutorials/migrating-node-pool If you don't care about pods terminating gracefully, you can create a new pool and just delete the old one.\nYou can find more content about this at https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-upgrading-your-clusters-with-zero-downtime.\n"
},
{
  "Question": "I've already manage to have max one pod with containerized application per node. Now I want to create Kubernetes cluster in which each user will have access to one, personal node. As a result I want to have architecture where each user will work in isolated environment. I was thinking about some LoadBalancing with Ingress rules, but I am not sure it's a good way to achieve this.\nArchitecture\n",
  "Answer": "Kubernetes in general is not a fit for you.\nIf you're trying to do \"max 1 per pod node\" or \"a node for every user\", you should not force yourself to use Kubernetes, maybe what you're looking for is just virtual machines.\n"
},
{
  "Question": "For a deployed Kubernetes CronJob named foo, how can I manually run it immediately?  This would be for testing or manual runs outside its configured schedule.\n",
  "Answer": "You can start a job based on an existing job's configuration, and a cronjob is just another type of job.  \nSyntax:\nkubectl create job --from=cronjob/$CronJobName $NameToGiveThePodThatWillBeCreated \ne.g.:\nkubectl create job --from=cronjob/foo foo-manual-1\n"
},
{
  "Question": "Hello StackOverflow users,\nI've started working in the Kubernetes space recently and saw that Custom Resource Definitions(CRDs) types are not namespaced and are available to to all namespaces.\nI was wondering why it isn't possible to make a CRD type scoped to a namespace. Any help would be appreciated!\n",
  "Answer": "See https://github.com/kubernetes/kubernetes/issues/65551#issuecomment-400909534 for a discussion of this issue.\nA particular CRD can define a custom resource that is namespaced or cluster-wide, but the type definition (the CRD itself) is cluster-wide and applies uniformly to all namespaces.\n"
},
{
  "Question": "When I make request for this started server: https://gist.github.com/Rasarts/1180479de480d7e36d6d7aef08babe59#file-server\nI get right response:\n{\n  \"args\": {}, \n  \"headers\": {\n    \"Accept-Encoding\": \"gzip\", \n    \"Connection\": \"close\", \n    \"Host\": \"httpbin.org\", \n    \"User-Agent\": \"Go-http-client/1.1\"\n  }, \n  \"origin\": \"\", \n  \"url\": \"https://httpbin.org/get\"\n}\n\nBut when I make request to that server on minikube which was created this way:\nhttps://gist.github.com/Rasarts/1180479de480d7e36d6d7aef08babe59#file-serve-yaml\nI get error:\nERROR: Get https://httpbin.org/get: EOF<nil>\n\nHow can I make http requests from kubernetes pod?\n",
  "Answer": "Knative uses Istio and Istio, by default, doesn't allow outbound traffic to external hosts, such as httpbin.org. That's why your request is failing.\nFollow this document to learn how to configure Knative (so that it configures Istio correctly) to make outbound connections. Or, you can directly configure the Istio by adding an egress policy: https://istio.io/docs/tasks/traffic-management/egress/\n"
},
{
  "Question": "We use Google Cloud Run on our K8s cluster on GCP which is powered by Knative and Anthos, however it seems the load balancer doesn't amend the x-forwarded-for (and this is not expected as it is TCP load balancer), and Istio doesn't do the same.\nDo you have the same issue or it is limited to our deployment?\nI understand Istio support this as part of their upcoming Gateway Network Topology but not in the current gcp version.\n",
  "Answer": "I think you are correct in assessing that current Cloud Run for Anthos set up (unintentionally) does not let you see the origin IP address of the user.\nAs you said, the created gateway for Istio/Knative in this case is a Cloud Network Load Balancer (TCP) and this LB doesn\u2019t preserve the client\u2019s IP address on a connection when the traffic is routed to Kubernetes Pods (due to how Kubernetes networking works with iptables etc). That\u2019s why you see an x-forwarded-for header, but it contains internal hops (e.g. 10.x.x.x).\nI am following up with our team on this. It seems that it was not noticed before.\n"
},
{
  "Question": "I'm mounting a local folder into minikube and using that folder inside a pod. The folder contains the code I am developing. It works great but changes I make are not being reflected in the browser. If I exec into the pod I can see my code changes, just not in the browser. \nIf I delete the pod when it is automatically recreated the changes are reflected in the browser. Is this a limitation of the solution? \nCan anybody please advise a novice?\n",
  "Answer": "Have a look at Skaffold \u2014 and its sync; it ensures your yaml files are running inside Minikube and ships files of your selection back and forth.\n"
},
{
  "Question": "My Kubernetes on AKS is using one resource group to inform their costs.\nAt this moment we have many projects in the company, will be great if each POD report their costs to a different resource group named as the project.\nHow can I do this?\n",
  "Answer": "You'll need to implement an intra-cluster cost tool.\nThe most popular in the kubernetes ecosystem is kubecost, and they have recently release and OSS version, OpenCost. https://www.opencost.io/\nTypically you'd create a namespace for each app/costcentre. The default dashboards show breakdown by namespace cost over a time period.\n"
},
{
  "Question": "I am trying to create a housekeeping job where I erase namespaces that have not been used for seven days. I have two options:\n\nI can use a Job, but I don't know how to make sure the Jobs are running on the date that I want.\nI read about CronJob. Unfortunately, CronJob in Kubernetes can only support 5 fields (default format). This means we can only define dates and months, but not years. \n\nWhich one is better to use?\n",
  "Answer": "Kubernetes CronJob API is very similar to cron as you said and doesn't have a  year field.\nIf you need something that gets scheduled on time, you should write a kubernetes controlller that waits until the date you want, and then calls into Kubernetes  API to create a Job object. This shouldn't be very complicated if you can program Go with the examples here: https://github.com/kubernetes/client-go\n"
},
{
  "Question": "I'm using the handy kubectl logs -l label=value command to get log from all my pods matching a label. I want to see which pod outputted what log, but only the log text is displayed. Is there a way to control the log format, or a command argument which will let me do this?\n",
  "Answer": "kubectl now has a --prefix option that allows you to prefix the pod name before the log message.\n"
},
{
  "Question": "Let's say that I have a running pod named my-pod\nmy-pod reads the secrets from foobar-secrets\nNow let's say that I update some value in foobar-secrets\nkubectl patch secret foobar-secrets --namespace kube-system --context=cluster-1 --patch \"{\\\"data\\\": {\\\"FOOBAR\\\": \\\"$FOOBAR_BASE64\\\"}}\"\n\nWhat I should do to restart/reload the pod in order to get the new value?\n",
  "Answer": "https://github.com/stakater/Reloader is the usual solution for a fully standalone setup. Another option is https://github.com/jimmidyson/configmap-reload or similar but that requires coordination with the daemon process to have an API of some kind for reloading.\n"
},
{
  "Question": "Is there any way to run kubectl proxy, giving it a command as input, and shutting it down when the response is received?\nI'm imagining something with the -u (unix socket) flag, like this:\nkubectl proxy -u - < $(echo \"GET /api/v1/namespaces/default\")\n\nI don't think it's possible, but maybe my socket fu just isn't strong enough.\n",
  "Answer": "You don't need a long-running kubectl proxy for this.\nTry this:\nkubectl get --raw=/api/v1/namespaces/default\n\n"
},
{
  "Question": "SSH to GKE node private IP from the jump server (Bastion host) is not working.\nI even tried the following as suggested by one of my friends, but it did not help.\ngcloud compute instances add-metadata $INSTANCE_NAME_OR_INSTANCE_ID --metadata block-project-ssh-keys=false --zone $YOUR_VM_ZONE --project $YOUR_PROJECT\n\nAlso please confirm if the solution works for Private GKE too.\n",
  "Answer": "GKE node is just a GCE VM. You can just access it as a normal GCE instance if with proper privilege and ssh key configured.\nOne thing worth to mention that GCP support IAP based ssh port forwarding \n"
},
{
  "Question": "I need to find out if all deployments having label=a is in READY state? Example is below. I need to return true or false based on wether all deployments are in READY or NOT ready? I can parse text but I think there might be a more clever way with just kubectl and json path or something\nPS C:\\Users\\artis> kubectl get deployment -n prod -l role=b\nNAME           READY   UP-TO-DATE   AVAILABLE   AGE\napollo-api-b   0/3     3            0           107s\nesb-api-b      0/3     3            0           11m\nfrontend-b     3/3     3            3           11m\n\n",
  "Answer": "Add -o yaml to see the YAML objects for each, which you can then use to build a -o jsonpath like -o jsonpath='{range .items[*]}{.status.conditions[?(@.type == \"Available\")].status}{\"\\n\"}{end}'. You can't do logic operations in JSONPath so you'll need to filter externally like | grep  False or something.\n"
},
{
  "Question": "Kubectl get cs -o ymal\n\nreturns the healthy status for the control plane, but due some some reason,\nkubectl get pods --all-namespaces\n\ndoes not show any control plane pods like api-server, schedular, controller manager etc.\nI can also see the manifest file at /etc/kubernetes/manifests location as well.\nPlease help what am I missing.\n",
  "Answer": "GKE does not run the control plane in pods. Google does not really talk about how they run it but it's likely as containers in some GKE-specific management system.\n"
},
{
  "Question": "I am getting metrics exposed by kube-state-metrics by querying Prometheus-server but the issue is I am getting duplicate metrics with difference only in the job field. . I am doing query such as :\ncurl 'http://10.101.202.25:80/api/v1/query?query=kube_pod_status_phase'| jq\n\nThe only difference is coming the job field. Metrics coming when querying Prometheus-Server\nAll pods running in the cluster: https://i.stack.imgur.com/WxNXz.jpg\nAny help is appreciated.\nThank You\nprometheus.yml\nglobal:\n  scrape_interval:     15s\n  evaluation_interval: 15s\n\nrule_files:\n  # - \"first.rules\"\n  # - \"second.rules\"\n\nscrape_configs:\n  - job_name: prometheus\n    static_configs:\n      - targets: ['localhost:9090']\n\n",
  "Answer": "You are running (or at least ingesting) two copies of kube-state-metrics. Probably one you installed and configured yourself and another from something like kube-prometheus-stack?\n"
},
{
  "Question": "How can I trigger the update (redeploy) of the hearth through the k8s golang client.\nAt the moment, I use these libraries to get information about pods and namespaces:\nv1 \"k8s.io/api/core/v1\nk8s.io/apimachinery/pkg/apis/meta/v1\nk8s.io/client-go/kubernetes\nk8s.io/client-go/rest\n\nMaybe there is another library or it can be done through linux signals\n",
  "Answer": "The standard way to trigger a rolling restart is set/update an annotation in the pod spec with the current timestamp. The change itself does nothing but that changes the pod template hash which triggers the Deployment controller to do its thang. You can use client-go to do this, though maybe work in a language you're more comfortable with if that's not Go.\n"
},
{
  "Question": "we have pods on GKE.\nwe can delete pod by kubectl -n <ns> delete pod <pod name>.\nwe can also delete the pod by clicking the following delete button.\n\nwhat are the differences?\nwhat are the results if I did both?\nthanks\nUPDATE\nthanks. The pod has terminationGracePeriodSeconds: 60. what will happen if I run kubectl delete pod pod_name and then ctrl C? Then click the delete button on the web ui? all these are in 60 seconds.\nI am curious whether it will delete the pod by force without waiting for 60 seconds.\nthanks\n$ kubectl -n ns delete pod pod-0\npod \"pod-0\" deleted\n^C\n\n",
  "Answer": "Both trigger the same API call to the kube-apiserver. If you try to delete something twice, the second call will fail either \"already deleted\" or \"not found\".\n"
},
{
  "Question": "I have created a custom Identity Service that mainly uses Identity Server 4 with Azure AD as an external provider. I have configured azure ad, having all the required ids & secrets and locally was able to authenticate any registered user in Azure.\nThe problem appears when we deployed that service into Kubernetes. \nI have added a public URL like https://myidentitydomain.com/signin-oidc. In a pod, we have a different domain of identity service than public one (be-identity-service), but it is how it works in Kubernetes. Not sure that its the issue connected to reply URL failure. But also my identity service has to be hosted in private network in Azure.\nReally appreciate for any given advice.\n",
  "Answer": "When you click on the sign-in button to authenticate with IdentityServer, do look at the URL to see what returnurl was actually sent to it and add it to the client definition.\nFor example:\nhttps://demo.identityserver.io/Account/Login?ReturnUrl=%2Fdiagnostics\n\n"
},
{
  "Question": "we need to initiate Kubernetes cluster and start our development.\nIs it OK to have 1 master Control Plane node and 1 worker node with our containers to start the development?\nWe can afford for services to be unavailable in case of upgrades, scaling and so on, I've just worried if I am lacking some more important info.\nI was planning to have 8CPUs and 64 GB since that are the similar resources which we have on one of our VMs without containers with the same apps.\nWe will deploy cluster with Azure Kubernetes Service.\nThank you\n",
  "Answer": "Sure, you can also have single node clusters. Just as you said, that means if one node goes down, the cluster is unavailable.\n"
},
{
  "Question": "While scaling-in, HPA shouldn't terminate a pod that has a job running on it.\nThis is taken care of by AWS autoscaling groups in the form of scale-in protection for instances. Is there something similar in kubernetes?\n",
  "Answer": "You use terminationGracePeriodSeconds to make your worker process wait until it is done. It will get a SIGTERM, then has that many seconds to finish (default 9 but you can make it anything, some of my workers have it set to 12 hours), then SIGKILL if it hasn't exited. So stop accepting new work units on SIGTERM, set the threshold to be the length of your longest work unit, and no worries :)\n"
},
{
  "Question": "When I run gdb on my binary inside kubernetes pod in  container it starts loading  symbol after that it  suddenly gets terminated with SIGTERM and exit code 137. I checked for describe pod it shows reason Error. I have added ptrace capabilities in yaml. Can someone help me with this.\n",
  "Answer": "Exit code 137 means that the process was killed by signal 9 (137=128+9). Most likely reason is that container was running out of memory and gdb was killed by OOM Killer. Check dmesg output for any messages from OOM Killer.\n"
},
{
  "Question": "I am unable to delete \u2018test/deployment/sandbox-v2/tmp/dns\u2019 after deleting the github repository 'test'. I am unable to reclone it in my CentOS system. Even after recloning in other folder,running site.yml file of sandbox fails at logs. So i'm trying to completely remove test repository and reclone it for fresh run. I have tried all ways and commands to remove it, it gets removed as well but then gets recreated automatically with this file mentioned. Any clue how to completely remove it and clone a fresh repo.\n",
  "Answer": "It's hard to say but if this was bind-mounted into a container and that container was running its process as root (uid 0) then files it created would be owned by uid 0 even outside the container. Gotta get your sudo on.\n"
},
{
  "Question": "I have k8s custer in digital ocean.\nI would like to expose some app to internet.\nDo I need for every app DO loadbalancer? It coast 10$/m it F* expensive when I would like to expose 5 aps.\nIs there any workaround without external DO loadbalancer?\n",
  "Answer": "Copying this from the last time someone asked this a few days ago (which has now been delete):\nYou can use a node port service (annoying because you'll have to use random high ports instead of 80/443) or you can switch your ingress controller to listen on the host network directly (allows use of 80/443 but potentially bigger security impact if your ingress controller is compromised).\n"
},
{
  "Question": "I have the following to exec into a pod to run a command\nfmt.Println(\"Running the command:\", command)\nparameterCodec := runtime.NewParameterCodec(scheme)\nreq.VersionedParams(&corev1.PodExecOptions{\n    Command:   strings.Fields(command),\n    Stdin:     stdin != nil,\n    Stdout:    true,\n    Stderr:    true,\n    TTY:       false,\n}, parameterCodec)\n\nI'm looking to run the same command but adding the Container option which is a string. I'm having a hard time figuring how how I can list all the containers in a pod. \nThanks\n",
  "Answer": "got it\npod, err := clientset.CoreV1().Pods(\"default\").Get(podname, metav1.GetOptions{})\nif err != nil {\n    return \"\", err\n}\n\nfmt.Println(pod.Spec.Containers[0])\n\n"
},
{
  "Question": "Is it alright to use Google Compute Engine virtual machines for MySQL DB?\ndb-n1-standard-2 costs around $97 DB for single Clould SQL instance and replication makes it double.\nSo I was wondering if its okay to use n1-standard-2 which costs around $48 and the applications will be in Kubernetes cluster and the pods would connect to Compute Engine VM for DB connection. Would the pods be able to connect to Compute Engine VM?\nAlso is it true that Google doesn't charge GKE Cluster Management Fees when using Zonal Kubernetes cluster? When I check with calculator it shows they don't charge management fees.\n",
  "Answer": "This is entirely up to your needs. If you want to be on call for DB failover and replication management, it will definitely be cheaper to run it yourself. Zalando has a lot of Postgres-on-Kubernetes automation that is very good, but at the end of the day who do you want waking up at 2AM if something breaks. I will never run another production SQL database myself as long as I live, it's just always worth the money.\n"
},
{
  "Question": "I've deployed Kubernetes cluster on my local machine.The default allocatable pods in Kubernetes are 110. I want to increase the number of pods per node in my cluster.Can anyone let me know if it's possible ? If yes, how can we do it?\n",
  "Answer": "Yes, you can control this with the max-pods option to the Kubelet, either via a command line flag or Kubelet config file option. But beware that we don't test as much outside the normal scaling targets so you might be able to break things.\n"
},
{
  "Question": "I am trying to mount S3 bucket using s3fs-fuse to the Kubernetes pod. My S3 bucket is protected by IAM roles and i dont have Access Keys and Secret Keys to access S3 bucket. I know how to access a S3bucket from the Kubernetes pod using Access & Secrets Keys, but how do we access S3 bucket using IAM roles ?\nDoes anyone has suggestion on doing this ?\n",
  "Answer": "You use the IRSA system, attaching an IAM role to a Kubernetes service account and then attaching that K8s SA to your pod. See https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html for a starting point.\n"
},
{
  "Question": "I have a kubectl installed on the my local system. I connect to K8S running on GCP. Where does my k8s master server run ? On my local machine or on GCP?\n",
  "Answer": "In GCP. If you mean GKE specifically, it is running on special magic internal GKE systems you can't see directly.\n"
},
{
  "Question": "I have two use cases where teams only want Pod A to end up on a Node where Pod B is running.  They often have many Copies of Pod B running on a Node, but they only want one copy of Pod A running on that same Node.  \nCurrently they are using daemonsets to manage Pod A, which is not effective because then Pod A ends up on a lot of nodes where Pod B is not running.  I would prefer not to restrict the nodes they can end up on with labels because that would limit the Node capacity for Pod B (ie- if we have 100 nodes and 20 are labeled, then Pod B's possible capacity is only 20).\nIn short, how can I ensure that one copy of Pod A runs on any Node with at least one copy of Pod B running?\n",
  "Answer": "The current scheduler doesn\u2019t really have anything like this. You would need to write something yourself.\n"
},
{
  "Question": "I have EKS cluster where my application code resides in pods (container).\nClient want this cluster to be hosted on their AWS cloud. How do I make sure that my code will be secure in client's environment. How do I make sure that he cannot copy or has no access to the code?\n",
  "Answer": "You can't. At most you can compile and obfuscate it with whatever tools your language provides. This is generally pointless though, decompilers are very very good these days.\n"
},
{
  "Question": "I want to know if the apiserver_request_duration_seconds accounts the time needed to transfer the request (and/or response) from the clients (e.g. kubelets) to the server (and vice-versa) or it is just the time needed to process the request internally (apiserver + etcd) and no communication time is accounted for ?\nAs a plus, I also want to know where this metric is updated in the apiserver's HTTP handler chains ?\n",
  "Answer": "How long API requests are taking to run. Whole thing, from when it starts the HTTP handler to when it returns a response.\n"
},
{
  "Question": "I am fairly new to kubernetes and learning kubernetes deployments from scratch. For a microservice based projecct that I am working on, each microservice has to authenticate with their own client-id and client-secret to the auth server, before requesting any information (JWT). These ids and secrets are required for each services and needs to be in their environment variables. Initially the auth service will generate those ids and secrets via database seeds. What is the best way in the world of kubernetes to automatically set this values in the environments of a pod deployment before pod creation?\n",
  "Answer": "Depends on how automatic you want it to be. A simple approach would be an initContainer to provision a new token, put that in a shared volume file, and then an entrypoint script in the main container which reads the file and sets the env var.\nThe problem with that is authenticating the initContainer is hard. The big hammer solution would be to write a custom operator to manage this but if you're new to Kubernetes that's going to be super hard and probably overkill anyway.\n"
},
{
  "Question": "I am trying to configure kubernetes cluster but as per blog it's telling me to disable SELinux.\nIs there any specific reason for it?\n",
  "Answer": "In theory you could write all the needed policies for it to work. But the Selinux subsystem doesn\u2019t really understand namespaces and this only barely understands containers. So if you\u2019re already running a minimalist host OS it gets you very little added security for a great deal of complexity, most people skip it.\n"
},
{
  "Question": "For multi tenancy support, I would like to create like a datastore concept limiting the storage per tenant across all namespaces in the tenant. Kubernetes website [1] says \"given namespace\", but I want to be able to set resource quota based on storage class but not limited to \"per namespace\". How can I do that ?\n",
  "Answer": "Not directly. The quota system is per namespace only. But of course you can make a validating webhook yourself which implements whatever logic you want.\n"
},
{
  "Question": "What would be the behavior of a multi node kubernetes cluster if it only has a single master node and if the node goes down?\n",
  "Answer": "The control plane would be unavailable. Existing pods would continue to run, however calls to the API wouldn't work, so you wouldn't be able to make any changes to the state of the system. Additionally self-repair systems like pods being restarted on failure would not happen since that functionality lives in the control plane as well.\n"
},
{
  "Question": "I am trying to ensure that a pod is deleted before proceeding with another Kubernetes Operation. So the idea I have is to Call the Pod Delete Function and then call the Pod Get Function.\n// Delete Pod\nerr := kubeClient.CoreV1().Pods(tr.namespace).Delete(podName, &metav1.DeleteOptions{})\n\nif err != nil {\n   ....\n}\n\npod, err := kubeClient.CoreV1().Pods(tr.namespace).Get(podName, &metav1.DeleteOptions{})\n\n// What do I look for to confirm that the pod has been deleted?\n\n\n",
  "Answer": "err != nil && errors.IsNotFound(err)\nAlso this is silly and you shouldn't do it.\n"
},
{
  "Question": "My team is using the Python3 kubernetes package in some of our code. What are the exceptions that can be raised by a call to kubernetes.config.load_kube_config?\n",
  "Answer": "On top of the standard errors that can be raised at any point (MemoryError, OSError, KeyboardInterrupt, etc), it mostly uses its own ConfigException class. Just go read the code for yourself https://github.com/kubernetes-client/python-base/blob/master/config/kube_config.py\n"
},
{
  "Question": "I'm looking a way to call /test endpoint of my service on all the pods I could have (essentially I have only 3 pods).\nIt's is that possible?\nI tried a Cloud Function that calls my balanced ip (https://10.10.10.10:1111/test) however this will send the request to only one pod i.e Pod1 therefore Pod2 and Pod3 don't execute /test and I need the request to be executed on all 3 pods.\nIt doesn't matter what this /test does I just need to be executed on all pods.\nAny hint to achieve this would be awesome.\n",
  "Answer": "There is no specific way, you'll have to use the Kubernetes API to get all the pod IPs and make a request to each individually. This is usually a sign of some wonky software design though.\n"
},
{
  "Question": "Let's say we have 2 Nodes in a cluster.\nNode A has 1 replica of a pod, Node B has 2 replicas. According to this talk (YouTube video with a time tag) from Google Cloud engineers, a request which was routed to Node A might be rerouted to the Node B by iptables which is inside the Node A. I have several questions regarding this behavior:\n\nWhat information iptables of Node A knows about replicas of a pod outside of it? How does it know where to send the traffic?\nCan it be that iptables of the Node B reroutes this request to Node C? If so, then will the egress traffic go back to the Node B -> Node A -> Client?\n\n",
  "Answer": "I think you might be mixing up two subsystems, service proxies and CNI. CNI is first, it\u2019s a plug-in based system that sets up the routing rules across all your nodes so that the network appears flat. A pod IP will work like normal from any node. Exactly how that happens varies by plugin, Calico uses BGP between the nodes. Then there\u2019s the service proxies, usually implemented using iptables though also somewhat pluggable. Those define the service IP -> endpoint IP (read: pod IP) load balancing. But the actual routing is handled by whatever your CNI plugin set up. There\u2019s a lot of special modes and cases but that\u2019s the basic overview.\n"
},
{
  "Question": "I'm trying to build a basic frontend-backend topology in Kubernetes (Amazon EKS actually) with both frontend and backend pods residing on the same node. I want every node to have 2 interfaces: public one, that will connect to internet gateway, and private one, that won't. So it would seem natural to somehow map frontend pods (or service) to the public interface to route traffic to/from the internet and map backend pods to private interface to prevent any external access to them. Is it even possible in Kubernetes? I know that I probably should use public interfaces everywhere and resrict access with ACLs but design with different interfaces looks simplier and more secure to me.\n",
  "Answer": "This is not usually how things work in Kubernetes. Pod IPs would always be \"private\", i.e. cluster IPs that are not used with the internet. You poke specific holes into the cluster IP space using LoadBalancer-type Services. In AWS terms, all pods have private IPs and you use ELBs to bridge specific things to the public network.\n"
},
{
  "Question": "I am exercising on K8S, and I need to share some data between my containers inside a POD, the problem I have is that I need to make containers have available some data from other containers that is already present on that containers at startup. Let me show an example of what I mean:\nContainer A at startup:\n/root/dir/a.txt\nContainer B at startup /root/dirB/b.txt\nIn container C I want to have a directory that contains both a.txt file and b.txt file without doing any operation such as writing etc. But just using volumes.\nHow can I do it?\nThank you in advance\n",
  "Answer": "Make a emptyDir volume, mount it at /newroot on both A and B with both of those set as initContainers which will run command: [bash, -c, \"cp /root/dir/a.txt /newroot/dir/a.txt] and similar for B. On C mount that emptyDir using subPath on either /root/dir or the whole path as needed.\n"
},
{
  "Question": "I have been trying to install Python in minikube of the below version\nLinux minikube 4.19.107 #1 SMP Thu May 28 15:07:17 PDT 2020 x86_64 GNU/Linux\n\nHowever i havent been able to find out a installation package that availabe in this O.S in the minikube.\nMy objective is to install python on minikube so that i could use ansible from my local machine to deploy things into minikube.Please guide.\n",
  "Answer": "Minikube is a dedicated application appliance. It is only for running Kubernetes. You would not install things on it via Ansible. You can use Ansible to automate Kubernetes, but you don't change anything on the host itself, that's just talking to the API.\n"
},
{
  "Question": "After making 2 replicas of PostgreSQL StatefulSet pods in k8s, are the the same database?\nIf they do, why I created DB and user in one pod, and can not find the value in the other.\nIf they not, is there no point of creating replicas?\n",
  "Answer": "There isn't one simple answer here, it depends on how you configured things. Postgres doesn't support multiple instances sharing the same underlying volume without massive corruption so if you did set things up that way, it's definitely a mistake. More common would be to use the volumeClaimTemplate system so each pod gets its own distinct storage. Then you set up Postgres streaming replication yourself.\nOr look at using an operator which handles that setup (and probably more) for you.\n"
},
{
  "Question": "I am trying to create k8s cluster. Is it necessary to establish ssh connection between hosts ?\nIf so, should we make them passwordless ssh enabled ?\n",
  "Answer": "Kubernetes does not use SSH that I know of. It's possible your deployer tool could require it, but I don't know of any that works that way. It's generally recommended you have some process for logging in to the underlying machines in case you need to debug very low-level failures, but this is usually very rare. For my team, we need to log in to a node about once every month or two.\n"
},
{
  "Question": "Currently I tried to fetch already rotated logs within the node using --since-time parameter.\nCan anybody suggest what is the command/mechanism to fetch already rotated logs within kubernetes architecture using commands\n",
  "Answer": "You can't. Kubernetes does not store logs for you, it's just providing an API to access what's on disk. For long term storage look at things like Loki, ElasticSearch, Splunk, SumoLogic, etc etc.\n"
},
{
  "Question": "Running k8s 1.6 and in api-server, below is configured:\n--enable-admission-plugins SecurityContextDeny\nIs it possible to disable it for one pod or is there an exclusion list or override for a deployment.\nI need to run a pod with:\n      securityContext:\n        runAsUser: 0\n\nNot able to figure it out, any pointers?\n",
  "Answer": "No, this was a very limited system which is why PodSecurityPolicies were added in 1.8 to be a far more flexible version of the same idea.\n"
},
{
  "Question": "In Docker Compose, when I mount an empty host volume to a location that already has data in the container, than this data is copied to the empty host volume on the first run.\nE.g. if I use the nginx image and mount my empty host volume nginx-config to /etc/nginx in the nginx container then on the first start of the container everything from /etc/nginx is copied to my host volume nginx-config.\nMeanwhile I am using Kubernetes and wondering how that's done in kubernetes? When I mount a empty PersistentVolume to an container at /etc/nginx, nothing is automatically copied to it ):\n",
  "Answer": "You need to use an initContainer, mount the volume on a different path and do the copy explicitly.\n"
},
{
  "Question": "I want to create hundreds of Jobs in kubernetes by its api. Is there any way to do this? I have to create them one by one now. Thanks.\n",
  "Answer": "I mean you have to make 1 API call per object you want to create, but you can certainly write a script for that. Kubernetes does not offer a \"bulk create\" API endpoint if that's what you are asking, or really much of anything for bulk operations. It's a boring old REST API :)\n"
},
{
  "Question": "Is there a way to fire alerts when some run commands over pods like delete, exec, cp?\nand alerts message should include in which namespace, which pod, which command, and who run these commands.\nthanks.\n",
  "Answer": "This isn\u2019t what Prometheus does, it\u2019s about metrics. For most api operations, you can use the audit log to check if they happen and why, but exec requests are complicated since they are opaque to the apiserver. The only tool I know of which can decode and log exec requests is Sysdig Trace and it isn\u2019t supported on all platforms since it needs direct access to the control plane to do it.\n"
},
{
  "Question": "\nHow to use kubectl with system:anonymous account without using impersonation with --as=system:anonymous?\n\nHow can I send requests with kubectl using the system:anonymous account?\nI've tried using the --as= option, but this requires that the default service account has impersonation privileges, which it doesn't by default.\nThe only way I currently can send anonymous requests is by using curl.\n",
  "Answer": "Set up a new configuration context that doesn't specify any authentication information and then use --context whatever. Or just use curl, that's honestly fine too since I'm really hoping this is just to confirm some security settings or similar. If you run kubectl with -v 10000000 (or some other huge number) it will actually show you the equivalent curl command to the request it is making.\n"
},
{
  "Question": "Is there a way to limit the number os replicas per node in Kubernetes? I found some info about Spreading Constraints but didn't understand if this is possible.\nExample: I want to have only 1 replica per node on the cluster.\nI know that K8S automatically balances the replicas spreading across the nodes, but I want to enforce a specific limit in each node.\nIs this possible?\n",
  "Answer": "The scheduler has many ways to just about everything but in the particular case of 1 replica per node you can use a required mode anti-affinity.\n"
},
{
  "Question": "How could I clear existing log of a specific pod?\nSo that I can get all logs since that time with kubectl logs next time.\nThanks!\n",
  "Answer": "You can't, the log rotation is generally implemented in Docker (or sometimes via logrotate on the node host). However you can use kubectl logs --since-time and fill in the time of your last get. If you're trying to build something to iteratively process logs automatically, probably use Fluentd to load them into some kind of database (Kafka is common for this).\n"
},
{
  "Question": "Im getting this error\nError creating: pods \"node-exporter\" is forbidden: unable to validate against any pod security policy: [spec.secur\nityContext.hostNetwork: Invalid value: true: Host network is not allowed to be used spec.securityContext.hostPID: Invalid value: true: Host PID is not allowed to be used spec.contain\ners[0].hostPort: Invalid value: 9100: Host port 9100 is not allowed to be used. Allowed ports: [0-8000]]\n\nBut i checked in another cluster in GCP, its not giving me any issue. Does anyone knows why i'm getting this issue\n",
  "Answer": "node-exporter needs direct access to the node-level network namespace to be able to gather statistics on this. You have a default security policy that blocks this access. You'll need to make a new policy which allows it, and assign that policy to node-exporter.\n"
},
{
  "Question": "when I run kubectl get pods it shows pod existing and ready, but when run kubectl port-forward I get pod not foud error. what's going on here?\n(base):~ zwang$ k get pods -n delivery\nNAME                                  READY   STATUS    RESTARTS   AGE\nscreenshot-history-7f76489574-wntkf   1/1     Running   86         7h18m\n(base):~ zwang$ k port-forward screenshot-history-7f76489574-wntkf 8000:8000\nError from server (NotFound): pods \"screenshot-history-7f76489574-wntkf\" not found\n\n",
  "Answer": "You need to specify the namespace on the port-forward command too. kubectl port-forward -n delivery screenshot-history-7f76489574-wntkf 8000:8000\n"
},
{
  "Question": "I was handed a kubernetes cluster to manage. But in the same node, I can see running docker containers (via docker ps) that I could not able to find/relate in the pods/deployments (via kubectl get pods/deployments).\nI have tried kubectl describe and docker inspect but could not pick out any differentiating parameters.\nHow to differentiate which is which?\n",
  "Answer": "There will be many. At a minimum you'll see all the pod sandbox pause containers which are normally not visible. Plus possibly anything you run directly such as the control plane if not using static pods.\n"
},
{
  "Question": "I have scenario; where I want to redirect to different services at back end based on a query parameter value. I have gone through documents but couldn't find any help there.\nFor example:\nif Path=/example.com/?color=red ---> forward request to service--> RED-COLOR-SERVICE:8080\nif Path=/example.com/?color=blue ---> forward request to service--> BLUE-COLOR-SERVICE:8080\nif Path=/example.com/?color=green ---> forward request to service--> GREEN-COLOR-SERVICE:8080\n\nThanks\n",
  "Answer": "In general no, the Ingress spec only offers routing on hostname and path. Check the annotation features offered by your specific controller, but I don\u2019t know of any for this off the top of my head.\n"
},
{
  "Question": "Prometheus deployed on kubernetes using prometheus operator is eating too much memory and it is at present at ~12G. I see /prometheus/wal directory is at ~12G. I have removed all *.tmp files but that couldn't help. \nUnable to figure out the solution for this problem. Any suggestions ??\n",
  "Answer": "Reduce your retention time or reduce your number of time series.\n"
},
{
  "Question": "I built a promethues for my kubernetes, and it works well now. It can get node and container/pod cpu, memory data, but I don't know how to get the kubernetes CPU Usage in promethues. Because in my application, if pod restart, deployment will not get data before.\n",
  "Answer": "A deployment is only an abstraction within the Kubernetes control plane, the things actually using CPU will all be pods. So you can use something like this container_cpu_usage_seconds_total{namespace=\"mynamespace\", pod_name=~\"mydeployment-.*\"}.\n"
},
{
  "Question": "In Minikube, created many Persistent Volumes and its claims as a practice? Do they reserve disk space on local machine? \nChecked disk usage \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv\nspec:\n  capacity:\n    storage: 100Gi\n   accessModes:\n     - ReadWriteMany\n   storageClassName: shared\n   hostPath:\n     path: /data/config\n\n---\n\n$ kubectl create -f 7e1_pv.yaml\n$ kubectl get pv\n\n\nNow create YAML for Persistent Volume Claim\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\nspec:\n  storageClassName: shared\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage:90Gi\n\n\n$ kubectl create -f 7e2_pvc.yaml\n\n",
  "Answer": "No, it's just a local folder. The size value is ignored.\n"
},
{
  "Question": "I have a case where we use custom Authorization in Kubernetes via a webhook. Once authorized is there any way the user id could propagated on to the metadata or labels or env of a resource in Kubernetes.\nEg - When a user creates a pod, the userid should be available on the request object.\nThe only place where the user data is available is in the events that is available via audit logs.\n",
  "Answer": "You could use a mutating webhook to inject it. The webhook admission request struct has the user identity data and you can patch the incoming object in the admission response. There is nothing off the shelf for that though, you would have to build it yourself.\n"
},
{
  "Question": "The Openshift documentation is absolutely abysmal. I can't find direct documentation for any of the objects that are available.\nI did find a section in the Kubernetes docs that seems to describe the ability to do something like this... \nhttps://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/\nBut it wasn't super clear how this translates to OoenShift, or how to actually implement this IPVS mode for a service. \n",
  "Answer": "Answered on Slack, but short version it is not an option for this user given their situation.\nFor others, IPVS does support this but it is enabled and configured at a global level. A better option is usually a userspace proxy, often via the Ingress system.\n"
},
{
  "Question": "I am seeing a continuous 8 to 15% CPU usage on Rancher related processes while there is not a single cluster being managed by it. Nor is any user interacting with. What explains this high CPU usage when idle? Also, there are several \"rancher-agent\" containers perpetually running and restarting. Which does not look right. There is no Kubernetes cluster running on this machine. This machine (unless Rancher is creating its own single node cluster for whatever reason).\nI am using Rancher 2.3\ndocker stats:\n\ndocker ps:\n\nhtop:\n\n\n",
  "Answer": "I'm not sure I would call 15% \"high\", but Kubernetes has a lot of ongoing stuff even if it looks like the cluster is entirely quiet. Stuff like processing node heartbeats, etcd election traffic, controllers with time-based conditions which have to be processed. K3s probably streamlines that a bit, but 0% CPU usage is not a design goal even in the fork.\n"
},
{
  "Question": "I'm using docker for windows on my local laptop, and I'm trying to mimic a dev installation of kubernetes by using the \"run kubernetes' setting on the same laptop.  One thing that's awkward is the docker registry.  I have a docker registry container running in-cluster that I can push to no problem from the laptop, but when the docker-for-windows kubernetes controller needs to 'pull' an image, I'm not sure how to reference the registry: I've tried referencing the registry using the laptops netbios name, with various DNS suffixes, but it doesn't seem to work.\nIs there a way I can accomplish this?\n",
  "Answer": "You would use the internal cluster DNS, as managed by the Service object you probably created for the registry. All Services are available inside the cluster via $name.$namespace.svc.cluster.local (technically cluster.local is the cluster domain however this is the default and most common value by far).\n"
},
{
  "Question": "I am trying to install fluxctl on my WSL (Ubuntu 18.04). I saw the official recommendation to install on Linux is through snapcraft but WSL flavors in general does not support snap yet. \nI know the other option is to compile from source or download binary. Is there another way to install fluxctl on WSL through a package/application manager? \n",
  "Answer": "You could check if someone had made a PPA but it seems unlikely. Also FWIW they publish Windows binaries too, right next to the Linux ones.\n"
},
{
  "Question": "I've got a Kubernetes cluster with nginx ingress setup for public endpoints. That works great, but I have one service that I don't want to expose to the public, but I do want to expose to people who have vpc access via vpn. The people who will need to access this route will not have kubectl setup, so they can't use port-forward to send it to localhost.\nWhat's the best way to setup ingress for a service that will be restricted to only people on the VPN?\nEdit: thanks for the responses. As a few people guessed I'm running an EKS cluster in AWS.\n",
  "Answer": "It depends a lot on your Ingress Controller and cloud host, but roughly speaking you would probably set up a second copy of your controller using a internal load balancer service rather than a public LB and then set that service and/or ingress to only allow from the IP of the VPN pods.\n"
},
{
  "Question": "There is a Kubernetes cluster that I am not really familiar with. I need to set up backups with Velero. It is possible that velero has been installed on the cluster by someone else. How do I make sure it has or has not been previously installed before I install it?\n",
  "Answer": "kubectl get pods --all-namespaces | grep velero \n\nThat\u2019s an easy place to start at least.\n"
},
{
  "Question": "queries which will allow us  to track kubeevents and get notified if there are any issues with the pods being scheduled or killed..\n",
  "Answer": "YAML is not a scripting language, it is a data markup language like JSON or XML. So no, but perhaps you meant to ask something else?\n"
},
{
  "Question": "I have a dockerfile where I am trying to copy everything in Github to dockerfile and build it as an image. I have a file called config.json which contains sensitive user data such as username and password. This will also be copied. The issue here is, I want this data to be encrypted and passed onto the dockerfile. While the image is being deployed onto kubernetes, I want this data to be decrypted back again. Can anyone please suggest an ideal method of doing this.\n",
  "Answer": "You shouldn't put this in the container image at all. Use a tool like Sealed Secrets, Lockbox, or sops-operator to encrypt the values separately, and then those get decrypted into a Secret object in Kubernetes which you can mount into your container as a volume so the software sees the same config.json file but it's stored externally.\n"
},
{
  "Question": "Today I have started to learn about Kubernetes because I have to use it in a project. When I came to the Service object, I started to learn what is the difference between all the different types of ports that can be specified. I think now I undertand it. \nSpecifically, the port (spec.ports.port) is the port from which the service can be reached inside the cluster, and targetPort (spec.ports.targetPort) is the port that an application in a container is listening to. \nSo, if the service will always redirect the traffic to the targetPort, why is it allowed to specify them separately? In which situations would it be necessary?    \n",
  "Answer": "The biggest use is with LoadBalancer services where you want to expose something on (usually) 80 or 443, but don't want the process to run as root so it's listening on 8080 or something internally. This lets you map things smoothly.\n"
},
{
  "Question": "What is the purpose of args if one could specify all arguments using command in kubernetes manifest file?\nfor example i can use below syntax which totally negates the usage of the args.\ncommand: [ \"bin/bash\", \"-c\", \"mycommand\" ]\n\nor also\ncommand:\n  - \"bin/bash\"\n  - \"-c\"\n  - \"mycommand\"\n\n",
  "Answer": "The main reason to use args: instead of command: is if the container has a specific entrypoint directive that you don't want to change. For example if the Dockerfile has ENTRYPOINT [\"/myapp\"] you might put args: [--log-level=debug] to add that one argument without changing the path to the binary. In many cases it isn't relevant though and you just use command: to do it all at once.\n"
},
{
  "Question": "In Kubernetes field selectors are limited to certain fields for each resource Kind. But almost every resource has field selector for name and namespace on metadata\nIf so why there's a need to have a separate label selector.\nlabels:\n{\n  app: foo\n}\n\nInstead of querying kubectl get pods -l app=foo, why couldn't it be part of generic field selector like:\nkubectl get pods --field-selector metadata.labels.app=foo \n",
  "Answer": "Short answer: because etcd is not optimized for general purpose querying and so Kubernetes has to pick and choose what to index and what not to. This is why both labels and annotations exist despite seeming very similar, labels are indexed for searching on and annotations are not.\n"
},
{
  "Question": "I have application run inside the kuberentes pod that update the user configuration file and on every deployment it flush the data, as the file reside in a folder which cann't be mounted so I created the empty configmap to mount that file as configmap with subpath mounting and also set the defaultmode of file 777 but still my application is unable to update the content of the file.\nIs there way I can mount a file with read/write permission enable for all user so my application can update the file at runtime.\n",
  "Answer": "No, a configmap mount is read-only since you need to go through the API to update things. If you just want scratch storage that is temporary you can use an emptyDir volume but it sounds like you want this to stick around so check out the docs on persistent volumes (https://kubernetes.io/docs/concepts/storage/persistent-volumes/). There's a lot of options and complexity, you'll need to work out what is the best match for your use case.\n"
},
{
  "Question": "I have network policy created and implemented as per https://github.com/ahmetb/kubernetes-network-policy-recipes, and its working fidn , however I would like to understand how exactly this gets implemeneted in the back end , how does network policy allow or deny traffic , by modifying the iptables ? which kubernetes componenets are involved in implementing this ?\n",
  "Answer": "\"It depends\". It's up to whatever controller actually does the setup, which is usually (but not always) part of your CNI plugin.\nThe most common implementation is Calico's Felix daemon, which supports several backends, but iptables is a common one. Other plugins use eBPF network programs or other firewall subsystems to similar effect.\n"
},
{
  "Question": "I need to connect a service running in a local container inside Docker on my machine to a database that's running on a Kubernetes cluster.\nEverything I found on port forwarding allowed me to connect my machine to the cluster, but not the local container to the cluster (unless I install kubectl on my container, which I cannot do).\nIs there a way to do this?\n",
  "Answer": "https://www.telepresence.io/ is what you're looking for. It will hook into the cluster network like a VPN and patch the services so traffic will get routed through the tunnel.\n"
},
{
  "Question": "I am testing connecting an application running in an external docker container, to a database running in a separate kubernetes cluster. What is the best way to make this connection with security practices in mind.\nI am planning on creating an ingress service for the database in the kubernetes cluster. Then, when making the connection from the application, I should only need to add the ingress/service connection to be able to use this db, right?\n",
  "Answer": "Just like anything else, use TLS, make sure all hops are encrypted and verified. Unless your database of choice uses an HTTP-based protocol, Ingress won't help you. So usually this means setting up TLS at the DB level and exposing it with a LoadBalancer service.\n"
},
{
  "Question": "How to run a helm hook based on a condition.\nWhat I want to solve: \nI have created a postupgrade hook which will load some data to service A, which is created from a zipped subchart.\nRight now it runs every time when an upgrade happened.\nI want it only run when the service A or the job itself has been upgraded.\nIs it possible on helm or k8s level?\n",
  "Answer": "Not really. It doesn't have enough info to know when that is the case.\n"
},
{
  "Question": "In Kubernetes deployments, you can specify volume mounts as readonly. Is there a performance advantage to it, or logical only?\nIs it dependant on the volume type?\nTo make my intentions clear, I'm using a pv in a scenario where I have one writer and many readers, and noticed any fs operation on the mounted volume is much slower than on the volatile disk.\n",
  "Answer": "It entirely depends on the volume type. Some might implement performance optimizations when they know the volume is read only.\n"
},
{
  "Question": "I'm reading through:\nhttps://github.com/kubernetes/ingress-nginx/issues/8\nhttps://github.com/kubernetes/kubernetes/issues/41881\nBut I can't seem to determine a conclusive answer. Does Kubernetes support wildcard domains in it's ingress or not? If not, what are the possible workaround approaches?\nAt least for V1.18 it seems to be officially suported - though still dependent on the ingress controllers also supporting it. (https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/) - though I still want to know about recent previous versions as well.\n",
  "Answer": "As you said it\u2019s up to each individual controller but all of them do that I know of.\n"
},
{
  "Question": "Kubernetes documentation describes pod as a wrapper around one or more containers. containers running inside of a pod share a set of namespaces (e.g. network) which makes me think namespaces are nested (I kind doubt that). What is the wrapper here from container runtime's perspective?\nSince containers are just processes constrained by namespaces, Cgroups e.g. Perhaps, pod is just the first container launched by Kubelet and the rest of containers are started and grouped by namespaces.\n",
  "Answer": "The main difference is networking, the network namespace is shared by all containers in the same Pod. Optionally, the process (pid) namespace can also be shared. That means containers in the same Pod all see the same localhost network (which is otherwise hidden from everything else, like normal for localhost) and optionally can send signals to processes in other containers.\nThe idea is the Pods are groups of related containers, not really a wrapper per se but a set of containers that should always deploy together for whatever reason. Usually that's a primary container and then some sidecars providing support services (mesh routing, log collection, etc).\n"
},
{
  "Question": "I have more kubernetes cluster and use different CNI plugin. \nWhen I coding a CMDB agent ,I want used kubernetes apiserver get CNI plugin, next write in to my CMDB database.\nI used Go languages.\n",
  "Answer": "This isn't really a thing. You would have to write a separate detection mode for each CNI plugin. Additionally it's probably possible (if inadvisable) to have multiple plugins active on the same node as long as only one tries to configure each pod.\n"
},
{
  "Question": "When I was labeling my worker nodes I accidentally labeled my master node as a worker.\nkubectl label node node01 node-role.kubernetes.io/worker=worker\nnow my cluster looks like this:\nNAME     STATUS   ROLES           AGE   VERSION\nnode01   Ready    master,worker   54m   v1.18.0\nnode02   Ready    worker          52m   v1.18.0\nnode03   Ready    worker          51m   v1.18.0\nnode04   Ready    worker          51m   v1.18.0\n\nHow do I remove worker from my Master node?\n",
  "Answer": "kubectl label node node01 node-role.kubernetes.io/worker-. The - tells it to remove the label.\n"
},
{
  "Question": "I'm using k8s java client and need a way to get OAuth access token for some clusters. Now I can do that only with this bash script:\nexport KUBECONFIG=~/.kube/<config-file>\n\nAPISERVER=$(kubectl config view --minify | grep server | cut -f 2- -d \":\" | tr -d \" \")\nSECRET_NAME=$(kubectl get secrets | grep ^default | cut -f1 -d ' ')\nTOKEN=$(kubectl describe secret $SECRET_NAME | grep -E '^token' | cut -f2 -d':' | tr -d \" \")\n\necho \"TOKEN: ${TOKEN}\"\n\nIs there a way to do that with java code? Don't ask for the whole solution but at least for some direction to look.\n",
  "Answer": "Kubernetes is not involved in the OAuth side of things at all. That\u2019s up to your IdP. More normally you would use a ServiceAccount token for automation though.\n"
},
{
  "Question": "So I have two hypervisors running the following Kubernetes VMs:\nA) 1x K8s master, 1x k8s node\nB) 1x K8s node\nIf hypervisor B goes offline, all pods still work, as designed. What happens to the cluster and the nodes when hypervisor A goes offline? Will all running pods on the hypervisor B K8s node still work, assuming I have node anti-affinity configured so that on every node at least one pod already runs?\nThanks!\n",
  "Answer": "Pods will keep running and will restart if they crash but the API will not be available so it will not be possible to run anything new or change them.\n"
},
{
  "Question": "I run kubectl delete with --all flag. This command deleted all namespace on my cluster. (I couldn't see any namespace on K8S Dashboard) So How can I recover all these deleted namespace ? \nAnd is it possible to restore data on namespance ?\n\u279c kubectl delete ns --all\nwarning: deleting cluster-scoped resources, not scoped to the provided namespace\nnamespace \"1xx\" deleted\nnamespace \"2xx\" deleted\nnamespace \"3xx\" deleted\nnamespace \"4xx\" deleted\nnamespace \"5xx\" deleted\nnamespace \"6xx\" deleted\nnamespace \"7xx\" deleted\n\n",
  "Answer": "No. Your cluster is probably no longer viable and will need to be restored from backup or rebuilt.\n"
},
{
  "Question": "I have a tiny Kubernetes cluster consisting of just two nodes running on t3a.micro AWS EC2 instances (to save money). \nI have a small web app that I am trying to run in this cluster. I have a single Deployment for this app. This deployment has spec.replicas set to 4.\nWhen I run this Deployment, I noticed that Kubernetes scheduled 3 of its pods in one node and 1 pod in the other node.\nIs it possible to force Kubernetes to schedule at most 2 pods of this Deployment per node? Having 3 instances in the same pod puts me dangerously close to running out of memory in these tiny EC2 instances.\nThanks!\n",
  "Answer": "The correct solution for this would be to set memory requests and limits correctly matching your steady state and burst RAM consumption levels on every pod, then the scheduler will do all this math for you.\nBut for the future and for others, there is a new feature which kind of allows this https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/. It's not an exact match, you can't put a global cap, rather you can require pods be evenly spaced over the cluster subject to maximum skew caps.\n"
},
{
  "Question": "I have a StatefulSet with 2 pods. It has a headless service and each pod has a LoadBalancer service that makes it available to the world. \nLet's say pod names are pod-0 and pod-1. \nIf I want to delete pod-0 but keep pod-1 active, I am not able to do that. \nI tried\nkubectl delete pod pod-0\n\nThis deletes it but then restarts it because StatefulSet replica is set to 2. \nSo I tried\nkubectl delete pod pod-0\nkubectl scale statefulset some-name --replicas=1\n\nThis deletes pod-0, deletes pod-1 and then restarts pod-0. I guess because when replica is set to 1, StatefulSet wants to keep pod-0 active but not pod-1. \nBut how do I keep pod-1 active and delete pod-0?\n",
  "Answer": "This is not supported by the StatefulSet controller. Probably the best you could do is try to create that pod yourself with a sleep shim and maybe you could be faster. But then the sts controller will just be unhappy forever.\n"
},
{
  "Question": "This question is regarding the Kubernetes behavior when request need more memory than allocated to Pod containing php app . If GC is not able to free memory and request need more memory due to which pods becomes unresponsive or request gets timed out , will kube restart the pod in itself ?\nIn case swap memory is given, will kube try to use it before it restart the pod.\n",
  "Answer": "Swap is not supported with Kubernetes and it will refuse to start if active. Request values for pod resources cannot be overcommitted so if a pod requests 1GB and no node has 1GB available (compared to total RAM minus the requests on all pods scheduled to that node) then the pod will remain unscheduled until something changes. Limits can be overcommitted, but Kubernetes is not involved in that process. It just sets the memory limit in the process cgroup. Out of memory handling works like it always does on Linux, if the kernel decides it is out of memory, then it triggers a thing called the oomkiller which ranks processes and kills the worst one.\n"
},
{
  "Question": "I want to create external authentication for Service A, which listens to traffic on port 8080.\nWhat I desire is to have a second container (Service B) running in the same pod as Service A, that intercepts, evaluates and (maybe) forwards the traffic going in on port 8080\n\"Maybe\" means that Service B should filter out every request, that is not authenticated.\nService B would be injected into every service that is getting deployed in order to ensure consistent authorisation and still keep the deployment process simple.\n(How) is this possible?\n",
  "Answer": "Look up Traefiks forward auth mode or nginx\u2019s mod auth request. They both do what you are looking for. Or more generally this kind of thing is called an API gateway and there are many good ones.\n"
},
{
  "Question": "I have a Openshift Route of type :\n- apiVersion: route.openshift.io/v1\n  kind: Route\n  metadata:\n    name: <Route name>\n    labels:\n      app.kubernetes.io/name: <name>\n  spec:\n    host: <hostname>\n    port:\n      targetPort: http\n    tls:\n      termination: passthrough\n    to:\n      kind: Service\n      name: <serviceName>\n\nI want to convert it into a Ingress Object as there are no routes in bare k8s. I couldn't find any reference to tls termination as passthrough in Ingress documentation. Can someone please help me converting this to an Ingress object?\n",
  "Answer": "TLS passthrough is not officially part of the Ingress spec. Some specific ingress controllers support it, usually via non-standard TCP proxy modes. But what you probably want is a LoadBalancer-type service.\n"
},
{
  "Question": "I have set up an nginx ingres that routes traffic to specific deployments based on host. \n\nhost A --> Service A, host B --> Service B\n\nIf I update the config for Deployment A, that update completes in less than 2 seconds. However, my nginx ingress goes down for host A after that and takes 5 to 7 seconds to point to Service A with new pod. \nHow can I reduce this delay? Is there a way to speed up the performance of the nginx ingress so that it points to the new pod as soon as possible ( preferably less than 3 seconds?)\nThank you!\n",
  "Answer": "You can use the nginx.ingress.kubernetes.io/service-upstream annotation to suppress the normal Endpoints behavior and use the Service directly instead. This has better integration with some deployment models but 5-7 seconds is extreme for ingress-nginx to be seeing the Endpoints update. There can be a short gap from when a pod is removed and when ingress-nginx sees the Endpoint removal. You usually fix that with a pre-stop hook that just sleeps for a few seconds to ensure by the time it actually exits, the Endpoint change has been processed everywhere.\n"
},
{
  "Question": "Is there a way to enable caching between pods in Kubernetes cluster? For eg: Lets say we have more than 1 pods running on High availability mode.And we want to share some value between them using distributed caching between the pods.Is this something possible?\n",
  "Answer": "There are some experimental projects to let you reuse the etcd that powers the cluster, but I probably wouldn\u2019t. Just run your own using etcd-operator or something. The specifics will massively depend on what your exact use case and software is, distributed databases are among the most complex things ever.\n"
},
{
  "Question": "I have a home Kubernetes cluster with multiple SSDs attached to one of the nodes.\nI currently have one persistence volume per mounted disk. Is there an easy way to create a persistence volume that can access data from multiple disks? I thought about symlink but it doesn't seem to work.\n",
  "Answer": "You would have to combine them at a lower level. The simplest approach would be Linux LVM but there's a wide range of storage strategies. Kubernetes orchestrates mounting volumes but it's not a storage management solution itself, just the last-mile bits.\n"
},
{
  "Question": "I have an image called http which has a file called httpd-isec.conf. I'd like to edit httpd-isec.conf before the image is started by kubernetes. Is that possible?\nWould initContainers and mounting the image work in some way? \n",
  "Answer": "This is not possible. Images are immutable. What you can do is use an init container to copy the file to an emptyDir volume, edit it, and then mount that volume over the original file in the main container.\n"
},
{
  "Question": "As an example, if I define a CRD of kind: Animal, can I define a CRD, Dog, as a specific type of Animal? The Dog CRD would have a different/extended schema requirement than the base Animal CRD. \nMy goal here is to be able to do a kubectl get animals and be able to see all the different type's of Animals that were created. \nThis seems to have been achieved by using the type metadata for certain resources like Secret, but I can't seem to find how to achieve this with a CRD. \nNote: my real use case isn't around storing Animals, but, it's just a typical example from OOP.\n",
  "Answer": "No, this is not a feature of Kubernetes. All Secret objects are of the same Kind, and Type is just a string field.\n"
},
{
  "Question": "I have an app that requires MS office. This app is docker containerized app and should run on GCP kubernetes cluster. How can i install MS office in a linux docker container?\n",
  "Answer": "Via https://learn.microsoft.com/en-us/dotnet/architecture/modernize-with-azure-containers/modernize-existing-apps-to-cloud-optimized/when-not-to-deploy-to-windows-containers this was not possible as of April 2018.\n"
},
{
  "Question": "The idea is instead of installing these scripts they can instead be applied via yaml perhaps and ran with access to kubectl and host tools to find potential issues with the running environment.  \nI figure the pod would need special elevated permissions, etc.  I'm not quite sure if there is an example or even a better way of accomplishing the same idea.\nIs there a way to package scripts in a container to run for diagnostic purposes against kubernetes?\n",
  "Answer": "It's an Alpha feature and not recommended for production use, but check out the ephemeral containers system: https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/\nIt's designed for exactly this, having a bundle of debugging tools that you can connect in to an existing file/pid namespace. However the feature is still incomplete as it is being added incrementally.\n"
},
{
  "Question": "i'm stack at k8s log storage.we have logs that can't output to stdout,but have to save to dir.we want to save to glusterfs shared dir like /data/logs/./xxx.log our apps are written in java ,how can we do that\n",
  "Answer": "This is mostly up to your CRI plugin, usually Docker command line options. They already do write to local disk by default, you just need to mount your volume at the right place (probably /var/log/containers or similar, look at your Docker config).\n"
},
{
  "Question": "I have created a k8s cluster by installing \"kubelet kubeadm kubectl\". Now i'm trying to Deploy microservice application as\n\ndocker build -t demoserver:1.0 .\n=>image created successfully\n\nkubectl run demoserver --image=demoserver --port=8000 --image-pull-policy=Never\n=>POD STATUS: ErrImageNeverPull\n\n\nI tried \" eval $(minikube docker-env)\" but it says bash: minikube: command not found...\nDo i need to install minikube? Is my above cluster setup is not enough for deployment??\n",
  "Answer": "Minikube and kubeadm are two unrelated tools. Minikube builds a (usually) single node cluster in a local VM for development and learning. Kubeadm is part of how you install Kubernetes in production environments (sometimes, not all installers use it but it's designed to be a reusable core engine).\n"
},
{
  "Question": "I know that k8s has default Hard Eviction Threshold memory.available<100Mi. So k8s should evict pods if thresholds exceed. In these conditions can pod provoke SYSTEM OOM? When I talk about SYSTEM OOM I mean such situation when Linux starts to kill processes randomly (or not almost randomly, doesn't matter). Lets suppose that other processes on node consume constant amount of memory. I hope that k8s watches pods and kills them before the threshold exceeded. Am I right?\n",
  "Answer": "Yes, very yes. Eviction takes time. If the kernel has no memory, oomkiller activates immediately. Also if you set a resources.limits.memory then if you exceed that you get an OOM.\n"
},
{
  "Question": "I have a Windows Server 2019 (v1809) machine with Kubernetes (bundled with Docker Desktop for Windows). I want to enable Vertical Pod Autoscaling for the cluster I have created.\nHowever, all the documentation I can find is either for a cloud service or a Linux-based system. Is it possible to implement VPA for the Docker Desktop Kubernetes in Windows or Minikube?\n",
  "Answer": "While VPA itself is a daemon, the pods it controls are just API objects as far it knows and can be anything on any platform. As for compiling a VPA container for Windows, I wouldn't expect any problems, you'll just need to build it yourself since we don't provide one.\n"
},
{
  "Question": "We tried attaching a shell to container inside \"Traefik\" Pod using following command but it didn't work. Just FYI, we used helm chart to install Traefik on our k8s cluster.  \nkubectl exec -it <traefik Pod name> -- /bin/sh \ntried this too but no success -  kubectl exec -it <traefik Pod name> -- /bin/bash\nAny help in this context will be appreciated. Thanks. \n",
  "Answer": "Traefik 1.7 uses a FROM scratch container image that has only the Traefik executable and some support files. There is no shell. You would have to switch to the -alpine variant of the image. For 2.x it looks like they use Alpine by default for some reason.\n"
},
{
  "Question": "Is it possible to set a liveness probe to check that a separate service is existing? For an app in one pod, and a database in a separate pod, I would like for the app pod to check the liveness of the database pod rather than this pod itself. The reason for this is that once the db is restarted, the app is unable to reconnect to the new database. My idea is to set this so that when the db liveness check fails, the app pod is automatically restarted in order to make a fresh connection to the new db pod.\n",
  "Answer": "No, you would need to write that in a script or as part of your http endpoint.\n"
},
{
  "Question": "In docker host and the containers do have separate process name-space. In case of Kubernetes the containers are wrapped inside a pod. Does that mean in Kubernetes the host (a worker node), the pod in the node and the container in the pod all have separate process namespace?\n",
  "Answer": "Pods don't have anything of their own, they are just a collection of containers. By default, all containers run in their own PID namespace (which is separate from the host), however you can set all the containers in a pod to share the same one. This is also used with Debug Containers.\n"
},
{
  "Question": "I have a few CRDs and each of them supposed to make edit Container.Spec's across the cluster.\nLike ENVs, Labels, etc...\nIs it okay, if the resource is managed by more that one controller? \nWhat are the possible pitfalls of this approach?\n",
  "Answer": "Yes, the same object can be updated by multiple controllers. The Pod object is updated by almost a dozen at this point I think. The main problem you can run into is write conflicts. Generally in an operator you do a get, then some stuff happens, then you do an update (usually to the status subresource for a root object case). This can lead to race conditions. I would recommend looking at using Server Side Apply to reduce these issues, it handle per-field tracking rather than whole objects via serial numbers.\n"
},
{
  "Question": "I am trying to parse a helm chart YAML file using python. The file contains some curly braces, that's why I am unable to parse the YAML file.\na sample YAML file\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: {{ .Values.nginx.name }}-config-map\n  labels:\n    app: {{ .Values.nginx.name }}-config-map\ndata:\n  SERVER_NAME: 12.121.112.12\n  CLIENT_MAX_BODY: 500M\n  READ_TIME_OUT: '500000'\n\nBasically, I could not figure out how to ignore the values present at right side.\nThank you,\n",
  "Answer": "You would have to write an implementation of Go's text/template library in Python. A better option is probably to push your content through helm template first and then parse it.\n"
},
{
  "Question": "Due to customer's requirements, I have to install k8s on two nodes(there's only two available nodes so adding another one is not an option). In this situation, setting up etcd in only one node would cause a problem if that node went down; however, setting up a etcd cluster(etcd for each node) would still not solve the problem since if one node went down, etcd cluster would not be able to achieve the majority vote for the quorum. So I was wondering if it would be possible to override the \"majority rule\" for the quorum so that quorum could just be 1? Or would there be any other way to solve this\n",
  "Answer": "No, you cannot force it to lie to itself. What you see is what you get, two nodes provide the same redundancy as one.\n"
},
{
  "Question": "I'm wondering for a batch distributed job I need to run. Is there a way in K8S if I use a Job/Stateful Set or whatever, a way for the pod itself(via ENV var or whatever) to know its 1 of X pods run for this job?\nI need to chunk up some data and have each process fetch the stuff it needs.\n--\nI guess the statefulset hostname setting is one way of doing it. Is there a better option?\n",
  "Answer": "This is planned but not yet implemented that I know of. You probably want to look into higher order layers like Argo Workflows or Airflow instead for now.\n"
},
{
  "Question": "One of the options to use Kubernetes on Windows 10 is to enable it from Docker for Windows.\nHowever reading many tutorials from K8S site they manage something by using minikube - for example adding addons.\nSince using the option with docker we don't have minikube.\nFor example, how to add addon to such instance?\n",
  "Answer": "You would have to manually grab the addon YAML file and kubectl apply -f it. But most things have Helm charts available too so maybe just do that instead?\n"
},
{
  "Question": "I have been experimenting with blue green deployment in kubernetes using nginx-ingress. I created few concurrent http request to hit v1 version of my  application. In the meanwhile I switched the router to point to v2 version of my application. As expected v2 version was serving the requests after the swap ,but what made me curious is that all the request were success. It is highly probable that there were some in-progress request in  v1 while I made the swap. Why didn't such request fail?\nI also tried the same by introducing some delay in my service so that the request take longer time to process.Still none of the request failed.\n",
  "Answer": "Usually in-flight requests are allowed to complete, just no new requests will be sent by the proxy.\n"
},
{
  "Question": "I am trying to run a process in only ONE docker pod (and not the other n pods),\ncan I know (from inside a pod/instance) \n\nam I the first pod?\nhow many pods are running?\n\nthanks.\n",
  "Answer": "Don't do this. Put that thing in its own deployment (or statefulset more likely) that is unrelated to the others.\n"
},
{
  "Question": "What is the best way to deploy a Helm chart using C#? Helm 3 is written in go and I could not find a C# wrapper, any advise on that? Thank you.\n",
  "Answer": "Helm is written in Go so unless you want to get incredibly fancy your best bet is running it as a subprocess like normal. A medium-fancy solution would be using one of the many Helm operators and then using a C# Kubernetes api client library to set the objects.\n"
},
{
  "Question": "I have four kubernetes clusters, and I want to check the expiration time of them with kubernetes-python-client.\nI am following this page: https://github.com/kubernetes-client/python\nIs there anyone know how to get it?\n",
  "Answer": "The apiserver certificate is generally handled out of band, either by your Kubernetes installer tool (kubeadm, rancher, talos, etc) or off-cluster in a load balancer layer. As such the K8s API won't help you with this.\nThat said, you can get the certificate of any HTTPS server in Python using ssl.get_server_certificate() (https://docs.python.org/3/library/ssl.html#ssl.get_server_certificate) along with other functions in the ssl module to parse the cert data and then look at the Not After timestamp.\n"
},
{
  "Question": "I am trying to run a Redis cluster on Kubernetes. I am not planning to persist any Redis data to the disk. Is it possible to run the Redis cluster as Kubernetes deployment and not as a stateful set?\n",
  "Answer": "Yes, though I would probably still use StatefulSet specifically for the features to ensure only one pod starts at a time.\n"
},
{
  "Question": "Whether the application will be live (In transaction) during the time of POD deployment in AKS?\nWhile we are performing the POD deployment, whether the application transactions will go through (or) get error out?\n",
  "Answer": "The Deployment system does a rolling update. New pods are created with the new template and once Ready they are added to the service load balancer, and then old ones are removed and terminated.\n"
},
{
  "Question": "Ask a question, how to control the usage of each GPU used on each machine in k8s cluster of two machines with two graphics cards on each machine. Now each card has 15g. I want to use 10g + for the other three cards, leaving 7g + free for one graphics card.\n",
  "Answer": "That's not how graphics cards work. the GPU RAM is physically part of the graphics card and is exclusive to that GPU.\n"
},
{
  "Question": "In a Kubernetes operator based on operator-sdk, do you know how to write code to synchronize CR resource when CR specification is updated with kubectl apply? Could you please provide some code samples?\n",
  "Answer": "It is mostly up to how you deploy things. The default skeleton gives you a Kustomize-based deployment structure so kustomize build config/default | kubectl apply -f. This is also wrapped up for you behind make deploy. There is also make install for just installing the generated CRD files.\n"
},
{
  "Question": "In plain nginx, I can use the nginx geo module to set a variable based on the remote address. I can use this variable in the ssl path to choose a different SSL certificate and key for different remote networks accessing the server. This is necessary because the different network environments have different CAs.\nHow can I reproduce this behavior in a Kubernetes nginx ingress? or even Istio?\n",
  "Answer": "You can customize the generated config both for the base and for each Ingress. I'm not familiar with the config you are describing but some mix of the various *-snippet configmap options (https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#server-snippet) or a custom template (https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/custom-template/)\n"
},
{
  "Question": "I have a kustomization.yaml file that uses a private repository as a resource:\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - https://gitlab.com/my-user/k8s-base-cfg.git\npatchesStrategicMerge:\n  - app-patch.yaml\n\nI want to automate this on a Jenkins Pipeline. I don't know how to pass Git credentials to the kustomize build command. Is there any option to do that?\nThank you\n",
  "Answer": "You can't, you would set up the credentials in git before starting Kustomize. In this case probably something very simple like git config --global user.password \"your password\" but look up the credentials.helper setting for more complex options, either from a local file or a tool that reads from some backing store directly.\n"
},
{
  "Question": "I'm using Jenkins with Kubernetes plugin but I think the problem will be the same with Tekton or any pipeline that build, test, and deploy a project using Kubernetes'pods and Gradle.\nIs there a way to share the Gradle daemon process through multiple pods?\nNote that I enabled remote Gradle caches.\n",
  "Answer": "Not easily. The whole model of the Kubernetes plugin is that every build runs in a new environment. You would have to run it outside of the build, probably via a DaemonSet with hostNetwork mode on and then configure Gradle in the build to look at a different IP (the host IP) instead of localhost. \nBasically everyone just copes with --no-daemon mode :-/\n"
},
{
  "Question": "How can i upload Binary file like cert file as Config-map\nI am trying to upload Cert file like .p12 as config map but it's failing every time. After upload i do not see the file just entry.\nCommand that i used:\noc create configmap mmx-cert --from-file=xyz.p12\n\nFailed.\nAlso used: \noc create configmap mmx-cert--from-file=game-special-key=example-files/xyz.p12\n\nAlso failed.\n",
  "Answer": "You cannot, ConfigMaps cannot contain binary data on their own. You will need to encode it yourself and decode it on the other side, usually base64. Or just a Secret instead, which can handle binary data.\n"
},
{
  "Question": "during deployment of new version of application sequentially 4 pods are terminated and replaced by newer ones; but for those ~10minutes the app is hitting other microservice is hitting older endpoints causing 502/404 errors - anyone know of a way to deploy 4 new pods, then drain traffic from old ones to new ones and after all connections to prev ver are terminated, then terminate the old pods ? \n",
  "Answer": "This probably means you don't have a readiness probe set up? Because the default is already to only roll 25% of the pods at once. If you have a readiness probe, this will include waiting until the new pods are actually available and Ready but otherwise it only waits until they start.\n"
},
{
  "Question": "I'm reading the Kubernetes docs on Reserve Compute Resources for System Daemons, and it says \"Be extra careful while enforcing system-reserved reservation since it can lead to critical system services being CPU starved, OOM killed, or unable to fork on the node.\"\nI've seen this warning in a few places, and I'm having a hard time understanding the practical implication.\nCan someone give me a scenario in which enforcing system-reserved reservation would lead to system services being starved, etc, that would NOT happen if I did not enforce it?\n",
  "Answer": "You probably have at least a few things running on the host nodes outside of Kubernetes' view. Like systemd, some hardware stuffs, maybe sshd. Minimal OSes like CoreOS definitely have a lot less, but if you're running on a more stock OS image, you need to leave room for all the other gunk that comes with them. Without leaving RAM set aside, the Kubelet will happily use it all up and then when you go to try and SSH in to debug why your node has gotten really slow and unstable, you won't be able to.\n"
},
{
  "Question": "I am working on moving a application which require near real time exchange of data between processes running in multiple containers in kubernetes cluster.\nI am thinking of using redis cache for this purpose. \nThe type of data that needs to be exchanges are simple types like double,string values. The frequency of exchange needs to near real time(sub seconds)\nare there any other more performant mechanisms available for exchanging data between containers hosted in kubernetes environment?\n",
  "Answer": "This is hugely complex question with way more nuance than can fit in here. It very much depends on the object sizes, uptime requirements, cluster scale, etc. I would recommend you try all of them, evaluate performance, and analyze failure modes as they apply to your use case.\nSome things you can try out:\n\nRedis\nMemcached\nLocal files with mmap\nNetwork block device with mmap\nNFS with mmap\nAll three of the above with RocksDB\nPostgres\nKafka\n\nOn the encodings side evaluate:\n\nJSON (don't use this, just for baseline)\nProtocolBuffers\nCapnProto\nMsgpack\nMaybe BSON?\n\n"
},
{
  "Question": "values.yaml\naa:\n  bb:\n    cc:\n      dd: \"hi~!!\"\n\nIn the values \u200b\u200bfile above, the value \"cc\" is a variable.\nI'm want to get \"hi~!!\".\nmyPod.yaml\napiVersion: v1\n...\n...\ndata:\n  myData: {{ printf \"%s\\.dd\" $variableKey | index .Values.aa.bb }}\n\nIs this possible?\n",
  "Answer": "You need two separate args, {{ index .Values.aa.bb $variableKey \u201cdd\u201d }}\n"
},
{
  "Question": "Does anyone know how to get a Kubernetes deployment to automatically update when a configMap is changed?\n",
  "Answer": "Unfortunately there is nothing built in for this. You used the helm tag, so with Helm you do this by setting a checksum of the rendered configmap (or secret, same issue there) as an annotation in the pod template. This means that changing the configmap results in a (meaningless) change to the pod template, which triggers a rolling update.\n"
},
{
  "Question": "I want to load-balance 2 stateful applications running on 2 pods. This application cannot have 2 replicas as it is stateful.  \nI tried giving the same service names to both the pods but it looks like Kubernetes get confused and nothing is served.\nI am using on-premies Kubernetes cluster with metallb as a load-balancer.\nCurrently, these pods are exposed over public IP with service TYPE as a load-balancer and added A record to both the pods. But it cannot do a health check with DNS.\nI only think of having Nginx pod and do mod-proxy to it. Is there any better solution other than this?\n",
  "Answer": "The selector on a service can be anything, and can match pods from multiple statefulsets (or deployments). So make a label on your pods and use that in the selector of a new service to target both.\n"
},
{
  "Question": "If a distributed computing framework spins up nodes for running   Java/ Scala operations then it has to include the JVM in every container. E.g. every Map and Reduce step spawns its own JVM.\nHow does the efficiency of this instantiation compare to spinning up containers for languages like Python? Is it a question of milliseconds, few seconds, 30 seconds? Does this cost add up in frameworks like Kubernetes where you need to spin up many containers?\nI've heard that, much like Alpine Linux is just a few MB, there are stripped down JVMs, but still, there must be a cost. Yet, Scala is the first class citizen in Spark and MR is written in Java.\n",
  "Answer": "Linux container technology uses layered filesystems so bigger container images don't generally have a ton of runtime overhead, though you do have to download the image the first time it is used on a node which can potentially add up on truly massive clusters. In general this is not usually a thing to worry about, aside from the well known issues of most JVMs being a bit slow to start up. Spark, however, does not spin up a new container for every operation as you describe. It creates a set of executor containers (pods) which are used for the whole Spark execution run.\n"
},
{
  "Question": "I am fairly new to Kubernetes and had a question concerning kube-state-metrics. When I simply monitor Kubernetes using Prometheus I obtain a set of metrics from the cAdvisor, the nodes (node exporter), the pods, etc. When I include the kube-state-metrics, I seem to obtain more \"relevant\" metrics. Do kube-state-metrics allow to scrape \"new\" information from Kubernetes or are they rather \"formatted\" metrics using the initial Kubernetes metrics (from the nodes, etc. I mentioned earlier). \n",
  "Answer": "The two are basically unrelated. Cadvisor is giving you low-level stats about the containers like how much RAM and CPU they are using. KSM gives you info from the Kubernetes API like the Pod object status. Both are useful for different things and you probably want both.\n"
},
{
  "Question": "My namespace contains multiple secrets and pods. The secrets are selectively mounted on pods as volumes using the deployment spec. Is it possible to deny specific secrets from being mounted as volumes in certain pods. I have tested RBAC and it prevents pods from accessing secrets over api. Is there a similar mechanism for mounted secrets considering that there is a security risk in allowing all secrets to be mounted in pods in the same namespace.\n",
  "Answer": "The other answer is the correct one but in the interest of completeness, you could write an admission controller which checks requests against some kind of policy. This is what the built in NodeRestriction admission controller does to help limit things so the kubelet can only access secrets for pods it is supposed to be running. \n"
},
{
  "Question": "I have a service that generates a picture. Once it's ready, the user will be able to download it.\nWhat is the recommended way to share a storage volume between a worker pod and a backend service?\n",
  "Answer": "In general the recommended way is \"don't\". While a few volume providers support multi-mounting, it's very hard to do that in a way that isn't sadmaking. Preferably use an external services like AWS S3 for hosting the actual file content and store references in your existing database(s). If you need a local equivalent, check out Minio for simple cases.\n"
},
{
  "Question": "I have a kustomization file that's generating a ConfigMap and behaving as expected. I need to be able to create a new pod that pulls in the environment variables from that same configMap without regenerating the configMap.\nIn other words, I'm having to do this:\nenvFrom:\n    - configMapRef:\n        name: config-name-HASH\n\nbut I want to do this:\nenvFrom:\n    - configMapRef:\n        name: config-name\n\nwithout needing to regenerate the ConfigMap with kustomize. \n I've found PodPresets which would seem to be the fix, but that's in Alpha, so it's not good for my organization.\n",
  "Answer": "That is not possible. While ConfigMap volumes update in-place and automatically (so you could switch that and make your app re-read the file when it updates), env vars pulled from a ConfigMap (or Secret, all of this applies to both) are only checked when the pod is launched. The usual workaround is to put a checksum or generation ID of your configmap as an annotation in the pod template which will automatically trigger a rolling update through the Deployment, StatefulSet, or DaemonSet controllers.\n"
},
{
  "Question": "I came across an article which States that we can have mixed os in cluster.\nArticle talk about having flannel as networking plugin but i want to use Calico opensource plugin instead as it provides encryption.\nAny idea if this is possible using Calico opensource?\n",
  "Answer": "Calico for Windows does exist https://www.tigera.io/tigera-products/calico-for-windows/\nBut it appears to be a commercial product so you would probably want to contact them to ask about it. Assuming it's equivalent to normal Calico, I don't see any reason it wouldn't work. BGP and IPIP are both standardized protocols that aren't specific to any OS.\n"
},
{
  "Question": "We got an existed secret in K8S(suppose it is \"secret_1\") and we want to write a yaml to create a new secret \"secret_2\", using some values from secret_1.\nThat is, in this yaml we'd like to \n\nRead values from other secret\nStore values to new secret\n\nIs it possible to do this? It will be great help if a sample can be provided.\nThanks in advance.\n",
  "Answer": "You cannot do this directly in YAML. You would need to write a script of some kind to do the steps you described, though you can use kubectl get secret -o yaml (or -o json) for a lot of the heavy lifting, possibly with jq for the reformatting.\n"
},
{
  "Question": "It's possible to make an Ingress Controller, or anything else (preferably something already done, not needing to code a service per say), to send traffic to an external IP?\nWhy: I have an application which will interact with my k8s cluster from the outside, I already know that I can use an Ingress Controller to make its connection to the cluster, but what if the other applications need to reach this external application? Is there a way to do this?\n",
  "Answer": "It depends on the controller, but most will work with an ExternalName type Service to proxy to an arbitrary IP even if that's outside the cluster.\n"
},
{
  "Question": "I have a requirements.yaml file:\ndependencies:\n  - name: mongodb-replicaset\n    # Can be found with \"helm search <chart>\"\n    version: 3.13.0\n    # This is the binaries repository, as documented in the GitHub repo\n    repository: https://kubernetes-charts.storage.googleapis.com/\n\n\nAnd i want to modify the values.yaml file of the mongodb-replicaset chart , espacialy this section:\nauth:\n\n  enabled: false\n  existingKeySecret: \"\"\n  existingAdminSecret: \"\"\n  existingMetricsSecret: \"\"\n  # adminUser: username\n  # adminPassword: password\n  # metricsUser: metrics\n  # metricsPassword: password\n  # key: keycontent\n\n\nHow can i override the values.yaml file on initialization in a dependency chart?\n",
  "Answer": "You put the values under a key matching the name of the upstream chart so\nmongodb-replicaset:\n  auth:\n    enabled: true\n    etc etc\n\n"
},
{
  "Question": "I am running filebeat as deamon set with 1Gi memory setting. my pods getting crashed with OOMKilled status.\nHere is my limit setting \n resources:\n          limits:\n            memory: 1Gi\n          requests:\n            cpu: 100m\n            memory: 1Gi\n\nWhat is the recommended memory setting to run the filebeat.\nThanks\n",
  "Answer": "The RAM usage of Filebeat is relative to how much it is doing, in general. You can limit the number of harvesters to try and reduce things, but overall you just need to run it uncapped and measure what the normal usage is for your use case and scenario.\n"
},
{
  "Question": "I want to create a symlink using a kubernetes deployment yaml. Is this possible?\nThanks\n",
  "Answer": "Not really but you could set your command to something like [/bin/sh, -c, \"ln -s whatever whatever && exec originalcommand\"]. Kubernetes isn't involved per se, but it would probably do the job. Normally that should be part of your image build process, not a deployment-time thing.\n"
},
{
  "Question": "With the instruction https://docs.aws.amazon.com/eks/latest/userguide/worker.html it is possible to bring up Kube cluster worker nodes. I wanted the worker node not to have public ip. I don't see Amazon gives me that option as when running the cloudformation script. How can I have option not to have public ip on worker nodes\n",
  "Answer": "You would normally set this up ahead of time in the Subnet rather than doing it per machine. You can set Auto-assign public IPv4 address to false in the subnets you are using the for the worker instances.\n"
},
{
  "Question": "I would like to mount an amazon ebs volume (with data on it) to my pod. The problem is that I didn't find a way to determine in advance the availability zone of the pod before starting it. If the pod doesn't start on the same availability zone of the volume, it leads to a binding error.\nHow can I specify or determine the availability zone of a pod before starting it?\n",
  "Answer": "You use the topology.kubernetes.io/zone label and node selectors for this kind of thing. However unless you're on a very old version of Kubernetes, this should be handled automatically by the scheduler.\n"
},
{
  "Question": "Dnsjava is an implementation of DNS in Java.\nWe have built some of our application logic around it..\nJust wanted to check if Kubernetes would support DNS interfaces at application level\n",
  "Answer": "Not entirely sure what you mean, but Kubernetes doesn't care what you run on it. Your workloads are your problem :)\n"
},
{
  "Question": "I am looking for keeping some kind of baseline for everything applied to kubernetes(or just a namespace).\nLike versioning microservices in a list and then check that in to github, in case of the need to roll something back.\n",
  "Answer": "Check out Velero, it is a backup tool for kubernetes. I don\u2019t think it can use git as a backend, but you could add that (or use s3 or similar).\n"
},
{
  "Question": "I am new to K8s. Say I want to start up a RabbitMQ in my cluster but I also want to ensure its default AMQP port is secure (AMQPS). Is it possible to do so using a GCP-managed key + certificate? If so, how? For example, I was thinking of using a LoadBalancer somehow to take care of it. Or, maybe Ingress, although it's not HTTP-based traffic (still, maybe we can work around this?)\nThanks\n",
  "Answer": "I don\u2019t think so, all the ways you can interact with Google certs are aimed at HTTPS. You can use cert-manager with LetsEncrypt though.\n"
},
{
  "Question": "My Traefik Ingress DaemonSet shows some awkard metrics in its dashboard. \nIs it correct? I really doubt that my average response time is beyond minutes.\nI think I'm doing something wrong but I have no idea what it is.\n\n",
  "Answer": "Answered in comments, Traefik's stats are very literal and when using Websockets it thinks that's a single HTTP connect (because it technically is) which is lasting for minutes or hours.\n"
},
{
  "Question": "I need this information to measure mean time to recovery (MTTR). I have tried using different kube-state-metrics but it does not seem to help much. Any hints on measuring MTTR will also be appreciated\n",
  "Answer": "You can use the pod status information, it records last transition time for each status signal. In this case you probably want the time difference between PodScheduled and Ready, but up to you to decide what counts as \"startup time\" or not (for example, does the time spent on pulling container images count?).\n"
},
{
  "Question": "Due to some internal issues, we need to remove unused images as soon as they become unused.\nI do know it's possible to use Garbage collection but it doesn't offer strict policy as we need. \nI've come across this solution but\n\nit's deprecated\nit also removes containers and possible mounted volumes\n\nI was thinking about setting a cron job directly over the nodes to run docker prune but I hope there is a better way\nNo idea if it makes a difference but we are using AKS\n",
  "Answer": "This doesn't really accomplish much since things will be re-downloaded if they are requested again. But if you insist on a silly thing, best bet is a DaemonSet that runs with the host docker control socket hostPath-mounted in and runs docker system prune as you mentioned. You can't use a cron job so you need to write the loop yourself, probably just bash -c 'while true; do docker system prune && sleep 3600; done' or something.\n"
},
{
  "Question": "I've finally managed to run my containers and let them communicate. Currently, they're 1-1 (1 frontend, 1 backend). Now I wish to have n instances of frontend and m instances of the backend, but a question came to me, about handling the logs. If I run only 1 instance of each, I can configure 2 volumes (1 for frontend and 1 for backend) and have them write there. When I have the containers orchestrated by Kubernetes, how can I set the volumes so that node1 of frontend wont' overwrite data written by node2 (frontend)\nThanks\n",
  "Answer": "You don't write logs to a volume, generally. You write them to stdout/err and then the container runtime system manages them for you. You can then access them via kubectl logs or ship them elsewhere using tools like Fluentd.\n"
},
{
  "Question": "I am trying to port a monolithic app to k8s pods. In theory, pods are considered ephemeral and it is suggested to use service concept to provide a static IP. But in my test so far, I have not seen the POD IP being changed. So now the question when will k8s assign a new IP to my POD?\nI have created POD (without using any controller) with fixed hostname and they are bounded to a single node. So the node and the hostname will never change and the POD will never be deleted? So in this unique case, when can the POD IP change. I looked at the documentation and this is not clear to me.\n",
  "Answer": "The IP won't change as long as the pod is running, but there are no promises that your pod will stay running. The closest there is to a stable network name is with a StatefulSet. That will create a consistent pod name, which means a consistent DNS name in kubedns/coredns. There is no generic way in Kubernetes to get long-term static IP on a pod (or on a service for that matter), though it's technically up to your CNI networking plugin so maybe some of those have special cases?\n"
},
{
  "Question": "I need to disable interactive session/ssh access to a Kubernetes pod.\n",
  "Answer": "It\u2019s controlled via the RBAC system, via the pods/exec subresource. You can set up your policies however you want.\n"
},
{
  "Question": "I have an application that has 14 different services. Some of the services are dependent on other services. I am trying to find a good way to deploy these in the right sequences without using thread sleeps. \n\nIs there a way to tell kuberentes a service deployment tree like don't deploy service B or service C until Service A is in a container and the status is running?\\\nIs there s good way to use kubectl to poll service A so I can do a while loop until I know it's up and running then run the scripts to deploy service B and C? \n\n",
  "Answer": "This is not how Kubernetes works. You can kind of shim it with an initContainer that blocks until dependencies are available (usually via kubectl in a while loop, but you get fancy you can try using --wait).\nBut the expectation is that you set up your applications to be \"eventually consistent\" when it comes to inter-service dependencies. In practical terms, this usually means just crashing if a dependent service isn't available, and it will just be restarted until things succeed.\n"
},
{
  "Question": "How can I duplicate a namespace with all content with a new name in the same kubernetes cluster?\ne.g. Duplicate default to my-namespace which will have the same content.\nI'm interested just by services and deployments, so\nwhen I try with method with kubectl get all and with api-resources i have error with services IP like :\nError from server (Invalid): Service \"my-service\" is invalid: spec.clusterIP: Invalid value: \"10.108.14.29\": provided IP is already allocated\n\n",
  "Answer": "There is no specific way to do this. You could probably get close with something like kubectl get all -n sourcens -o yaml | sed -e 's/namespace: sourcens/namespace:  destns/' | kubectl apply -f - but get all is always a bit wonky and this could easily miss weird edge cases.\n"
},
{
  "Question": "I'm running a Kubernetes cluster on Google cloud. My cluster has a deployment that exposing the health-check interface (over HTTP). In my deployment yaml file, I configured:\nlivenessProbe:\n  # an http probe\n  httpGet:\n    path: /hc\n    port: 80\n    scheme: HTTP\n  initialDelaySeconds: 30\n  timeoutSeconds: 60\n  periodSeconds: 90\n\nIf my health check endpoint return anything but 200, the pod will be killed and restarted.\nCurrently, after pod restart, it just counts it on the \"restart\" counter but not notify anyone. I want to notify the sysadmin that this event has happened. I thought to notify with a webhook.\nIs this possible? If not, what is my other notification alternative?\n",
  "Answer": "The somewhat convoluted standard answer to this is Kubernetes -> kube-state-metrics -> Prometheus -> alertmanager -> webhook. This might sound like a lot for a simple task, but Prometheus and its related tools are used much more broadly for metrics and alerting. If you wanted a more narrow answer, you could check out Brigade perhaps? But probably just use kube-prometheus (which is Prom with a bunch of related components all setup for you).\n"
},
{
  "Question": "I would like to know, what are the actual memory and CPU capacity in mi and m in the following results:\nCapacity:\n alpha.kubernetes.io/nvidia-gpu:    0\n cpu:                   2\n memory:                2050168Ki\n pods:                  20\nAllocatable:\n alpha.kubernetes.io/nvidia-gpu:    0\n cpu:                   2\n memory:                2050168Ki\n pods:                  20\n\n",
  "Answer": "2 CPUs (2 cores) and 2050168Kb of RAM (more simply, 2GB). Which also happens to be the Minikube defaults.\n"
},
{
  "Question": "I download a pod's log, it has size bigger then 1G, how to shrike/truncate the pod's log? so I can download it faster?\nPS: I need erase the old part of logs so I can free the disk space\n",
  "Answer": "kubectl logs --tail=1000 to show only the most recent 1000 lines. Adjust as needed.\n\n--tail=-1: Lines of recent log file to display. Defaults to -1 with no selector, showing all log lines otherwise\n  10, if a selector is provided.\n\n"
},
{
  "Question": "Is it possible to modify a Kubernetes pod descriptor postponing the restart to a later event? \nBasically I want to defer the changes to scheduled restart event.\n",
  "Answer": "You cannot. That is not a feature of Kubernetes.\n"
},
{
  "Question": "Lets I have four separate microservice application which I want to deploy in K8's clusters. These applications interact with each other a lot. So is it recommended to have them in the same pod or individual pod for each microservcie\n",
  "Answer": "Different pods. You only combine containers into one pod when they are very specifically for the same thing. Generally this means sidecars supporting the primary container in the pod for things like metrics, logging, or networking.\n"
},
{
  "Question": "The goal: \nUsing Helm Chart pre-install hook, take a file on the filesystem, encode it and place it as a resource file (referenced by a configMap.\nQuestions:\n\nCan a Helm Chart pre-install hook access files that are not under the root chart?\nCan a Helm Chart pre-install hook modify or add a file under the Chart root?\nOther then implicitly writing the bash script inside the chart resource yaml, can the pre-install hook execute a bash script if it is placed in the chart?\n\n",
  "Answer": "No, Hooks run as Jobs inside the Kubernetes cluster, so they have no access to your workstation. What you want is the Events system (https://github.com/helm/community/blob/master/helm-v3/002-events.md) which is still a WIP I think.\n"
},
{
  "Question": "According to \"Finding credentials automatically\" from Google Cloud:\n\n...ADC (Application Default Credentials) is able to implicitly find the credentials as long as the GOOGLE_APPLICATION_CREDENTIALS environment variable is set, or as long as the application is running on Compute Engine, GKE, App Engine, or Cloud Functions.\n\nDo I understand correctly that GOOGLE_APPLICATION_CREDENTIALS does not need to be present, if I want to call Google Cloud APIs in current Google Cloud project?\nLet's say I'm in a container in a pod, what can I do from within acontainer to test that calling Google Cloud APIs just work\u2122?\n",
  "Answer": "Check out https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity for how to up permissions for your pods. You have to do some mapping a so Google knows which pods get which perks, but after that it\u2019s auto-magic as you mentioned. Otherwise calls will use the node-level google permissions which are generally minimal.\n"
},
{
  "Question": "i am trying to setup a complete GitLab Routine to setup my Kubernetes Cluster with all installations and configurations automatically incl. decommissioning at the end. \nHowever the Creation and Decommissioning Progress is one of the most time consuming because i am basically waiting for the provisioning till i can execute further commands. \nas i have some times troubles in the bases of the Kubernetes Setup, i currently decomission my cluster and create a new one. But this is pretty un-economical and time consuming.\nQuestion:\nIs there a command or a series of commands to completely reset a Kubernetes to his state after creation ? \n",
  "Answer": "The closest is probably to do all your work in a new namespace and then delete that namespace when you are done. That automatically deletes all objects inside it.\n"
},
{
  "Question": "By mistake I created a service account to give admin permission for dashboard. But now I am unable to delete it.\nThe reason I want to get rid of that service account is if I follow the steps here https://github.com/kubernetes/dashboard. When I jump to the URL it doesn't ask for config/token anymore.\n$ kubectl get serviceaccount --all-namespaces | grep dashboard\n\nNAMESPACE     NAME                   SECRETS   AGE\n\nkube-system   kubernetes-dashboard   1         44m\n\n$ kubectl delete serviceaccount kubernetes-dashboard\n\nError from server (NotFound): serviceaccounts \"kubernetes-dashboard\" not found\n\n",
  "Answer": "You have to specify the namespace when deleting it:\nkubectl delete serviceaccount -n kube-system kubernetes-dashboard\n\n"
},
{
  "Question": "I've raised the pod replicas to something like 50 in a cluster, watched it scale out, and then dropped the replicas back to 1.  As it turns out I've disabled scale-down for one node.  I've noticed that k8s will leave the remaining replica on that node.  However, I've seen it remove that node when the annotation to prevent scale-down is not present.  So somehow k8s makes decisions based on some kind of knowledge of nodes, or at least that the oldest POD is the one on the given node.  Or something else altogether.\nAfter a scale down of k8s pod replicas how does k8s choose which to terminate?\n",
  "Answer": "Roughly speaking it tries to keep things spread out over the nodes evenly. You can find the code in https://github.com/kubernetes/kubernetes/blob/edbbb6a89f9583f18051218b1adef1def1b777ae/pkg/controller/replicaset/replica_set.go#L801-L827 If the counts are the same, it's effectively random though.\n"
},
{
  "Question": "I'm installing Prometheus on GKE with Helm using the standard chart as in\nhelm install -n prom stable/prometheus --namespace hal\nbut I need to be able to pull up the Prometheus UI in the browser.  I know that I can do it with port forwarding, as in\nkubectl port-forward -n hal svc/prom-prometheus-server 8000:80\nbut I'm being told \"No, just expose it.\"  Of course, there's already a service so just doing\nkubectl expose deploy -n hal prom-prometheus-server \nisn't going to work.  I assume there's some value I can set in values.yaml that will give me an external IP, but I can't figure out what it is.  \nOr am I misunderstanding when they tell me \"Just expose it\"?\n",
  "Answer": "It is generally a very bad idea to expose Prometheus itself as it has no authentication mechanism, but you can absolutely set up a LoadBalancer service or Ingress aimed at the HTTP port if you want.\nMore commonly (and supported by the chart) you'll use Grafana for the public view and only connect to Prom itself via port-forward when needed for debugging.\n"
},
{
  "Question": "I have been using saltstack for a few years with bare metal server. Now we need to setup a whole new environment on AWS. I'd prefer to use saltstack set everything up because I like the orchestration of salt and the event based stuff, like beacons & reactors. Plus it's easy to write your own customised python module. We will also be running kubernetes clusters on EC2 instances. Can someone provide some best practices for using salt with AWS and k8s?\n",
  "Answer": "There\u2019s a few reusable setups floating around, last I remember https://github.com/valentin2105/Kubernetes-Saltstack was the most complete of them. But all of them are less solid than tools closer to the community mainstream (kops, kubespray) so beware of weird problems. I would recommend going through Kubernetes The Hard Way just so you have some familiarity with the underlying components that make up Kubernetes so you\u2019ll have a better chance of debugging them :)\n"
},
{
  "Question": "Is there a way to do active and passive load balancing between 2 PODs of a micro-service. Say I have 2 instance(PODs) running of Micro-service, which is exposed using a K8s service object. Is there a way to configure the load balancing such a way that one pod will always get the request and when that pod is down , the other pod will start receiving the request?\nI have ingress object also on top of that service.\n",
  "Answer": "This is what the Kubernetes Service object does, which you already mentioned you are using. Make sure you set up a readiness probe in your pod template so that the system can tell when your app is healthy.\n"
},
{
  "Question": "Here we have a sample of the job \napiVersion: batch/v1\nkind: Job\nmetadata:\n  # Unique key of the Job instance\n  name: example-job\nspec:\n  template:\n    metadata:\n      name: example-job\n    spec:\n      containers:\n      - name: pi\n        image: perl\n        command: [\"perl\"]\n        args: [\"-Mbignum=bpi\", \"-wle\", \"print bpi(2000)\"]\n      # Do not restart containers after they exit\n      restartPolicy: Never\n\nI want to run a MySQL script as a command:\nmysql  -hlocalhost -u1234 -p1234 --database=customer < script.sql\nBut Kubernetes documentation is silent about piping a file to stdin. How can I specify that in Kubernetes job config?\n",
  "Answer": "Would set your command to something like [bash, -c, \"mysql  -hlocalhost -u1234 -p1234 --database=customer < script.sql\"], since input redirection like that is actually a feature of your shell.\n"
},
{
  "Question": "If I run a minikube instance on ubuntu do I need a VM like virtualbox?\nIf I run minikube start it complains that I need to install a VM, seems weird to have to do that on linux tho.\n",
  "Answer": "While it is possible to run without a VM via --vm-driver=none it really isn't recommended outside of ephemeral CI instances. Minikube expects to be able to take over the system pretty fully to do its thang. I would recommend checking out one of the other tools to manage a local environment like microk8s (available as a Snap too), Kind, or possibly k3s.\n"
},
{
  "Question": "i am relatively new to kubernetes and i am trying to catch things, in my understanding, a pod can have single container as well as multiple containers. Lets say i have a pod with 5 tightly coupled containers and is it possible to autoscale only 2 of them based on the usage. or autoscale will only happen podwise.\n",
  "Answer": "No, the definition of a pod is co-located containers. You can vertically scale them differently but there will always be exactly one of each per pod.\n"
},
{
  "Question": "I got a VPS server running ubuntu, which is technically simply a VM. Now I thought about hosting a cluster on that VM instead of using AWS. So I would need to run the cluster directly on ubuntu.\nOne solution would be to simply use minikube which initializes another VM on top. But I'd prefer to run the kubernetes cluster directly on the VM.\nAs I am pretty new to all these topics I have no idea how to start at all. I'd guess I need a cluster management tool such as kops (which is used for AWS). \nIs it possible to run a kubernetes cluster directly on ubuntu? \nWould be glad if you can help me getting started.\n",
  "Answer": "Microk8s from Ubuntu makes this easy, or check out Kind from sig-testing.\n"
},
{
  "Question": "Considering Kubernetes 1.13, how does compare the features of the Job object to the features available around the units of systemd? And vice-versa? \n",
  "Answer": "They do not meaningfully compare. The closest you could say is that a Job object is vaguely similar to a oneshot-mode service unit in systemd, in that both run a process until completion and that's it. But a service unit can't run multiple copies and systemd's containment features are different from K8s/Docker.\n"
},
{
  "Question": "my Kubernetes setup:\n\nv1.16.2 on bare metal\n1 master node: used for Jenkins Master + Docker registry\n5 slave nodes: used for Jenkins JNPL slaves\n\nI use kubernetes-plugin to run slave docker agents. All slave k8 nodes labeled as \"jenkins=slave\". When I use nodeSelector (\"jenkins=slave\") for podTemplate, kubernetes always schedule new pod on same node  regardless the amount of started Jenkins jobs.\nPlease give me advice, how I can configure kubernetes or kubernetes-plugin to schedule each next build by round-robin (across all labeled nodes in kubernetes cluster)\nThank you.\n",
  "Answer": "This is generally handled by the inter-pod anti affinity configuration https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity. You would set this in the pod template for your builder deployment. That said, it's more common to use the Kubernetes plugin for Jenkins which runs each build as a temporary pod, rather than having long-lived JNLP builders.\n"
},
{
  "Question": "My kubernetes cluster has 3 pods for postgres. I have configured persistent volume outside of the cluster on a separate virtual machine. Now as per kubernetes design, multiple pods will be responding to read/write requests of clients. Is their any dead lock or multiple writes issues that can occur between multiple postgres pods?\n",
  "Answer": "You would need a leader election system between them. There can be only one active primary in a Postgres cluster at a time (give or take very very niche cases). I would recommend https://github.com/zalando-incubator/postgres-operator instead.\n"
},
{
  "Question": "I have a cronjob that runs and does things regularly. I want to send a slack message with the technosophos/slack-notify container when that cronjob fails.\nIs it possible to have a container run when a pod fails?\n",
  "Answer": "There is nothing built in for this that i am aware of. You could use a web hook to get notified when a pod changes and look for state stuff in there. But you would have to build the plumbing yourself or look for an existing third party tool.\n"
},
{
  "Question": "Does the bolt protocol which is used by Neo4j works with Traefik? \nTCP is not supported yet by Traefik, but according to the Traefik documention, it supports WebSocket(which operates over a TCP connection), and this confuses me! \nIs it possible to run Neo4j databases behind Traeffik and access them using something like bolt://myhost/path/that/traefik/routes/to/my/db?\n",
  "Answer": "This appears to be up to each client library, and from what I can see it looks like only a few browser-based clients actually use the WebSocket mode. So overall, probably no, pending 2.0 coming out.\n"
},
{
  "Question": "If I have 1 Django project and multiple Django apps. Every Django app has it's own requirements.txt and settings. Hence every app has its own docker image. My doubt is, can I execute code from one Django app to other Django app while both apps have a different container?\n",
  "Answer": "No, in the context of Django an \u201capp\u201d is a code level abstraction so they all run in one process, which means one image. You can, sometimes, break each app into its own project and have then communicate via the network rather than locally, this is commonly called \u201cmicroservices\u201d and smaller images is indeed one of the benefits.\n"
},
{
  "Question": "I'm trying to add a new node pool into an existing GKE cluster.  Failing with the below error.\nNode pool version cannot be set to 1.14.6-gke.1 when releaseChannel REGULAR is set.\n\nAny advice on how i can get around this?\nEDIT:  I finally managed to create a new pool but only after my master was auto-updated.   looks like for auto-updated clusters this is a limitation.  the new node being created seems to default to the version of the master and if the master is on a deprecated version and is pending auto upgrade, all one can do it wait.\n",
  "Answer": "That version was removed from GKE yesterday: https://cloud.google.com/kubernetes-engine/docs/release-notes#version_updates\nThe following versions are no longer available for new clusters or upgrades.\n\n    1.13.7-gke.24\n    1.13.9-gke.3\n    1.13.9-gke.11\n    1.13.10-gke.0\n    1.13.10-gke.7\n    1.14.6-gke.1\n    1.14.6-gke.2\n    1.14.6-gke.13\n\n"
},
{
  "Question": "My first thought was using the downward API, but that doesn't seem to expose the scale of a deployment / statefulset. I was hoping to be able to avoid adding it in as a separate environment variable manually, or having to rely on pods all discovering each other to determine the scale if possible. \nUse-case: Deploying many pods for an application that connects to an external service. Said service does some form of consistent hashing (I believe is the right term?) for sending the data to clients, so clients that connect send an id number from 0 - N-1 and a total number of clients N. In this case, the deployment/statefulset scale would be N.\n",
  "Answer": "You would definitely have to use a StatefulSet for this, and I don't think you can pull it from the DownwardAPI because the replica count isn't part of the pod spec (it's part of the statefulset spec). You could get the parent object name and then set up a service account to be able to query the API to get the replica count, but that seems like more work than putting the value in a label or env var.\n"
},
{
  "Question": "I am using Kubernetes HPA to scale up my cluster. I have set up target CPU utilization is 50% . It is scaling up properly. But, when load decreases and it scales down so fast. I want to set a cooling period. As an example, even the CPU util is below 50% , it should wait for 60 sec before terminating a node.\nI have checked this article, but it is not saying that I can change the default value in HPA, https://kubernetes.io/docs/concepts/workloads/pods/pod/index.html#termination-of-pods\nKops version :- 1.9.1\n",
  "Answer": "This is configured at the HPA level: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/#support-for-cooldown-delay\n\n--horizontal-pod-autoscaler-downscale-delay: The value for this option is a duration that specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed. The default value is 5 minutes (5m0s).\n\n"
},
{
  "Question": "Try to append some new entries to /etc/hosts in pods, but failed:\n$ ips=$(cat ips.txt); kubectl exec -u root myspark-master-5d6656bd84-5zf2h echo \"$ips\" >> /etc/hosts\n-sh: /etc/hosts: Permission denied\n\nHow to fix this?\nThanks\nUPDATE\n$ ips=$(cat ips.txt); kubectl exec myspark-worker-5976b685b4-8bcbl -- sh -c \"echo $ips >> /etc/hosts\"\nsh: 2: 10.233.88.5: not found\nsh: 3: 10.233.96.2: not found\nsh: 4: 10.233.86.5: not found\n10.233.88.4 myspark-master-5d6656bd84-dxhxc\ncommand terminated with exit code 127\n\n",
  "Answer": "I think you mean to write to the file inside the container, but bash is parsing that on your workstation and try to apply the redirect locally. Use kubectl exec ... -- sh -c \u201c...\u201d instead. \n"
},
{
  "Question": "When we scrape the etcd exposing end point (i.\u00a0e. \"/metrics\"), we get a flat text. Is there any way we can structure the whole data to work on it instead of working on string comparison on the required metric?\nNote: I don't want to use prometheus for monitoring. Instead I want to create my own framework to monitor etcd.\n",
  "Answer": "https://github.com/prometheus/prom2json is probably what you are looking for.\n"
},
{
  "Question": "I understand that Kubernetes make great language-agnostic distributed computing clusters, easy to deploy,  etc.\nHowever, it seems that each platform has his own set of tools to deploy and manage Kubernetes.\nSo for example, If I use Amazon Elastic Container Service for Kubernetes (Amazon EKS), Google Kubernetes engine or Oracle Container Engine for Kubernetes,   how easy (or hard) is to switch between them ?\n",
  "Answer": "\"It depends\". The core APIs of Kubernetes like pods and services work pretty much the same everywhere, or at least if you are getting into provider specific behavior you would know it since the provider name would be in the annotation. But each vendor does have their own extensions. For example, GKE offers integration with GCP IAM permissions as an alternative to Kuberenetes' internal RBAC system. If you use that, then switching is that much harder. The more provider-specific annotations and extensions you use, the more work it will be to switch.\n"
},
{
  "Question": "I am new to kubernetes and have installed KOPS and now I want to use traefik as a ingress controller and cant get through it, I have a domain \"xyz.com\" and nameserver on cloudflare. I tried to create a \"CNAME\" record with my domain with ELB of aws which I get after deploying all files but it didn't worked and I don't want to use route53. I actually can't get proper guidance regarding this. Have gone through the documentation but somewhere got an idea but not working. Can anyone tell me somewhere like step wise, I will be very grateful. \n",
  "Answer": "Given your comment, that is correct. You would set your DNS for all relevant domains to CNAME to the ELB generated for the Traefik Service.\n"
},
{
  "Question": "I am using docker and trying to enable kubernetes and set CPU and Memory via command line.\nI have looked at this answer but unfortunately cannot find this file.\nIs there any way to enable Kubernetes on Docker for Mac via terminal?\n",
  "Answer": "Docker does not have an app-ified version for Linux that I know of, so there is no relation to the Docker for Mac/Windows app. There are many tools to locally install Kubernetes on Linux so they probably didn't see much reason to make something new. Minikube is the traditional one, but you can also check out microk8s, k3s, KinD, and many others.\n"
},
{
  "Question": "I am perfectly aware of the Kubernetes API and that a manifest can be broken down in several K8S API calls.\nI am wondering if there is a way to apply a whole manifest at once in a single API call. A REST equivalent to kubectl apply.\n",
  "Answer": "The feature is still in alpha but yes, it's called \"server-side apply\". Given the in-flux nature of alpha APIs, you should definitely check the KEPs before using it, but it's a new mode for the PATCH method on objects.\n"
},
{
  "Question": "As per prometheus storage.md , the recommendation is not to use nfs storage as persistent volume for prometheus.\nBut solutions like prometheus operator and openshift shows examples which uses nfs as persistent volumes for prometheus.\nSo what am I missing here? If nfs is not recommended then why do these tools share examples to use nfs as the storage options for prometheus?\nDoes anyone know what could be the nfs alternative for NetApp/ Trident for prometheus?\n",
  "Answer": "The example in the prom-operator docs is just a hypothetical to show how to manually control the storage provisioning. NFS is generally an option of last resort in all cases :) Check out https://kubernetes.io/docs/concepts/storage/persistent-volumes/ for more general information on how to use each of the various PV plugins (or if none of those work, look up CSI stuffs), but for NetApp you would probably use the iSCSI interface.\n"
},
{
  "Question": "I created kubernetes cluster on aws ec2 using kubeadm. Now I need to autoscale the K8's cluster when there are not enough resources on nodes to schedule new pods, How can I achieve autoscaling feature for my cluster?\n",
  "Answer": "Unfortunately there isn't a great answer if you mean you manually ran kubeadm on some EC2 instance. cluster-autoscaler is the thing to use, but it requires you deploy your nodes using Autoscaling Groups. It's possible to use ASGs and kubeadm but I don't know of anything off-the-shelf for it.\n"
},
{
  "Question": "I'm new to Kubernetes. Got confused with how CustomResourceDefinations changes got to apply:-)\nEx: If I have a CustomResourceDefinations \"Prometheus\", it creates a Statefulsets which create one pod. After the CRD changed, I need to use the latest CRD to create my pod again. What is the correct way? Should I completely remove the Statefulsets and pod then recreate them or just simply do \"kubectl delete pod\" then the change will auto apply when the new pod gets created? Thanks much!\n",
  "Answer": "The operator, or more specifically the custom controller at the heart of the operator, takes care of this. It watches for changes in the Kubernetes API and updates things as needed to respond.\n"
},
{
  "Question": "I am trying to understand security implications of running containers with --cap-add=NET_ADMIN. The containers will be running in k8s cluster and they will execute user provided code (which we can't control and can be malicious).\nIt is my understanding that unless I add --network host flag, the containers will be able to only change their own networking stack. Therefore, they can break their own networking but can't affect the host or other containers in any way. \nIs this correct? Are there any considerations when deciding if this is safe to do?\n",
  "Answer": "At a minimum, they would be able to turn on promiscuous mode on the pod's network adapter which could then potentially see traffic bound for other containers. Overall this seems like a very very very bad idea.\n"
},
{
  "Question": "In defining a service, can I somehow hook up a value, eg. TargetPort so that if I change the configmap, it will automatically update my service? I don't think so, but maybe I am unclear if I can fully psrameterize my application port.\nI can do this with a manual script but w wondering what other solutions there are. \n",
  "Answer": "This is not something you do directly in Kubernetes. You would use a higher-level system like Kustomize or Helm to put the correct value in both places. That said, why would you? It's not like you ever need things to coexist so just pick a port and roll with it.\n"
},
{
  "Question": "error: unable to recognize \"xxxxx-pod.yml\": Get http://localhost:8080/api?timeout=32s: dial tcp 127.0.0.1:8080: connect: connection refused, \nI tried the solutions available online but none of them really worked.\n",
  "Answer": "This means your kubeconfig is not correct. It is using the default server URL which is probably not what you intended.\n"
},
{
  "Question": "Does the following configuration for configMap create the test.json file of type empty array or string []\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: myconfigmap\ndata:\n  test.json: |-\n    []\n\nThe convertor to JSON suggests string:\n{\n    \"kind\": \"ConfigMap\",\n    \"apiVersion\": \"v1\",\n    \"metadata\": {\n        \"name\": \"myconfigmap\"\n    },\n    \"data\": {\n        \"test.json\": \"[]\"\n    }\n}\n\nMy goal is to create configMap file with empty array.\n",
  "Answer": "Sure, you can make it whatever string you want, it just has to be a string. The thing you can't do is test.json: [] since that's an array. The fact that your string happens to be valid JSON is not something K8s knows or cares about.\n"
},
{
  "Question": "I deployed a kubernetes cluster and using 3 replicas for my backend NodeJS.\nI am now applying some socket function and want to make sure my redis pub and sub function is working from different pod.\nTherefore, I want to display the NodeJS pod name on client side to test whether is working.\n*and I am using ReactJS as my frontend (client side)\n",
  "Answer": "The server pod would have to put that in a header or otherwise purposefully send it down to the client.\n"
},
{
  "Question": "I am trying to implement pgbouncer on k8s, using a helm chart created deployment,service\u2026now how do I expose the service to outside world? Not much familiar with k8s networking, tried to create an ingress resource and it created an elb in aws\u2026how do I map this elb to the service and expose it?\nthe service is created with type ClusterIP\u2026the service is a tcp service i.e. not http/https application (edited) \nThe helm chart used is - https://github.com/futuretechindustriesllc/charts/tree/master/charts/pgbouncer\n",
  "Answer": "Ingresses are only used for HTTP and friends. In this case what you want is probably a LoadBalancer type service. That will make a balancer fabric and then expose it via an ELB.\n"
},
{
  "Question": "I want to add some annotations to the metadata block of a service within an existing helm chart (I have to add an annotation for Prometheus so that the service is auto discovered). The chart (it is the neo4j chart) does not offer me a configuration that I can use to set annotations. I also looked into the yaml files and noticed that there is no variable I can use to insert something in the metadata block. The only solution I can see is that I have to fork the chart, insert the annotation data to the correct place and create my own chart out of it. Is that really the only solution or is there some trick I am missing that allows me to modify the helm chart without creating a new one?\n",
  "Answer": "In Helm 2, you are correct. Either you would have to fork the chart or pass it through another tool after rendering like Kustomize. Helm 3 has some planned features to improve this in the future.\n"
},
{
  "Question": "on \n kops edit ig nodes \n\nI am getting \nerror determining default DNS zone: Found multiple hosted zones matching cluster \".domain\"; please specify the ID of the zone to use\n\ncluster looks like this\n$ kops get ig\nUsing cluster from kubectl context: dev3.onchain.live\n\nNAME                    ROLE    MACHINETYPE     MIN     MAX     ZONES\nmaster-us-east-1b       Master  m4.large        1       1       us-east-1b\nnodes                   Node    m4.large        3       3       us-east-1b\n\nadding \n --state=$KOPS_STATE_STORE\n\ndid not help. \n",
  "Answer": "It lives in the ClusterSpec YAML file:\n// DNSZone is the DNS zone we should use when configuring DNS\n// This is because some clouds let us define a managed zone foo.bar, and then have\n// kubernetes.dev.foo.bar, without needing to define dev.foo.bar as a hosted zone.\n// DNSZone will probably be a suffix of the MasterPublicName and MasterInternalName\n// Note that DNSZone can either by the host name of the zone (containing dots),\n// or can be an identifier for the zone.\nDNSZone string `json:\"dnsZone,omitempty\"`\n\nThough having more than one is usually a configuration issue in Route53. Or at least it's not normal to have multiple matching zones.\n"
},
{
  "Question": "I am writing an exporter and am having trouble with metrics for my collector. From what I can tell, metrics in Prometheus need to be defined beforehand. Is there any way to define them dynamically at runtime instead? I won't know how many metrics I'll have or what metrics I'll need until the code is running. (For example, if k8s finds all volumes connected to the cluster, and I need EACH volume to get its own total_capacity/available_capacity metric).\n",
  "Answer": "You would handle this by using dynamic label values rather than dynamic metric names. That said, you can call prometheus.Register() at runtime just fine, client_golang won't know the difference.\n"
},
{
  "Question": "I have two application, both deployed in same cluster.\nNow from web, there is an ajax request to get data from api, but it always return 502 Connection refused.\nhere is my jquery code (web).\n$.get(\"http://10.43.244.118/api/users\", function (data) {\n        console.log(data);\n        $('#table').bootstrapTable({\n            data: data\n        });\n    });\n\nNote: when I change the service type to LoadBalancer from ClusterIP then it works fine.\n",
  "Answer": "ClusterIP services (usually) only work within the cluster. You can technically make your CNI address space available externally, but that is rare and you probably shouldn't count on it. The correct way to expose something outside the cluster is either a NodePort service or a LoadBalancer (which is a NodePort plus a cloud load balancer).\n"
},
{
  "Question": "I want to modify particular config file from kubernetes running pod at runtime.\nHow can I get pod name at runtime and I can modify the file from running pod and restart it to reflect the changes? I am trying this in python 3.6.\nSuppose,\nI have two running pods.\nIn one pod I have config.json file. In that I have \n\n{\n      \"server_url\" : \"http://127.0.0.1:8080\" \n  }\n\nSo I want to replace 127.0.0.1 to other kubernetes service's loadbalancer IP in it.  \n",
  "Answer": "Generally you would do this with an initContainer and a templating tool like envsubst or confd or Consul Templates.\n"
},
{
  "Question": "Is there a way to have the same label key but different values for a pod. For example, can a pod have labels as \"app=db\" and \"app=web\". I tried to use kubectl label command but it picks only one label.\n",
  "Answer": "Labels are a map[string]string so you are correct, this is not possible.\n"
},
{
  "Question": "I've had my services set with type NodePort however in reality external access it not required - they only need to be able to talk to each other.\nTherefore I presume I should change these to the default ClusterIP however the question is - how can I continue to access these services during my local development?\nSo when i make the change from NodePort to ClusterIP then go to minikube service list it naturally shows no node port however how can I now access - is there some special endpoint address I can get from somewhere?\nThanks.\n",
  "Answer": "You would need to access it like any other out-of-cluster case. Generally this means either kubectl port-forward or kubectl proxy, I favor the former though. In general, ClusterIP services are only accessible from inside the cluster, accessing through forwarders is only used for debugging or infrequent access.\n"
},
{
  "Question": "I'm trying to automate the process of simultaneously deploying an app onto multiple machines with kubernetes clusters. I'm new to kubernetes.\nWhich tool/technology should I use for this?\n",
  "Answer": "Kubernetes doesn't really think in terms of machines (nodes in k8s jargon). When you set up a Deployment object, you specify a number of replicas to create, that's how many copies of the Pod to run. So rather than deploying on multiple machines, you create/update the Deployment once and set it to have multiple replicas which then get run on your cluster.\n"
},
{
  "Question": "\nUsing Kubernetes deploying nginx in several pods. Each pod is mounting access.log file to hostPath in order to read by Filebeat to collect to other output.\nIf do log rotation in the same cron time in every pod, they are using common access.log file, it works.\nI tested with few data in a simple cluster. If large data occurred in production, is it a good plan or something wrong will happen with logrotate's design?\n",
  "Answer": "This will not usually work well since logrotate can't see the other nginx processes to sighup them. If nginx in particular can detect a rotation without a hup or other external poke then maybe, but most software cannot.\nIn general container logs should go to stdout or stderr and be handled by your container layer, which generally handles rotation itself or will include logrotate at the system level.\n"
},
{
  "Question": "In kubernetes, you can listen for events using kubectl get events. This works for other resources, but I would like to know when a Service is created and destroyed.\nWhen I run kubectl describe my-service I get Events:            <none>. \nHow can I know when a service was created?\n",
  "Answer": "Every api object has a creation timestamp in the metadata section. Though that doesn\u2019t tell when it is edited. For that you might want an audit webhook or something like Brigade.\n"
},
{
  "Question": "Here's my environment, I have k8s cluster and some physical machines outside k8s cluster. Now I create a pod in k8s, and this pod will act like a master to create some processes in these physical machines outside k8s cluster. And I need to establish rpc connection between the k8s pod and these external processes. I don't want to use k8s service here. So what kind of other approach I can use to connect a pod in k8s from external world. \n",
  "Answer": "You would need to set up your CNI networking in such a way that pod IPs are routable from outside the cluster. How you do this depends on you CNI plugin and your existing network design. You could also use a VPN into the cluster network in some cases.\n"
},
{
  "Question": "I am attempting to set up ThingsBoard on a google k8s cluster following the documentation here.\nEverything is set up and running, but I can't seem to figure out which IP I should use to connect to the login page. None of the external ips I can find appear to be working\n",
  "Answer": "Public access is set up using an Ingress here https://github.com/thingsboard/thingsboard/blob/release-2.3/k8s/thingsboard.yml#L571-L607\nBy default I think GKE sets up ingress-gce which uses Google Cloud Load Balancer rules to implement the ingress system, so you would need to find the IP of your load balancer. That said the Ingree doesn't specify a hostname-based routing rule so it might not work well if you have other ingresses in play.\n"
},
{
  "Question": "Is it possible to have more than one instance of the actual Service object created in deployments that actually manages the access to pods (containers) and what happens if the actual service object itself is somehow deleted or destroyed? \nThis is the service object specified in a deployment YAML file:\nkind: Service\napiVersion: v1\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app: MyApp\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 9376\n\n",
  "Answer": "The Service object only exists as an abstraction within the Kubernetes API. The actual implementation is distributed across your entire cluster, generally in the form of iptables rules created by kube-proxy on every node.\n"
},
{
  "Question": "Can multiple long waiting threads (blocked on remote rest call response, non cpu-bound) throttle CPU ?\nThis cpu throttle causes leads to pod restart as health check response takes time to respond.\n",
  "Answer": "Something blocked in a waiting syscall (select and friends, sleep(), blocking read or write) does not count as using any CPU time, the task (how Linux thinks about threads internally) won't be marked as runnable until something interrupts the wait.\n"
},
{
  "Question": "By default Kubernetes cluster has a taint on master nodes that does not allow to schedule any pods to them.\nWhat is the reason for that?\nDocker Swarm, for example, allows to run containers on manager nodes by default.\n",
  "Answer": "Safety during failures. Imagine if a pod running on a control plane node started eating all your CPU. Easy to fix right, just kubectl delete pod and restart it. Except if that CPU outburst has kube-apiserver or Etcd locked up, then you have no way to fix the problem through the normal tools. As such, it's usually just safer to keep unvetted workloads off the control plane nodes.\n"
},
{
  "Question": "Is there a way to specify a hostPath volume so that it includes the pod's name? For example, something like the following:\n  volumes:\n    - name: vol-test\n      hostPath:\n        path: /basedir/$(POD_NAME)\n        type: Directory\n\nSimply using /basedir directly and then have the pod itself query for its name and create the directory doesn't satisfy my needs: I specifically want to map each of my containers' /tmp folders to specific volumes.\nI know that something similar works with valueFrom for environment variables (see here), but so far I haven't been able to find anything similar for volumes.\nPlease let me know if anyone has any ideas. Thanks!\n",
  "Answer": "You set it as an env var first via valueFrom and then certain fields understand $(FOO) as an env var reference to be interpolated at runtime.\nEDIT: And path is not one of those fields. But subPathExpr is.\n"
},
{
  "Question": "I'm running rook-ceph-cluster on top of AWS with 3 masters - 3 worker node configuration.\nI have created my cluster using this.\nEach worker node is 100 GiB each.\nAfter setting everything up. I have my pods running (6 pods to be exact,3 for master and 3 for nodes).\nHow can I crash/fail/stop those pods manually (to test some functionality)?.\nIs there is any way I can add more load manually to those pods so that it can crash?.\nOr can I somehow make them Out Of Memory?.\nOr can I simulate intermittent network failures and disconnection of nodes from the network?\nOr any other ways like writing some script that might prevent a pod to be created?\n",
  "Answer": "You can delete pods manually as mentioned by Graham, but the rest are trickier. For simulating an OOM, you could kubectl exec into the pod and run something that will burn up RAM. Or you could set the limit down below what it actually uses. Simulating network issues would be up to your CNI plugin, but I'm not aware of any that allow failure injection. For preventing a pod from being created, you can set an affinity it that is not fulfilled by any node.\n"
},
{
  "Question": "External firewall logs show blocked connection from < node IP >:< big port >. \nThe current cluster uses calico networking.\nHow do I detect which pod trying to connect?\n",
  "Answer": "This would usually be pretty hard to work out, you would have to check the NAT table on the node where the packets exited to the public internet.\n"
},
{
  "Question": "enter image description hereI don't' see why adding selector when we are creating replicaSet I thought maybe we can select different pod but we can't so I don't see what is used for\napiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n    name: rs-name\nspec:\n    replicas: 2\n    selector: // here the selector\n        matchLabels:\n            podptr: my-pod-l\n    template:\n        metadata:\n            name: pods-name\n            labels:\n                podptr: my-pod-l // here it's lable\n        spec:\n            containers:\n                - name: cons-name\n                  image: nginx\n                  ports:\n                      - containerPort: 50\n\n",
  "Answer": "Because you can have more than one label on the resulting pods and it doesn't know which of them to use for tracking. The selector labels are read-only once initialized, but other labels you can add and remove as needed for other purposes.\n"
},
{
  "Question": "I have an NFS based PVC in a kubernetes cluster that I need to freeze to take a snapshot of.  I tried fsfreeze, but I get \"operation not supported\".  I assume because it is trying to freeze the entire nfs instead of just the mount. I have checked and I can freeze the filesystem on the side of the NFS server. Is there a different way that I can stop writes to the filesystem to properly sync everything?\n",
  "Answer": "From https://github.com/vmware-tanzu/velero/issues/2042 and some other quick poking around, fsfreeze doesn't support NFS mounts. In general it seems to mostly on work with real local volumes which you'll almost never use with Kubernetes.\n"
},
{
  "Question": "Our goal is to run kubernetes in AWS and Azure with minimal customization (setting up kubernetes managed env), support and maintenance. We need portability of containers across cloud providers. \nOur preferred cloud provider is AWS. We are planning on running containers in EKS. We wanted to understand the customization effort required to run these containers in AKS. \nWould you recommend choosing a container management platform like Pivotal Cloud Foundry or Redhat OpenShift or run them on AWS EKS or AKS where customization is less to run containers across different cloud providers.\n",
  "Answer": "You need to define a common set of storage classes that map to similar volume types on each provider. If you are using some kind of provider based Ingress controller those can vary so I would recommend using an internal one like nginx or traefik. If you are using customization annotations for things like networking those can vary, but using those is pretty rare. Others k8s is k8s.\n"
},
{
  "Question": "What is the best way to send slack notification when a k8s cluster node is not ready? (Master and worker)\nI already have Prometheus and alert manager up and running. Ideally I would use them to do that.\n",
  "Answer": "Use kube-state-metrics and an alert rule. The query to start with is something like kube_node_status_condition{condition=\"Ready\",status!=\"true\"} > 0 but you can play with it as needed.\n"
},
{
  "Question": "I am currently working in dynamic scaling of services with custom metrics and I wanted to send some data from HPA to my external API service. Is there any way or any post request which will send the current replica count to my external API ?\nLike HPA has its sync period and it sends GET request to API to fetch the metric value so is there any way to also have some POST request so that I can get some data from HPA ?\n",
  "Answer": "You don't per se, your service can watch the Kubernetes API for updates like everything else though. All components in Kubernetes communicate through the API.\n"
},
{
  "Question": "Stateless is the way to go for services running in pods however i have been trying to move a stateful app which needs to perform session persistence if one pod goes does for resiliency reasons.\nIn websphere world IHS can be used to keep track of the session and if a node goes down it can be recreated on the live clone. \nIs there an industry standard way to handle this issue without having to refactor the applications code by persisting the session using some sidecar pod ?\n",
  "Answer": "Cookie-based sessions are just that, based on cookies. Which are stored by the user's browser, not your app. If you mean a DB-based session with a cookie session ID or similar, then you would need to store things in some kind of central database. I would recommend using an actual database like postgres, but I suppose there is nothing stopping you from using a shared volume :)\n"
},
{
  "Question": "I am installing Custom Resources through an  Operator. However, kubectl apply is blocked on\n\"Error from server (NotFound): customresourcedefinitions.apiextensions.k8s.io \"my-crd.example.com\" not found.\"\nIf there were a switch on  kubectl apply along the lines of --no-typechecking, it would solve this. I would not cause a problem with a missing CRD, because the apply just sends the Kubernetes objects to etcd. Then, by the time that the Operator  actually constructs the Custom Resource,  the Custom Resource Definition would be available. (I have other code that guarantees that.) \nSo, can I suspend the typechecking that produces this error?\n",
  "Answer": "No, you can\u2019t use a CRD API without actually creating the CRD. It\u2019s not a type check, it\u2019s how the system works through and through.\n"
},
{
  "Question": "I have a k8s cluster with an nginx based ingress and multiple services (ClusterIP). I want to use Consul as a service mesh and documentation is very clear on how to set up and govern communication between services. What is not clear though is how to setup the nginx ingress to talk to these services via the injected sidecar connect proxies using mutual ssl. I'm using cert-manager to automatically provision and terminate ssl at the ingress. I need to secure the communication between the ingress and the services with Consul provisioned mutual SSL. Any documentation related to this scenario will definitely help.\n",
  "Answer": "You would inject the sidecar into the ingress-nginx controller and have it talk to backend services just like any other service-to-service thing. This will probably require overriding a lot of the auto-generated config so I'm not sure it will be as useful as you hope.\n"
},
{
  "Question": "I am using client-go to read K8s container resource usage using the Get method of the core client-go client, but re-fetching the K8s object on an interval seems like the wrong approach. What is the better approach for periodically fetching status changes to a K8s container?\n",
  "Answer": "You would use an Informer which runs an API watch that receives push updates from kube-apiserver. Though that said, you might find it easier to use one of the operator support frameworks like Kubebuilder, or at least directly use the underlying controller-runtime library as raw client-go can be a little gnarly to configure stable informers. Doable either way though.\n"
},
{
  "Question": "I have a umbrella chart and I want to know if it's possible to update an existing helm deployment through my requirements.yaml in my umbrella chart.\n\n\n",
  "Answer": "Not directly. If you did some kind of funky CRD with one of the existing Helm operators then maybe, but overall releases don't know about each other.\n"
},
{
  "Question": "I am using kubernetes and its resources like secrets. During deployment one secret has been created (say test-secret) with some values inside it.\nNow I need to renamed this secretes (dev-secret) within the same namespace.\nHow can I rename the secret or how can I copy test-secret value to dev-secret.\nPlease let me know the correct approach for this.\n",
  "Answer": "There is no specific way to do this. The Kubernetes API does not have \"rename\" as an operation. In this particular case you would kubectl get secret test-secret -o yaml, clean up the metadata: sections that don't apply anymore, edit the name of the secret, and kubectl apply it again.\n"
},
{
  "Question": "I want to deploy eureka server on kubernetes, and want to specify the service name to the clients , so that whenever any client wants to connect to eureka server , it should do it using the service name of eureka server.\n",
  "Answer": "It looks like the official images for Eureka haven\u2019t been updated in years, so I think you\u2019re on your own and will have to work this out from scratch. There are a few Helm charts but they all seem to reference private images and are pretty simple so not sure how much that will help.\n"
},
{
  "Question": "I came across this page regarding the kube auto-scaler: https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-are-the-parameters-to-ca\nFrom this, I can tell that part of the reason why some of my nodes aren't being scaled down, is because they have local-data set on them...\nBy default, the auto-scaler ignores nodes that have local data. I want to change this to false. However, I can't find instructions anywhere about how to change it.\nI can't find any obvious things to edit. There's nothing in kube-system that implies it is about the autoscaler configuration - just the autoscaler status ConfigMap.\nCould somebody help please? Thank you!\n",
  "Answer": "You cannot. The only option the GKE gives is a vague \"autoscaling profile\" choice between the default and \"optimize utilization\". You can, however, override it with per-pod annotations.\n"
},
{
  "Question": "I have a Kubernetes cluster and I am figuring out in what numbers have pods got scaled up using the Kubectl command.\nwhat is the possible way to get the details of all the scaled-up and scaled-down pods within a month?\n",
  "Answer": "That is not information Kubernetes records. The Events system keeps some debugging messages that include stuff about pod startup and sometimes shutdown, but that's only kept for a few hours. For long term metrics look at something like Prometheus + kube-state-metrics.\n"
},
{
  "Question": "I am new to the writing a custom controllers for the kubernetes and trying to understand this. I have started referring the sample-controller https://github.com/kubernetes/sample-controller.\nI want to extend the sample-controller to operate VM resource in cloud using kubernetes. It could create a Vm  if new VM kind resource is detected. Update the sub resources or delete if user want.\nSchema should be like the below:\napiVersion: samplecontroller.k8s.io/v1alpha1\nkind: VM\nmetadata:\nname: sample\nspec:\nvmname: test-1\nstatus:\nvmId: 1234-567-8910\ncpuUtilization: 50 \n\nAny suggestions or help is highly appreciable :)\n",
  "Answer": "Start from https://book.kubebuilder.io/ instead. It's a much better jumping off point than sample-controller.\n"
},
{
  "Question": "I'm running Kubernetes service using exec which have few pods in statefulset. \nIf I kill one of the master pod used by service in exec, it exits with code 137. I want to forward it to another pod immediately after killing or apply wait before exiting. I need help. Waiting for answer. Thank you.\n",
  "Answer": "137 means your process exited due to SIGKILL, usually because the system ran out of RAM. Unfortunately no delay is possible with SIGKILL, the kernel just drops your process and that is that. Kubernetes does detect it rapidly and if you're using a Service-based network path it will usually react in 1-2 seconds. I would recommend looking into why your process is being hard-killed and fixing that :)\n"
},
{
  "Question": "I want to ssh minikube/docker-desktop, but I cant. How can i do that?\nNAME       STATUS   ROLES                  AGE     VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nminikube   Ready    control-plane,master   4m47s   v1.20.2   192.168.49.2   <none>        Ubuntu 20.04.2 LTS   4.19.121-linuxkit   docker://20.10.6\n\nssh minikube\nssh: Could not resolve hostname minikube: nodename nor servname provided, or not known\n\nI am learning K8s and able to ssh while working on K8s hands-on labs available online. I'd like t test some stuff on my local environment.\n",
  "Answer": "minikube is the node name within the Kubernetes API, not a hostname in this case. Minikube offers a wrapper minikube ssh command to automate pulling the IP and whatnot. Docker Desktop does not offer an official way to get a shell in the VM as it's a single-purpose appliance and they want it in a known state, but you can fake it by running a super-superuser container like docker run -it --rm --privileged --pid=host justincormack/nsenter1 to break out into the host namespaces.\n"
},
{
  "Question": "my natural thought is that if nginx is just a daemon process on the k8s node, but not a pod(container) in the k8s cluster, looks like it still can fullfill ingress controller jobs. because:\nif it's a process, because it is on the k8s node, it still can talk to apiserver to fetch service  backend pods information, like IP addresses, so it's still can be used as a http proxy server to direct traffic to different services.\nso 2 questions,\n\nwhy nginx ingress controller has to be a pod?\nwhy nginx ingress controller only got 1 replica? and on which node? if nginx controller pod is dead, things will go unstable.\n\nThanks!\n",
  "Answer": "Because Pods are how you run daemon processes (or really, all processes) inside Kubernetes. That's just how you run stuff. I suppose there is nothing stopping you from running it outside the cluster, manually setting up API configuration and authentication, doing all the needed networking bits yourself. But ... why?\nAs for replicas, you should indeed generally have more than one across multiple physical nodes for redundancy. A lot of the tutorials show it with replicas: 1 because either it's for a single-node dev cluster like Minikube or it's only an example.\n"
},
{
  "Question": "I need to update a file in a container running in k8s using my local editor and save back the updates to the original file in the container without restarting/redeploying the container.\nRight now I do:\n$ kubectl exec tmp-shell -- cat /root/motd > motd && vi motd && kubectl cp motd tmp-shell:/root/motd\n\nIs there some better way to do this?\nI have looked at:\nhttps://github.com/ksync/ksync\nbut seems heavyweight for something this \"simple\".\nNotice:\nI don't want to use the editor that might or might not be available inside the container - since an editor is not guaranteed to be available.\n",
  "Answer": "One option that might be available is ephemeral debug containers however they are an alpha feature so probably not enabled for you at time of writing. Barring that, yeah what you said is an option. It probably goes without saying but this is a very bad idea, might not work at all if the target file isn't writable (which it shouldn't be in most cases) either because of file permissions, or because the container is running in immutable mode. Also this would only matter if the thing using the file will detect the change without reloading.\nA better medium term plan would be to store the content in the ConfgMap and mount it into place. That would let you update it whenever you want.\n"
},
{
  "Question": "I have certain requirements where volumeMounts: should be an optional field. \nspec:\n      volumes:\n        -\n          name: aaa\n          secret:\n            secretName: aaa-certs\n      containers:\n        -\n          name: my-celery\n          volumeMounts:\n            -\n              name: aaa\n              mountPath: /tmp/aaa_certs\n              readOnly: true\n\nIf secret is present then it will mount, else create an empty folder. Is this possible \n",
  "Answer": "No, that is not possible. You would need a higher level system like Helm or an operator to manage that kind of dynamic configuration.\n"
},
{
  "Question": "How do I get rid of them? Docker doesn't think they exist and my kubernetes-fu isn't good enough yet.\n\n",
  "Answer": "You cannot remove them while they are in use. You'll have to shut down the Kubernetes system first, that's under Preferences (the gear icon) and then Kubernetes. Once that is done, run docker system prune to clean up unused everything.\n"
},
{
  "Question": "I've started to use kustomize to update the yaml files. \nI'm trying to add labels to Deployment object. commonLabels seems pretty good but it applies the label to selector as well (which I don't want)\nIs there a way to add new label only to metadata.labels or spec.template.metadata.labels?\nhere's a sample deployment object :\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  labels:\n    service: servieName\n    owner: ownerName\n  name: myName\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n         app: servieName\n         owner: ownerName\n         <newLableHere>: <newValueHere>\n ...\n\n\n",
  "Answer": "You would need to define a patch for that object specifically.\n"
},
{
  "Question": "I need to create pods on demand in order to run a program. it will run according to the needs, so it could be that for 5 hours there will be nothing running, and then 10 requests will be needed to process, and I might need to limit that only 5 will run simultaneously because of resources limitations.\nI am not sure how to build such a thing in kubernetes.\nAlso worth noting is that I would like to create a new docker container for each run and exit the container when it ends.\n",
  "Answer": "There are many options and you\u2019ll need to try them out. The core tool is HorizontalPodAutoscaler. Systems like KEDA build on top of that to manage metrics more easily. There\u2019s also Serverless tools like knative or kubeless. Or workflow tools like Tekton, Dagster, or Argo.\nIt really depends on your specifics.\n"
},
{
  "Question": "I want to create an environment variable in the pod created by my sts which would consist of its FQDN.\nAs the dns domain - \".cluster.local\" can be different for various clusters, like '.cluster.abc', or '.cluster.def', I want to populate that dynamically based on the cluster settings.\nI checked that .fieldRed.fieldPath doesn't consist of anything like it.\nIs there any other option to do that?\n",
  "Answer": "This is not a feature of Kubernetes. You would need to use a higher level templating system like Kustomize or Helm.\n"
},
{
  "Question": "I'm aware that I can use k8s node affinity to select for nodes that have a value of a label gt than some value. However is it possible to make k8s prefer to schedule on nodes that have the greatest value? I.e. if one node has schedule-on-me: 5 and another has schedule-on-me: 6 it will prefer to schedule on the one with the higher value?\n",
  "Answer": "Not directly. You can tune weights in the scheduler but I don't think any those would help here so would need a custom scheduler extender webhook.\n"
},
{
  "Question": "DockerFile\nFROM centos\nRUN yum install java-1.8.0-openjdk-devel -y\nRUN curl --silent --location http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo |  tee /etc/yum.repos.d/jenkins.repo\nRUN rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key\nRUN yum install jenkins --nogpgcheck -y\nRUN yum install jenkins -y\nRUN yum install -y initscripts\nCMD /etc/init.d/jenkins start && /bin/bash\n\nthe output of describing command\nenter image description here\noutput of logs\nStarting Jenkins [  OK  ]\n",
  "Answer": "There isn't an init system inside a container so this isn't going to work. Likely the specific issue is that with plain Docker you are using docker run -it so there is a stdin, so bash starts in interactive mode and keeps running. In Kubernetes there is no input so bash exits immediately and the container exits with it. You can't run stuff in the background like that. Maybe just use the official jenkins/jenkins image? Or at least check out how it's built.\n"
},
{
  "Question": "When kops is used to create a k8s cluster, a /srv/kubernetes folder gets created and distributed to all the nodes, populated with files automatically created by the provisioning process. \nMy question is whether it's possible for the cluster admin to add files to this volume so that such files can be referenced by passing command-line arguments to kubernetes processes? If so, how to add files so that they are ready when the nodes boot up?\n",
  "Answer": "Use the fileAssets key in your Cluster manifest:\nfileAssets:\n  - content: |\n      asdf\n    name: something\n    path: /srv/kubernetes/path/to/file\n\n"
},
{
  "Question": "\nHow to change the Docker ENTRYPOINT in a Kubernetes deployment, without changing also the Docker CMD?\n\nIn the Pod I would do\nimage: \"alpine\"\nentrypoint: \"/myentrypoint\"\n\nbut this overwrites either ENTRYPOINT and the CMD from the Dockerfile.\nThe documentation doesn't mention it, but seems like a big use-case missed.\n",
  "Answer": "That's not a thing.\n\nENTRYPOINT (in Dockerfile) is equal to command: (in PodSpec)\nCMD (in Dockerfile) equals args: (in PodSpec)\n\nSo just override command but not args.\n"
},
{
  "Question": "I have applications needs to give each pod a public ip and expose ports on this public ip.\nI am trying not to use virtual machines.\nmatellb has similar feature. But, it binds a address to a service not pod. And, it wastes a lot of bandwidth.\n",
  "Answer": "Technically this is up to your CNI plugin, however very few support this. Pods generally live in the internal cluster network and are exposed externally through either NodePort or LoadBalancer services, for example using MetalLB. Why do you think this \"wastes bandwidth\"? If you're concerned about internal rerouting, you may want to enable externalTrafficPolicy: Local to reduce internal bounces but your internal network probably has a lot more bandwidth available than your internet connection so it that's not usually a reason to worry.\n"
},
{
  "Question": "I want to run two commands in my cronjob.yaml one after each other. The first command runs a python-scipt and the second changes an environment variable in another pod. The commands added separately work.\nThis is what I'm trying right now (found the syntax in How to set multiple commands in one yaml file with Kubernetes? ) but it gives me an error.\ncommand:\n- \"/bin/bash\"\n- \"-c\"\nargs: [\"python3 recalc.py && kubectl set env deployment recommender --env=\"LAST_MANUAL_RESTART=$(date)\" --namespace=default\"]\n\nThe error I get in cloudbuild:\nerror converting YAML to JSON: yaml: line 30: did not find expected ',' or ']'\n\n(for the long line)\n",
  "Answer": "You have nested double quotes, try something more like this:\ncommand:\n- /bin/bash\n- -c\n- python3 recalc.py && kubectl set env deployment recommender --env=\"LAST_MANUAL_RESTART=$(date)\" --namespace=default\n\ni.e. without the outer double quotes.\n"
},
{
  "Question": "Currently I have a Spring containers running in a Kubernetes cluster.  I am going through Udacity's Spring web classes and find the Eureka server interesting.\nIs there any benefit in using the Eureka server within the cluster?\nany help will be appreciated.\nThank you\n",
  "Answer": "This is mostly an option question but ... probably not? The core Service system does most of the same thing. But if you're specifically using Eureka's service metadata system then maybe?\n"
},
{
  "Question": "Declarative definitions for resources in a kubernetes cluster such as Deployments, Pods, Services etc.. What are they referred to as in the Kubernetes eco-system? \nPossibilities i can think of: \n\nSpecifications (specs)\nObjects\nObject configurations\nTemplates\n\nIs there a consensus standard?\n\nBackground\nI'm writing a small CI tool that deploys single or multiple k8s YAML files. I can't think of what to name these in the docs and actual code.\n",
  "Answer": "The YAML form is generally a manifest. In a Helm chart they are templates for manifests (or more often just \"templates\"). When you send them to the API you parse the manifest and it becomes an API object. Most types/kinds (you can use either term) have a sub-struct called spec (eg. DeploymentSpec) that contains the declarative specification of whatever that type is for. However that is not required and some core types (ConfigMap, Secret) do not follow that pattern.\n"
},
{
  "Question": "I am deploying Metrics Server https://github.com/kubernetes-sigs/metrics-server\nand notice there is one Requirements in README:\nMetrics Server must be reachable from kube-apiserver by container IP address (or node IP if hostNetwork is enabled).\nWonder how to make it except deploying cni in kube-apiserver host or setting container network to hostNetwork?\n",
  "Answer": "You do it by doing it, there's no specific answer. If you run kube-apiserver within the cluster (e.g. as a static pod) then this is probably already the case. Otherwise you have to arrange for this to work in whatever networking layout you have.\n"
},
{
  "Question": "I know a scenario of kubernetes headless service with selector.\nBut what\u2019s the usage scenario of kubernetes headless service without selector?\n",
  "Answer": "Aliasing external services into the cluster DNS.\n"
},
{
  "Question": "Is there a way to configure the minikube cluster to automatically pull \"all\" the latest docker images from GCR for all the pods in the cluster and restarted those pods once you start your minikube cluster?\n",
  "Answer": "You can use the AlwaysPullImages admission controller which forces the imagePullPolicy to Always which will repull images on pod restart. And then just restart all your pods.\n"
},
{
  "Question": "Assuming that the pods have exposed port 80.\nHow to send a requets to all the running pods, rather than 1.\nSince the load balancer would route the traffic to only 1 pod. \n(Note : using HAproxy load balancer here, FYI)\n",
  "Answer": "There is no particular way, kubectl exec only works one container at a time so you will need to call it into a loop if you want to use it on many.\n"
},
{
  "Question": "apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n name: testingHPA\n\nspec:\n scaleTargetRef:\n   apiVersion: apps/v1beta1\n   kind: Deployment\n   name: my_app\n minReplicas: 3\n maxReplicas: 5\n targetCPUUtilizationPercentage: 85\n\nAbove is the normal hpa.yaml structure, is it possible to use kind as a pod and auto scale it ??\n",
  "Answer": "A single Pod is only ever one Pod. It does not have any mechanism for horizontal scaling because it is that mechanism for everything else.\n"
},
{
  "Question": "GKE uses Calico for networking by default. Is there an option to use some other CNI plugin ?\n",
  "Answer": "No, GKE does not offer such option, you have to use the provided Calico.\n"
},
{
  "Question": "An existing Pod(P) is running 3 containers for API.\nTo scale Pod P horizonatally,\nIs it possible to add one(or n) more container to an existing Pod(running 3 containers)?\nor\nIs Pod replica set concept supposed to be applied for this scenario(to scale horizontally)?\n",
  "Answer": "No, you don't use multi-container Pods for scaling. Pods with multiple containers are for cases where you need multiple daemons running together (on the same hardware) for a single \"instance\". That's pretty rare for new users so you almost certainly want 3 replicas of a Pod with one container.\n"
},
{
  "Question": "By default most people seem to avoid running anything on the masters. These nodes are less likely to be re-provisioned or moved around than the rest of the cluster. It would make them a perfect fit for ingress controllers.\nIs there any security and/or management implications/risks in using the masters as ingress nodes?\n",
  "Answer": "As always, the risk is that if your Ingress Controller eats all your IOPS (or memory or CPU but in this case it would probably be IOPS) then your control plane can go unavailable, leaving you fewer ways to fix the problem.\n"
},
{
  "Question": "I have a problem: I use eviction policies (evict-soft and evict-hard) and when my pods are being evicted out beacause of resource lack on one node, pod dies and starts on another node, so, that during this time service is down. What can I do to make pod first start on another node before being killed on the first?\n",
  "Answer": "Run two of them and use a pod disruption budget so it won\u2019t do a soft evict of both at the same time (or use affinity settings so they run on different nodes, or both).\n"
},
{
  "Question": "I currently have a single-node Kubernetes instance running on a VM. The disk attached to this VM is 100GB, but 'df -h' shows that the / partition only has 88GB available (other stuff is used for OS overhead etc...)\nI have a kubernetes manifest that creates a 100GB local Persistent Volume.\nI also have a pod creating a 100GB Persistent Volume Claim.\nBoth of these deploy and come up normally even though the entire VM does not even have 100GB available.\nTo make things even more complicated, the VM is thin provisioned, and only using 20 GB on the actual disk right now...\nHOW IS THIS WORKING !?!?\n",
  "Answer": "The local provisioner does no size checks, nor is the size enforced anyway. The final \"volume\" is just a bind mount like with hostPath. The main reason local PVs exist is because hostPath isn't something the scheduler understands so in a multi-node scenario it won't restrict topology.\n"
},
{
  "Question": "I have a ansible script to deploy my microservices to the customer cluster... I have a lot of k8s deployment files, one for each microservice. It's a good idea to deploy a PVCs and PVs to the customer cluster?\n",
  "Answer": "The feature does what it says in docs. If that is a good idea depends entirely on your needs and how you use it.\n"
},
{
  "Question": "I have a deployment with two containers, let's say A and B.\ncontainer A serves http requests, the important container in the deployment. has liveness and readiness probes properly set.\nContainer B serves as a proxy to a certain third party service through an ssh tunnel, has a very specific usecase, but for whatever reason, the third party service can be unreachable which puts this container on a crash loop.\nThe question is: How can I make this pod serve requests even if Container B is on a crashloop?\nTLDR; How to make a deployment serve requests if a specific container is on a crashloop?\n",
  "Answer": "You kind of can't. You could remove the readiness probe from B but any kind of crash is assumed to be bad. Probably your best bet is to change B so that it doesn't actually crash out to where the kubelet can see it. Like put a little while true; do theoriginalcommand; sleep 1; done bash loop on it or something so the kubelet isn't aware of it crashing. Or just make it not crash ...\n"
},
{
  "Question": "Quick question regarding Kubernetes job status.\nLets assume I submit my resource to 10 PODS and want to check if my JOB is completed successfully.\nWhat is the best available options that we can use from KUBECTL commands.\nI think of kubectl get jobs but the problem here is you have only two codes 0 & 1. 1 for completion 0 for failed or running, we cannot really depend on this\nOther option is kubectl describe to check the PODS status like out of 10 PODS how many are commpleted/failed.\nAny other effective way of monitoring the PODs? Please let me know\n",
  "Answer": "Anything that can talk to the Kubernetes API can query for Job object and look at the JobStatus field, which has info on which pods are running, completed, failed, or unavailable. kubectl is probably the easiest, as you mentioned, but you could write something more specialized using any client library if you wanted/needed to.\n"
},
{
  "Question": "I'm trying to support two systems both of which pull  images from private repos. One is from Kubernetes - which needs imagePullSecrets via a mounted secret and the other just needs standard docker login.\nBased on this - Pulling images from private repository in kubernetes without using imagePullSecrets - it does not appear there's a way to use injected values to pull an image inside of Kubernetes - am I wrong?\nWhat i'd really like is for both Kubernetes/Kubeflow and the other system to just both get an array of values (server, login, password, email) and be able to pull a private image.\n",
  "Answer": "You can handle both by doing the login at the lower level of dockerd or containerd on the host itself. Otherwise not really, other than mounting the image pull secret into the container if it will respect a dockerconfig.\n"
},
{
  "Question": "I have kubernetes HA environment with three masters. Just have a test, shutdown two masters(kill the apiserver/kcm/scheduler process), then only one master can work well. I can use kubectl to create a deployment successfully ,some pods were scheduled to different nodes and start. So can anyone explain why it is advised odd number of masters? Thanks.\n",
  "Answer": "Because if you have an even number of servers, it's a lot easier to end up in a situation where the network breaks and you have exactly 50% on each side. With an odd number, you can't (easily) have a situation where more than one partition in the network thinks it has majority control.\n"
},
{
  "Question": "I have a kubectl config map like below.\napiVersion: v1\ndata:\n  server.properties: |+\n    server.hostname=test.com\n\nkind: ConfigMap\nmetadata:\n  name: my-config\n\nAnd I tried to read this config inside a container.\ncontainers:\n        - name: testserver\n          env:\n            - name: server.hostname\n              valueFrom:\n                configMapKeyRef:\n                  name: my-config\n                  key: server.properties.server.hostname\n\nHowever, these configs are not passing to the container properly. Do I need do any changes to my configs?\n",
  "Answer": "What you have in there isn't the right key. ConfigMaps are strictly 1 level of k/v pairs. The |+ syntax is YAML for a multiline string but the fact the data inside that is also YAML is not something the system knows. As far as Kubernetes is concerned you have one key there, server.properties, with a string value that is opaque.\n"
},
{
  "Question": "I am debugging certain behavior from my application pods; i am launching on K8s cluster. In order to do that I am increasing logging by increasing verbosity of deployment by adding --v=N flag to Kubectl create deployment command.\nmy question is : how can i configure increased verbosity globally so all pods start reporting increased verbosity; including pods in kube-system name space.\ni would prefer if it can be done without re-starting k8s cluster; but if there is no other way I can re-start.\nthanks\nAnkit \n",
  "Answer": "For your applications, there is nothing global as that is not something that has global meaning. You would have to add the appropriate config file settings, env vars, or cli options for whatever you are using.\nFor kubernetes itself, you can turn up the logging on the kubelet command line, but the defaults are already pretty verbose so I\u2019m not sure you really want to do that unless you\u2019re developing changes for kubernetes.\n"
},
{
  "Question": "For Traefik 1.7 as an ingress controller, how to match HTTP Method to route to a specific service?\nFor example, I want to pass a request to the service, only if HTTP method is GET and it matches with provided path.\nI am looking documentation at: https://doc.traefik.io/traefik/v1.7/configuration/backends/kubernetes/\nBut cannot find any relevant annotation. Is there any possible workaround?\n[I found a similar question: Restrict allowed methods on Traefik routes  but answered to handle CORS]\n",
  "Answer": "Nope, that was not a feature of 1.7.\n"
},
{
  "Question": "recently I'm trying to create ingress gateways per API I create, I need to know that is there any limit for that, or can we create as many as I want?\n",
  "Answer": "The exact answer depends on which Ingress Controller you are using, but if there are limits for any of them, expect them to be in the billions (like the kind of limit where 2^32 or 2^64 are involved because there's an integer index on something somewhere).\n"
},
{
  "Question": "As per: https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/\nI'm trying to install ingress-nginx with custom ports, but it does not expose those ports when I pass in the controller.customPorts parameter. I think I'm not passing it in the right format. The documentation says\nA list of custom ports to expose on the NGINX ingress controller pod. Follows the conventional Kubernetes yaml syntax for container ports.\n\nCan anyone explain to me what that format should be?\n",
  "Answer": "Assuming they mean what shows up in Pod definitions:\n- port: 1234\n  name: alan\n\n"
},
{
  "Question": "We are moving towards Microservice and using K8S for cluster orchestration. We are building infra using Dynatrace and Prometheus server for metrics collection but they are yet NOT in good shape.\nOur Java Application on one of the Pod is not working. I want to see the application logs.\nHow do I access these logs? \n",
  "Answer": "Assuming the application logs to stdout/err, kubectl logs -n namespacename podname.\n"
},
{
  "Question": "Why does kubernetes (using minikube) use dns for nginx's proxy_pass but not rewrite?\nKubernetes replaces auth-proxy-service.default with ip address\n  location /widget-server/ {\n    proxy_pass http://auth-proxy-service.default:5902/;\n  }\n\nkubernetes does not replace auth-proxy-service.default with ipaddress and the url in browser actually shows http://auth-proxy-service.default:5902/foo\n  location /form-builder-auth {\n    rewrite ^(.*)$ http://auth-proxy-service.default:5902/foo redirect;\n  }\n\n",
  "Answer": "Because that's what a redirect means? Reverse proxies are transparent, redirects are user-facing.\n"
},
{
  "Question": "How to list the current deployments running in Kubernetes with custom columns displayed as mentioned below:\nDEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE\nThe data should be sorted by the increasing order of the deployment name.\n",
  "Answer": "Look a the -o custom-columns feature. https://kubernetes.io/docs/reference/kubectl/overview/#custom-columns shows the basics. The hard one would be container_image, since a pod can contain more than one, but assuming you just want the first, something like .spec.template.containers[0].image? Give a shot and see how it goes.\n"
},
{
  "Question": "I am new to Kubernetes.  Setting up nginx-ingress in a test cluster. One of our senior people rolled by and noticed the following.\n# kubectl get services\n...\ningress-ingress-nginx-controller-admission   ClusterIP      xx.xxx.xxx.xxx   <none>        443/TCP\n...\n\nWhat's that, he asked.  Get rid of it if you don't need it.\nBefore I rip it out and maybe get cripple my test cluster .. what is ingress-nginx-controller-admission and why do I need it?\n",
  "Answer": "It's the service for the validating webhook that ingress-nginx includes. If you remove it, you'll be unable to create or update Ingress objects unless you also remove the webhook configuration.\ntl;dr it's important, no touchy\n"
},
{
  "Question": "I am doing the end of course work and I would like to know if I can make a regular volume a shared directory with ftp, that is to say that when you mount the disk kubeneretes takes the directory of the external ftp server.\nI know it can be done with NFS but I would like to do it with SFTP.\nThanks in advance.\n",
  "Answer": "There is code floating around for a FlexVolume plugin which delegates the actual mount to FUSE: https://github.com/adelton/kubernetes-flexvolume-fuse\nBut I have no idea if that will even compile anymore, and FlexVolume is on its way out in favor of CSI. You could write a CSI plugin on top of the FUSE FS but I don't know of any such thing already existing.\nMore commonly what you would do is use a RWX shared volume (such as NFS) and mount it to both the SFTP server and whatever your hosting app pod is.\n"
},
{
  "Question": "We are storing secrets in GCP Secret Manager, during an app deployment we using an init container which fetches secrets and places them in volume (path). Going forward we need the requirement is to load the secrets as env variable on the main container needing the secrets from the init container, instead of the paths. How can it be achieved ? Any workaround ?\nThank you !\n",
  "Answer": "You can copy from GSM into a Kubernetes Secret and then use that in a normal envFrom or you can have the init container write a file into a shared emptyDir volume and then change the command on the main container to be something like command: [bash, -c, \"source /shared/env && exec original command\"]. The latter requires you rewrite the command fully though which is annoying.\n"
},
{
  "Question": "I'd like to understand when it's better to favor a Custom Initializer Controller vs a Mutating Webhook.\nFrom what I can gather, webhooks are:\n\nMore powerful (can trigger on any action).\nMore performant (only persist to etcd once).\nEasier to write (subjective, but production grade controllers aren\u2019t trivial).\nLess likely to break during complete redeploy (there seems to be a chicken-and-egg problem requiring the deployment to exist before the initializer is in place, or the initializer will block the deployment).\n\nWhen would I want an initializer instead?  \u2026or are they being abandoned in favor of webhooks?\n",
  "Answer": "Always favor webhooks. Initializers are unlikely to ever graduate from alpha, and will probably be removed as the apimachinery team dislikes the approach. They might remain in a few specialized cases like Namespaces, but not in general.\n"
},
{
  "Question": "What is the best way to restart all pods in a cluster? I was thinking that setting a cronjob task within kubernetes to do this on a normal basis and make sure that the cluster is load balanced evenly, but what is the best practice to do this on a normal basis? Also, what is the best way to do this as a one-time task?\n",
  "Answer": "This is a bad idea. Check out https://github.com/kubernetes-sigs/descheduler instead to do it selectively and with actual analysis :)\nBut that said, kubectl delete pod --all --all-namespaces or similar.\n"
},
{
  "Question": "I need to monitor the creation of new namespaces in my k8s infrastructure so that when new namespaces are created a series of commands should be executed like assigning permissions and pvc creation, could someone help me?\n",
  "Answer": "Yes, it is possible. But showing how to do it is way out of scope for a StackOverflow answer. The short answer is you do it just like any other controller but the root object is something from Kubernetes core rather than your own struct(s).\n"
},
{
  "Question": "I made a deployment and scaled out to 2 replicas. And I made a service to forward it.\nI found that kube-proxy uses iptables for forwarding from Service to Pod. But the load balancing strategy of iptables is RANDOM.\nHow can I force my service to forward requests to 2 pods using round-robin strategy without switching my kube-proxy to userspace or ipvs mode?\n",
  "Answer": "You cannot, the strategies are only supported in ipvs mode. The option is even called --ipvs-scheduler.\n"
},
{
  "Question": "I have a docker image. I want to analyze the docker image history, for this I can use docker image history command in the docker installed environment.\nBut when am working in a Openshift cluster, I may not have the access to the docker command here. So here I want get the docker history command result for the given image.\nSo basically I have a docker image and I don't have docker installed there. In this case how can we get the history of that docker image?\nCan anyone please help me on this?\n",
  "Answer": "You can get the registry info either via curl or skopeo inspect. But the rest of the metadata is stored inside the image itself so you do have to download at least the final layer.\n"
},
{
  "Question": "kubectl explain serviceaccount.secrets describes ServiceAccount Secrets as the secrets allowed to be used by Pods running using this ServiceAccount, but what effect does adding a Secret name to this list have?\nThe ServiceAccount token Secret (which is automatically added to this list) gets automatically mounted as a volume into all containers in a Pod running using this ServiceAccount (as long as the ServiceAccount admission controller is enabled), but what happens for other secrets?\n",
  "Answer": "It holds the name of all secrets containing tokens for that SA so when the controller goes to rotate things, it knows where to find them.\n"
},
{
  "Question": "I create a secret and mount it as file in pod(config in deployment).\nIf I change the secret, the mounted file will be changed in few seconds.\nBut how can I check the file is updated actually? I don't want to exec into pod and check it , because I want to check it by k8s api or k8s resource status. Is there any way to do that?\n",
  "Answer": "You wouldn't really in general check that Kubernetes is not broken. Unless you think you've found a bug, in which case you would use kubectl exec and probably many other things to try and track it down.\n"
},
{
  "Question": "When trying to run command:\nkubectl get deployments\n\nI get this message:\nC:\\Users\\win 10\\AppData\\Local\\Google\\Cloud SDK\\helloworld-gke>kubectl rollout status deployment helloworld-gke\nWaiting for deployment \"helloworld-gke\" rollout to finish: 0 of 1 updated replicas are available...\n\nand nothing has happened since then, is this a freeze or it is taking time to deploy ?\n",
  "Answer": "You gave an invalid docker image name in your deployment so it can\u2019t succeed.\n"
},
{
  "Question": "I have K8s deployed on an EC2 based cluster,\nThere is an application running in the deployment, and I am trying to figure out the manifest files that were used to create the resources,\nThere were deployment, service and ingress files used to create the App setup.\nI tried the following command, but I'm not sure if it's the correct one as it's also returning a lot of unusual data like lastTransitionTime, lastUpdateTime and status-\nkubectl get deployment -o yaml\n\nWhat is the correct command to view the manifest yaml files of an existing deployed resource?\n",
  "Answer": "There is no specific way to do that. You should store your source files in source control like any other code. Think of it like decompiling, you can do it, but what you get back is not the same as what you put in. That said, check for the last-applied annotation, if you use kubectl apply that would have a JSON version of a more original-ish manifest, but again probably with some defaulted fields.\n"
},
{
  "Question": "I have 3 replicas of same application running in kubernetes and application exposes an endpoint. Hitting the endpoint sets a boolean variable value to true which is having a use in my application. But the issue is, when hitting the endpoint, the variable is updated only in one of the replicas. How to make the changes in all replicas just by hitting one endpoint?\n",
  "Answer": "You need to store your data in a shared database of some kind, not locally in memory. If all you need is a temporary flag, Redis would be a popular choice, or for more durable stuff Postgres is the gold standard. But there's a wide and wonderful world of databases out there, explore which match your use case.\n"
},
{
  "Question": "can it be  that  process inside container used  more  memory then the container  itself?\ni have a  pod with single container, that based on stackdriver graphs uses 1.6G memory at his peak.\nat the same time, i saw an error on the container and while looking the root casue i saw on the VM itself oom-killer message that indicate one of the processes inside the container  killed due to usage of 2.2G. (rss)\nhow can it be?\nMemory cgroup out of memory: Killed process 2076205 (chrome) total-vm:4718012kB, anon-rss:2190464kB, file-rss:102640kB, shmem-rss:0kB, UID:1001 pgtables:5196kB oom_score_adj:932\n\n10x!\n",
  "Answer": "Two pieces. First what you see in the metrics is probably the working set size, which does not include buffers while I think the oom_killer shows rss which does. But more importantly, the data in metrics output is sampled, usually every 30 seconds. So if the memory usage spiked suddenly, or even if it just tried to allocate one huge buffer, then it would be killed.\n"
},
{
  "Question": "I want to add some flags to change sync periods. can I do it with minikube and kubectl? Or will I have to install and use kubeadm for any such kind of initialization? I refered the this link.\nI created and ran the yaml file but there was an error stating that\n\nerror: unable to recognize \"sync.yaml\": no matches for kind \"ClusterConfiguration\" in version \"kubeadm.k8s.io/v1beta2\"\n\nsync.yaml that I have used to change the flag (with minikube):\napiVersion: kubeadm.k8s.io/v1beta2\nkind: ClusterConfiguration\nkubernetesVersion: v1.16.0\ncontrollerManager:\n  extraArgs:\n    horizontal-pod-autoscaler-sync-period: \"60\" \n\n",
  "Answer": "Minikube and kubeadm are separate tools, but you can pass custom CLI options to minikube control plane components as detailed here https://minikube.sigs.k8s.io/docs/handbook/config/#modifying-kubernetes-defaults\nminikube start --extra-config=controller-mananger.foo=bar\n\n"
},
{
  "Question": "I an changed my kubernetes cluster(v1.15.2) configmap, now I want to make my config apply to all of my deployment in some namespace. What is the best practice to do? I am tried to do like this:\nkubectl rollout restart deployment soa-report-consumer\n\nbut my cluster has so many deployment, should I write shell script to complete this task, any simple way?\n",
  "Answer": "The usual fix for this is to use some automation from a tool like Kustomize or Helm so that the deployments automatically update when the config data changes.\n"
},
{
  "Question": "Is it possible to block egress network access from a sidecar container?\nI'm trying to implement capability to run some untrusted code in a sidecar container exposed via another trusted container in same pod having full network access.\nIt seems 2 containers in a pod can't have different network policies. Is there some way to achieve similar functionality?\nAs a sidenote, I do control the sidecar image which provides runtime to untrusted code.\n",
  "Answer": "You are correct, all containers in a pod share the same networking so you can't easily differentiate it. In general Kubernetes is not suitable for running code you assume to be actively malicious. You can build such a system around Kubernetes, but K8s itself is not nearly enough.\n"
},
{
  "Question": "I am planning to deploy Keycloak on my K8S cluster but for the moment not in cluster mode as described on https://www.keycloak.org/2019/04/keycloak-cluster-setup.html. PostgresSQL will be use as data storage for it.  \nMy plan is:\n \nCreate a pod with Keycloak and PostgreSQL inside. Deployment replicas will be 1, because for the moment I do not need clustering.    \nI know, that it is recommended to run one container in one pod but for my purpose will be acceptable to run 2 containers in one pod?\n",
  "Answer": "No, you should only run things in the same pod if there is no way to not do that. In this case the alternative is run separate pods so you should do that.\n"
},
{
  "Question": "I have a Kubernetes cluster with an undefined series of services, and what I want to do is serve each service on an endpoint, with the ability to add new services at any time, with them still being available at an endpoint.\nI'm looking for a way to set, in my ingress, a wildcard on serviceName, so that /xx will be routed to service xx, /yy to service yy, etc.\nAnother solution that I could also use would be matching http://xx.myurl.com to service xx.\nIs this something doable with Kubernetes?\nI imagine something of the like\n- path: /(.*)\n  backend:\n    serviceName: $1\n    servicePort: 80\n\nThanks,\nColin\n",
  "Answer": "This is not something the Ingress system supports. Some other tools may, you can do this pretty easily with a static Nginx config for example.\n"
},
{
  "Question": "I am trying to purge images from the local kubernetes cache on a set cadence. Before you could setup some volumeMounts on a daemonSet and talk to the docker runtime directly.\nThe latest runtime is based on containerd but I can't seem to connect using the containerd.sock - when I run ctr image ls or nerdctl it shows as nothing running or images on the node. It also returns no errors.\nIs there a different method for manually purging from the containerd runtime running on a daemonSet?\n",
  "Answer": "Answered in comments, most containerd commands are built for the Docker integration which uses the default containerd namespace (note, nothing to do with Linux namespaces, this is administrative namespacing inside containerd). Most commands have an option to set the ns being used but crictl is already set up for the CRI namespace that Kubernetes uses (because it's also a CRI client).\n"
},
{
  "Question": "I have a case where I want to set up an alert where at least one value of the label is distinct.\nFor example, a Kubernetes cluster xyz (having 20 nodes) with metric test_metric{cluster_name=\"xyz\",os=\"ubuntu\"}. I want to find out/setup an alert if any of these 20 nodes are having different \"os\" values.\nBasically, the idea is to get an alert when the os's value is not the same across all nodes in a cluster.\nAt the moment I am testing a very simple rule which I think is not correct:\ncount(test_metric{cluster_name=\"xyz\",os!=\"\"} != count(test_metric{cluster_name=\"xyz\",os!=\"\"})\n\n",
  "Answer": "Nested counts is the way to handle this:\ncount by (cluster_name) (\n   count by (os, cluster_name)(test_metric)\n) != 1\n\n"
},
{
  "Question": "I am trying to understand default prometheus rules for kubernetes. And then I came across with this expression:\n      sum(namespace_cpu:kube_pod_container_resource_requests:sum{})\n        /\n      sum(kube_node_status_allocatable{resource=\"cpu\"})\n        >\n      ((count(kube_node_status_allocatable{resource=\"cpu\"}) > 1) - 1) / count(kube_node_status_allocatable{resource=\"cpu\"})\n\nSpecifically, i am curious at namespace_cpu:kube_pod_container_resource_requests:sum{}. namespace_cpu does not appear to be an operation or reserved word in promql.\nI can't seem to find it either in Prometheus documentation. https://prometheus.io/docs/prometheus/latest/querying/basics/\nAny hints?\n",
  "Answer": "Nothing, it's not an operator : is just a legal character in metric names. Some standard rulesets use it for grouping rollup rules together but it's just a naming scheme at most.\n"
},
{
  "Question": "I have an application that needs to know it's assigned NodePort. Unfortunately it is not possible to write a file to a mountable volume and then read that file in the main container. Therefore, i'm looking for a way to either have the initContainer set an environment variable that gets passed to the main container or to modify the main pod's launch command to add an additional CLI argument. I can't seem to find any examples or documentation that would lead me to this answer. TIA.\n",
  "Answer": "There's no direct way so you have to get creative. For example you can make a shared emptyDir mount that both containers can access, have the initContainer write export FOO=bar to that file, and then change the main container command to something like [bash, -c, \"source /thatfile && exec originalcommand\"]\n"
},
{
  "Question": "I know about snapshots and tested volume cloning. And it works, when storage class is the same.\nBut what if I have two storage classes: one for fast ssd and second for cold storage hdd over network and I want periodically make backup to cold storage? How to do it?\n",
  "Answer": "This is not a thing Kubernetes supports since it would be entirely up to your underlying storage. The simple version would be a pod that mounts both and runs rsync I guess?\n"
},
{
  "Question": "I have a cluster on GKE currently on version v1.19.9-gke.1400. Accordingly do kubernetes release notes, on 1.20 dockershim will be deprecated. My cluster is configured to auto-upgrades and in one specific application I use docker socket mapped to the application, where I run direct containers through their API.\nMy question: In a hypothetical upgrade of the cluster to the 1.20 version of kubernetes, the docker socket will be unavailable immediately? Or the deprecated flag only points that in the future it will be removed?\n",
  "Answer": "Yes, if you use the non-containerd images. In the node pool config you can choose which image type you want and COS vs COS_Containerd are separate choices there. At some point later in 2021 we may (if all goes according to plan) remove Docker support in Kubernetes itself for 1.23. However Google may choose to remove support one version earlier in 1.22 or continue it later via the out-of-tree Docker CRI that Mirantis is working on.\nI am running 1.20 in the Rapid channel and can confirm that Docker is still there and happy. Also FWIW if you need to run dockerd yourself via a DaemonSet it takes like 30 seconds to set up, really not a huge deal either way.\n"
},
{
  "Question": "I have a kubernetes cluster and I've been experimenting so far with cert-manager and letsencrypt ssl certificates.\nEverything goes fine, I have issued an SSL certificate and applied to the cluster and https connection is working excellent.\nThe problem I face is that I am experimenting with new things and it often leads me to delete the whole cluster and create a new fresh one, which on it's side makes me lose the SSL certificate and issue a new one, but there's a rate limit by 50 certificates per week per domain.\nIs there a way I can reuse a certificate in a new k8s cluster?\n",
  "Answer": "Copy the secret locally (kubectl get secret -o yaml and then clean up unneeded fields) and then upload it to the new cluster (kubectl apply).\n"
},
{
  "Question": "I have a service deployed on Kubernetes and it has url app.io (using ingress).\nWhat if I need a user to every time go to app.io and:\n\nif it's running okay with no errors, it redirects to the app.io (on k8s)\n\nand if not running well or have an error, it would redirect on a backup service on Heroku for example with url backup.io.\n\n\nHow can I do that?\nThanks in advance\n",
  "Answer": "Fallback routing like you describe is not part of the Ingress standard. It only does routing based on incoming Host header and request path. It's possible some specific Ingress Controller supports this as a custom extension but I don't know of any that do.\n"
},
{
  "Question": "It's probably something obvious but I don't seem to find a solution for joining 2 vectors in prometheus.\nsum(\n  rabbitmq_queue_messages{queue=~\".*\"}\n) by (queue)\n* \non (queue) group_left max(\n  label_replace(\n    kube_deployment_labels{label_daemon_name!=\"\"},\n    \"queue\",\n    \"$1\",\n    \"label_daemon_queue_name\",\n    \"(.*)\"\n  )\n) by (deployment, queue)\n\nBelow a picture of the output of the two separate vectors.\n\n",
  "Answer": "Group left has the many on the left, so you've got the factors to the * the wrong way around. Try it the other way.\n"
},
{
  "Question": "I work with API kuberneteswith (library is @kubernetes/client-node).\nI can to get a list of pods of specigic namespace, but i don`t understand to get a list of name all namespaces\nHow i may code with @kubernetes/client-node?\n",
  "Answer": "In the corev1 API, it's listNamespace.\nconst k8s = require('@kubernetes/client-node');\n\nconst kc = new k8s.KubeConfig();\nkc.loadFromDefault();\n\nconst k8sApi = kc.makeApiClient(k8s.CoreV1Api);\n\nk8sApi.listNamespace().then((res) => {\n    console.log(res.body);\n});\n\n"
},
{
  "Question": "I want to execute set in a pod, to analyze the environment variables:\nkubectl exec my-pod -- set\n\nBut I get this error:\nOCI runtime exec failed: exec failed: container_linux.go:370: starting container process caused: exec: \"set\": executable file not found in $PATH: unknown\n\nI think, this is a special case, because there's no executable set like there's for example an execute ls.\nRemarks\n\nWhen I open a shell in the pod, it's possible to call set there.\nWhen I call kubectl exec with other commands, for example ls, I get no error.\nThere are some other questions regarding kubectl exec. But these do not apply to my question, because my problem is about executing set.\n\n",
  "Answer": "set is not a binary but instead a shell command that sets the environment variable.\nIf you want to set an environment variable before executing a follow up command consider using env\nkubectl exec mypod -- env NAME=value123 script01\n\n# or \n\nkubectl exec mypod -- /bin/sh -c 'NAME=value123 script01'\n\nsee https://stackoverflow.com/a/55894599/93105 for more information\nif you want to set the environment variable for the lifetime of the pod then you probably want to set it in the yaml manifest of the pod itself before creating it.\nyou can also run set if you first run the shell\nkubectl exec mypod -- /bin/sh -c 'set'\n\n"
},
{
  "Question": "Is there a way to remove kubernetes cluster contexts from kubectx? Or can this only be done by manually removing them from kubeconfig?\n",
  "Answer": "There is a \"delete\" command for kubectx. You can see the kubectx  help with kubectx --help\nFor reference, the syntax is\nkubectx -d <NAME> [<NAME...>]\n\neg, kubectx -d followed by one or more Kube context names.\n"
},
{
  "Question": "How do I make the celery -A app worker command to consume only a single task and then exit.\nI want to run celery workers as a kubernetes Job that finishes after handling a single task.\nI'm using KEDA for autoscaling workers according to queue messages.\nI want to run celery workers as jobs for long running tasks, as suggested in the documentation:\nKEDA long running execution\n",
  "Answer": "There's not really anything specific for this. You would have to hack in your own driver program, probably via a custom concurrency module. Are you trying to use Keda ScaledJobs or something? You would just use a ScaledObject instead.\n"
},
{
  "Question": "On step 8 of Deploying the app to GKE in Running Django on Kubernetes Engine, it asks you to run this command:\nkubectl create secret generic cloudsql-oauth-credentials --from-file=credentials.json=[PATH_TO_CREDENTIAL_FILE]\n\nWhat is PATH_TO_CREDENTIAL_FILE supposed to be? I'm a bit lost here.\n",
  "Answer": "As it says in the previous line, it's the \"location of the key you downloaded when you created your service account\".\n"
},
{
  "Question": "I recently encountered an issue where something (which I was not able to identify) deleted a PVC and the corresponding PV in my k8s cluster. The data can be recovered but I have two questions:\n\nIs there some hack to prevent the PVC from being deleted accidentally if someone issues a wrong command which deletes it?\nIs it possible to check what command caused the deletion of the PVC via some logs?\n\n",
  "Answer": "For question 1, you can set the Reclaim Policy to Retain. This means that the PV and PVC can be deleted but the underlying storage volume will stick around forever (or until you delete it in whatever the underlying system is).\nFor 2, yes if you have audit logging turned on. https://kubernetes.io/docs/tasks/debug-application-cluster/audit/#audit-backends. Otherwise not really.\n"
},
{
  "Question": "Is there a way to specify the nodeSelector when using the Kubernetes run command? \nI don't have a yaml file and I only want to override the nodeSelector.\nI tried the following but didn't work:\nkubectl run myservice --image myserviceimage:latest --overrides='{ \"nodeSelector\": { \"beta.kubernetes.io/os\": \"windows\" } }'\n\n",
  "Answer": "nodeSelector must be wrapped with a spec. Like so\nkubectl run -ti --rm test --image=ubuntu:18.04 --overrides='{\"spec\": { \"nodeSelector\": {\"kubernetes.io/hostname\": \"eks-prod-4\"}}}'\n\n"
},
{
  "Question": "in the output of $ kubectl describe node ip-10-0-1-21\nI receive the following annotations:\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\n\ncan you please tell me the meaning of them and if there is a universal guide for all of these annotations - I could not find them by googling.\nis there any logic how these annotations are created?\n",
  "Answer": "node.alpha.kubernetes.io/ttl is a tuning parameter for how long the Kubelet can cache objects, only rarely used for extreme high-density or high-scale clusters. controller-managed-attach-detach is a feature flag from long ago, Kubernetes 1.3. It was originally used to enable or disable the attach-detach-controller for specific nodes. From the code it looks like it probably still works though that controller has been the default mode for years so we should probably remove it some day.\n"
},
{
  "Question": "Is there a recommended way to use Kubernetes Secrets? They can be exposed as environment variables or using a volume mount. Is one more secure than the other?\n",
  "Answer": "https://www.oreilly.com/library/view/velocity-conference-2017/9781491985335/video316233.html\nKubernetes secrets exposed by environment variables may be able to be enumerated on the host via /proc/. If this is the case it's probably safer to load them via volume mounts.\n"
},
{
  "Question": "Apologies if this question is asked before, am new to Kubernetes\nAm trying to access the k8s cluster through ingress-nginx as proxy running on my machine, through react app running on localhost\n\n\nAm getting NET::ERR_CERT_AUTHORITY_INVALID Error in my browser.\nI tried this but didn't worked.\nHow can I get around this?\nThank You.\n",
  "Answer": "If you don't install a real TLS certificate, you're just getting the default, self-signed one that the ingress controller includes as a fallback. Check out cert-manager for a path forward or just ignore the error for now (but probably don't ignore it, that's bad).\n"
},
{
  "Question": "I am trying to copy files from the pod to local using following command:\nkubectl cp /namespace/pod_name:/path/in/pod /path/in/local\n\nBut the command terminates with exit code 126 and copy doesn't take place.\nSimilarly while trying from local to pod using following command:\nkubectl cp /path/in/local /namespace/pod_name:/path/in/pod\n\nIt throws the following error:\nOCI runtime exec failed: exec failed: container_linux.go:367: starting container process caused: exec: \"tar\": executable file not found in $PATH: unknown\nPlease help through this.\n",
  "Answer": "kubectl cp is actually a very small wrapper around kubectl exec whatever tar c | tar x. A side effect of this is that you need a working tar executable in the target container, which you do not appear to have.\nIn general kubectl cp is best avoided, it's usually only good for weird debugging stuff.\n"
},
{
  "Question": "I have a nodejs pod running in kubernetes production environment. Additionally there is staging and review environment in the same cluster running the same app. I recently added --inspect to the start command in the dockerfile which gets deployed to all environments.\nMy question is, if I enable debugging in production as well, will it impact performance or memory usage? Is it a good practice in general? Otherwise I'll need to create a separate dockerfile for production.\n",
  "Answer": "\nwill it impact performance or memory usage?\n\nBoth probably negligiable if just having the flag enabled, mileage may vary if actually live debugging.\n\nIs it good practice\n\nI would say no, and it does have security implications. Although, this would only be a problem if you were to set a public IP, by default debugging would only be permitted on the localhost.\nMy.advice would be create a separate Dockerfile for prod.\n"
},
{
  "Question": "I have a Kubernetes cluster (installed on premise), and I deployed an application based on Websphere Liberty image (from docker hub).\nI configured a session affinity (or sticky session) for my service, then it can keep session via requests (access the same pod). But now, I want to keep application session when a node or pod died (for HA and using LB). Can I do that in Websphere liberty ? How to setup a Websphere liberty cluster\n",
  "Answer": "You can configure session persistence via hazelcast or via a traditional database running inside or outside of the cluster.    This frees the application from being sensitive to scaling up/down.\nhttps://openliberty.io/guides/sessions.html\nhttps://www.ibm.com/support/knowledgecenter/en/SSEQTP_liberty/com.ibm.websphere.wlp.doc/ae/twlp_admin_session_persistence.html\n"
},
{
  "Question": "Reading the Kubernetes official docs on Job parallel execution (e.g. one job, multiple pods with parallelism set to > 1), under the section, \"Parallel execution for Jobs / Parallel Jobs with a fixed completion count,\" the documentation states:\n\nnot implemented yet: Each Pod is passed a different index in the range 1 to .spec.completions.\n\nThis statement suggests that a future version of Kubernetes will be able to pass a unique counter to each pod in a job with parallelism > 1.\nSince this is in the official documentation as of (17 Mar 2021) I would like to know if there is an official timeline or expected release version for this feature. It would alleviate a lot of pain for me.\n",
  "Answer": "Yes, this feature will be released as alpha in 1.21 which is due in a few weeks. https://github.com/kubernetes/kubernetes/pull/98812 has details and links to the KEP.\n"
},
{
  "Question": "I've been doing a lot of digging on Kubernetes, and I'm liking what I see a lot! One thing I've been unable to get a clear idea about is what the exact distinctions are between the Deployment and StatefulSet resources and in which scenarios would you use each (or is one generally preferred over the other).\n",
  "Answer": "Deployments and ReplicationControllers are meant for stateless usage and are rather lightweight. StatefulSets are used when state has to be persisted. Therefore the latter use volumeClaimTemplates / claims on persistent volumes to ensure they can keep the state across component restarts.\nSo if your application is stateful or if you want to deploy stateful storage on top of Kubernetes use a StatefulSet.\nIf your application is stateless or if state can be built up from backend-systems during the start then use Deployments.\nFurther details about running stateful application can be found in 2016 kubernetes' blog entry about stateful applications\n"
},
{
  "Question": "Below is the report for liveness & readiness after running kubectl -n mynamespace describe pod pod1:\nLiveness:   http-get http://:8080/a/b/c/.well-known/heartbeat delay=3s timeout=3s period=10s #success=1 #failure=3\nReadiness:  http-get http://:8080/a/b/c/.well-known/heartbeat delay=3s timeout=3s period=10s #success=1 #failure=3\n\n\n\nIs this the valid(working) url? http://:80/\n\nWhat does #success=1 #failure=3 mean?\n\n\n",
  "Answer": "The results are completely right:\n\nhttp://:8080 indicates that it will try an http-get in port 8080 inside your pod\n#success=1 indicates a success threshold of 1 (the default), so the first time it gets an answer it will mark the pod as live or ready\n#failure=3 indicates a failure threshold of 3 (the default again), so the third time the call fails will mark it unready or try to restart it.\n\nSee the official docs: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/#configure-probes\nYou may try to execute this command to see how the probes are configured:\nkubectl -n mynamespace get pod pod1 -o yaml\n\n"
},
{
  "Question": "Team,\nI need to execute a shell script that is within a kubernetes pod. However the call needs to come from outside the pod. Below is the script for your reference:\necho 'Enter Namespace: '; read namespace; echo $namespace;\nkubectl exec -it `kubectl get po -n $namespace|grep -i podName|awk '{print $1}'` -n $namespace --- {scriptWhichNeedToExecute.sh}\nCan anyone suggest on how to do this?`\n",
  "Answer": "There isn't really a good way. A simple option might be cat script.sh | kubectl exec -i -- bash but that can have weird side effects. The more correct solution would be to use a debug container but that feature is still in alpha right now.\n"
},
{
  "Question": "I have a simple setup that is using OAuth2 Proxy to handle authentication. It works fine locally using minikube but when I try to use GKE when the oauth callback happens I get a 403 status and the the following message...\n\nLogin Failed: Unable to find a valid CSRF token. Please try again.\n\nThe offending url is http://ourdomain.co/oauth2/callback?code=J_6ao0AxSBRn4bwr&state=r_aFqM9wsSpPvyKyyzE_nagGnpNKUp1pLyZafOEO0go%3A%2Fip\nWhat should be configured differently to avoid the CSRF error?\n",
  "Answer": "In my case it was because I needed to set the cookie to secure = false. Apparently I could still have secure true no problem with http and an IP but once I uploaded with a domain it failed.\n"
},
{
  "Question": "I'm writing a program that can deploy to Kubernetes.\nThe main problem that I'm facing is \"Offline mode\" when I disconnect the computer from the router Kubernetes stops working because it needs the default route in the network interfaces.\nDoes anyone know how to set up Kubernetes so it will work without the default network interface?\nI tried Minikube and MicroK8S without success.\n",
  "Answer": "Few Kubernetes installers support air-gapped installation and doing it yourself is way out of scope for a new user. If this is for work, you'll want to talk to some of the major commercial distros (OpenShift I'm pretty sure has an air-gap installer, probably also Tanzu) but for new-user home use you should consider this not an option.\n"
},
{
  "Question": "Note: solution can use netcat or any other built-in Linux utility\nI need to implement an initContainer and liveness probe that confirms my redis pod is up for one of my redis dependent pods. I have attempted the netcat solution offered as the answer here ((printf \"PING\\r\\n\"; sleep 1) | nc 10.233.38.133 6379) but I get -NOAUTH Authentication required. error in response. Any way around this? I am aware I could install redis-cli or make a management command in my Django code but would prefer not to. Nor do I want to implement a web server for my Redis instance and use curl command.\n",
  "Answer": "You could always send in your AUTH command as part of your probe, like:\n`\"AUTH ....\\r\\nPING\\r\\n\"`\n\nUnless you're getting INFO from the server, you don't seem to care about the nature of the response, so no auth is required, just test for NOAUTH.\n"
},
{
  "Question": "I have 2 yaml files with configuration and certs and everything from 2 different hyperscaler to use to access kubernetes clusters in each of them, so I wonder if I can add to my actual .kube/config file both of them , on my mac I have kind clusters and also in a VM so everything is fine I see them configured on my config file (one cluster from KIDN and another running on my VM) but idk if the merging this yamls can break this file and then have to get the config files again.\nIn short, I don't want to use kubectl get ns -kubeconfig=configfile.yaml every single time to access a context for a cluster, instead to put them in my config file\nAny help will be very appreciated\n",
  "Answer": "export KUBECONFIG=/path/to/first/config:/path/to/second/config\"\nhttps://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/ has details.\n"
},
{
  "Question": "I read a bit about Deployment vs StatefulSet in Kubernetes. We usually need StatefulSet when we have a stateful app, so every pod can have its own volume.\nNow, I have a task to introduce persistence for RabbitMq. I will have only one pod replica of RabbitMq. Can I do it with Deployment? I don't see any problem with this. That one RabbitMq replica will have its own PersistentVolume(it will not share volume with other pods since I have only one replica). Also, I would say that if for any reason my RabbitMq pod gets restarted, it will continue to read and write from the same storage as before restart.\nAm I missing something?\n",
  "Answer": "Even with 1 replica, a statefulset still gets you some thing, like stable network ID. You are right that most features stop mattering but it's really up to your specific needs.\n"
},
{
  "Question": "I'm facing an issue with the deployement of my Node.js application on my Kubernetes container.\nThe container is stuck on Crashlooping with this error \"Back-off restarting failed container\" and as error code i have this \"Reason: Error - exit code: 243\"\nI did a describe of the pod i found nothing except the \"Back-off restarting failed container\" .\nIf someone could help that would be great thanks\n",
  "Answer": "I'm not sure why this worked, but it seems to be something with using npm run... to start the node service. I experimented with changing my Docker file to launch the container using:\nCMD npm run start\nTo just running the node command, using exactly what NPM should have been running, directly:\nCMD node ...\nEDIT:\nIn our environment it was an access problem. To get NPM working, we had to chown all the directories:\nCOPY --chown=${uid}:${gid} --from=builder /app .\n"
},
{
  "Question": "I need to deploy Grafana in a Kubernetes cluster in a way so that I can have multiple persistent volumes stay in sync - similar to what they did here.\nDoes anybody know how I can use the master/slave architecture so that only 1 pod writes while the others read? How would I keep them in sync? Do I need additional scripts to do that? Can I use Grafana's built-in sqlite3 database or do I have to set up a different one (Mysql, Postgres)?\nThere's really not a ton of documentation out there about how to deploy statefulset applications other than Mysql or MongoDB.\nAny guidance, experience, or even so much as a simple suggestion would be a huge help. Thanks!\n",
  "Answer": "\nStatefulSets are not what you think and have nothing to do with replication. They just handle the very basics of provisioning storage for each replica.\nThe way you do this is as you said by pointing Grafana at a \"real\" database rather than local Sqlite.\nOnce you do that, you use a Deployment because Grafana itself is then stateless like any other webapp.\n\n"
},
{
  "Question": "Is there a way to reference a secret value from a configmap?\nExample:\nCONFIGMAP: app.properties\ncontext-path=/test-app1\ndbhost=www.db123.com\nusername=user1\npassword=[getValueFromSecret]\n\nthe value of password here is saved in k8s secret\n",
  "Answer": "Not in core, but you can use the configmapsecrets operator for this. https://github.com/machinezone/configmapsecrets\nHelm also has accessors to do it client side.\n"
},
{
  "Question": "I have a SpringBoot application, dockerized, and deployed in a kubernetes cluster.\nThere is any way to log the pod name and pod ip from the springboot application inside the container?\nThanks in advance.\n",
  "Answer": "One approach is to run a Fluentd agent on each cluster node.  The agent collects all pod sysouts, decorates the logs with pod attributes and pipes them into ElasticSearch or some other searchable store.  ala kubernetes-fluentd\n"
},
{
  "Question": "I have built two services in k8s cluster, how can they interact with each other, if I want to make http request from one service to another, I know I can\u2019t use local host, but how can I know the host when I am coding.\n",
  "Answer": "Service objects are automatically exposed in DNS as <servicename>.<namespace>.svc.<clusterdomain> where clusterdomain is usually cluster.local. The default resolv.conf allows for relative lookups so if the service is in the same namespace you can use just the name, otherwise <servicename>.<namespace>.\n"
},
{
  "Question": "I have a kubernetes cluster\nIn Master node,\nIf I give the command kubectl get nodes it should show all the nodes.\nBut, If I give the same command in nodes it should not show the master node.\nIs it possible in kubernetes?\nPlease help anyone. Thanks in advance.\n",
  "Answer": "No, this is not possible. The kubernetes API will always respond to the same queries in the same way. kubectl get nodes is asking for information about all nodes, and the api will always answer an authorized user with all of the nodes. \nWith RBAC it is possible to limit what a particular user or account has access to view or edit, but the nodes resource is not namespaced, and does not give granularity to restrict access to certain nodes.\nYou can, however, filter the results of kubectl get nodes any way you like. This question has some good examples of showing only worker nodes using the -l argument to kubectl.\n"
},
{
  "Question": "I am currently using ubuntu machines for creating a kubernetes cluster.\nAll machines are on-prem.\nbut adding / upgrating machines, require lot of maintanence like installing ubuntu, adding needed packages, open-ssh, then adding kubernetes and adding to cluster.\nIs there a better way to install and add machines to kubernetes cluster.\n",
  "Answer": "There are many products and projects available for this. You'll just have to try some and see which you like. I couldn't list them all if I tried but a few I'm pretty sure are compatible with Ubuntu (in no particular order):\n\nkubespray\nRancher (and RKE with it)\nMicrok8s (uses Snaps)\nCharmed Kubernetes\n\n"
},
{
  "Question": "How can I replace the Image used in a Kubernetes Deployment manifest with jq?\nFor example:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: myapp\n  name: myapp-deployment\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:v1\n\nI tried using something like this jq '.spec.template.spec.containers[0].image = \"myapp:v2\"'. However, it always ends with a syntax or parse error.\n",
  "Answer": "Using yq, you can simply write:\nyq -y '.spec.template.spec.containers[0].image = \"foo:latest\"' pod.yml\n\nWhich produces:\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: myapp\n  name: myapp-deployment\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: myapp\n          image: foo:latest\n\nBut I would use kustomize for something like this, as @DavidMaze suggested.\n"
},
{
  "Question": "I am trying to terminate the namespace argo in Kubernetes. In the past, I have succesfully followed the directions found here Kubernetes Namespaces stuck in Terminating status\nthis time, however, I am getting the following error message. What does it mean and how can I work around this?\n{\n  \"kind\": \"Status\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n\n  },\n  \"status\": \"Failure\",\n  \"message\": \"namespaces \\\"argo\\\" is forbidden: User \\\"system:anonymous\\\" cannot update resource \\\"namespaces/finalize\\\" in API group \\\"\\\" in the namespace \\\"argo\\\"\",\n  \"reason\": \"Forbidden\",\n  \"details\": {\n    \"name\": \"argo\",\n    \"kind\": \"namespaces\"\n  },\n  \"code\": 403\n}\n\n",
  "Answer": "You need to use an authenticated user that has permissions for the subresource (or more often, for *).\n"
},
{
  "Question": "I want to deploy hyperkube in a Kubernetes pod. \nI already have a Kubernetes cluster. I tried few docker images in the docker hub. But all pods are failing with some issues. \nI am not able to deploy hyperkube image in a Kubernetes pod. \n",
  "Answer": "hyperkube is the binary to run k8s components on the nodes. It is not intended to run inside the k8s cluster.\nYou may want to start with busybox image:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  containers:\n  - image: busybox\n    command:\n      - sleep\n      - \"600\"\n    name: busybox\n\n"
},
{
  "Question": "I am trying to follow this tutorial to backup a persistent volume in Azure AKS:\nhttps://learn.microsoft.com/en-us/azure/aks/azure-disks-dynamic-pv\nI can see the volumes doing\naz aks get-credentials --resource-group MYRESOURCEGROUP --name  HIDDEN --subscription MYSUBSCRIPTION\n\nkubectl get pv\n\n(Both disk and file, managed-premium and standard storage classes)\nbut then I do:\naz disk list --resource-group MYRESOURCEGROUP  --subscription MYSUBSCRIPTION\n\nand I get an empty list, so I can't know the source full path to perform the snapshot.\nAm I missing something?\n",
  "Answer": "Upgrade your az cli version.\nI was getting this issue with az cli 2.0.75 returning an empty array for the disk list, with an AKS PV.\nupgraded to az cli 2.9.1 and same command worked.\n"
},
{
  "Question": "I am running the command\nkubectl create -f mypod.yaml --namespace=mynamespace\n\nas I need to specify the environment variables through a configMap I created and specified in the mypod.yaml file. Kubernetes returns\n\npod/mypod created\n\nbut kubectl get pods doesn't show it in my list of pods and I can't access it by name as if it does not exist. However, if I try to create it again, it says that the pod is already created.\nWhat may cause this, and how would I diagnose the problem?\n",
  "Answer": "By default, kubectl commands operate in the default namespace.  But you created your pod in the mynamespace namespace.\nTry one of the following:\nkubectl get pods -n mynamespace\nkubectl get pods --all-namespaces\n\n"
},
{
  "Question": "I have docker and kubernetes (enable kubernetes checked on docker settings) installed on my local macbook. I create containers using docker and after my machine restarts, these exactly same containers are still present. However, if I create containers within pods using kubernetes and machine is restarted, then I do see the containers but those are like freshly created containers and not the same containers prior to restart.\nWhat changes do I need to make so that even after machine restart my containers within the pods remain the same same before restart.\n",
  "Answer": "Even at runtime, Kubernetes may move Pods at will (e.g. during scaling, cluster upgrades, adding and removing nodes, etc.).\nIt is a good idea to try to treat containers as 'cattle, not pets', i.e. don't expect them to be long-lived.\nIf you need a container to be 'the same' after restart, consider using Persisent Volumes to store their state. Depending on your requirements, StatefulSets may also be worth considering.\nOr consider having them reload / recompute any additional data they need after startup. The mechanism for this will depend on your code.\n"
},
{
  "Question": "I found that we can create subcharts and conditionally include them as described here: Helm conditionally install subchart\nI have just one template that I want conditionally include in my chart but I could not find anything in the docs. Is there such feature?\n",
  "Answer": "I discovered that empty templates are not loaded. I solved it by wrapping my yaml file content in an if condition.\n{{ if .Values.something }}\ncontent of yaml file\n{{ end }}\n\n"
},
{
  "Question": "I have seen both serviceAccount and serviceAccountName been used in a pod manifest. What is the difference?\n",
  "Answer": "There is no difference.\nserviceAccount is DEPRECATED and you should use serviceAccountName instead.\nQuoting from  the Kubernetes API docs > pod spec:\n\nserviceAccount: DeprecatedServiceAccount is a deprecated alias for ServiceAccountName: Deprecated: Use serviceAccountName instead\n\n"
},
{
  "Question": "Is it possible to have array as environmental variable in deployment?\nkind: Deployment\nmetadata:\n  name: array-deployment\n  namespace: array-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: array-deployment\n  template:\n    metadata:\n      labels:\n        app: array-deployment\n    spec:\n      containers:\n      - name: array-deployment\n        image: array-deployment:beta\n        env:\n        - name: ENV_PROJECTS\n          value: \"project1\"\n        ports:\n        - containerPort: 80\n        resources: {}\n\nFor an example, I want to have an array of projects for ENV_PROJECT.\n",
  "Answer": "Environment variables are plain strings and do not support arrays as input\nIn order to achieve what you want, you would want to pass the values as a comma separated list. (You might want to use some other separator if your data contains ,)\nso your yaml manifest would become\n    - name: ENV_PROJECTS\n      value: \"project1,project2\"\n\nThis assumes that your code in the image array-deployment:beta supports reading comma separated values from the env var\n"
},
{
  "Question": "I have setup a kubernetes cluster of elasticsearch in GCP.\nkubectl get svc\n\ngives me\nNAME                                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\nipgram-elasticsearch-elasticsearch-svc   ClusterIP   10.27.247.26   <none>        9200/TCP,9300/TCP   2h\n\nHow to set an external IP for this ?\n",
  "Answer": "You have to convert the service to be of type LoadBalancer which will assign an external IP to the LB or NodePort and then use the nodes IP.\n"
},
{
  "Question": "I have a shell script my-script.sh like:\n#!/bin/bash\nwhile true; do\n  echo '1'\ndone\n\nI can deploy a bash pod in Kubernetes like:\nkubectl run my-shell --rm -it --image bash -- bash\n\nNow, I want to execute the script on bash. How can I pass my-script.sh as input to bash? Something like\nkubectl run my-shell --rm -it --image bash -- /bin/bash -c < my-script.sh\n\n",
  "Answer": "Just drop the -t to kubectl run (because you're reading from stdin, not a terminal) and the -c from bash (because you're passing the script on stdin, not as an argument):\n$ kubectl run my-shell --rm -i --image docker.io/bash -- bash < my-script.sh\nIf you don't see a command prompt, try pressing enter.\n1\n1\n1\n1\n...\n\n"
},
{
  "Question": "I use .kube/config to access Kubernetes api on a server. I am wondering does the token in config file ever get expired? How to prevent it from expire?\n",
  "Answer": "the solution is to use kubernetes service account\n"
},
{
  "Question": "I'm trying to install Jenkins X on an existing Kubernetes cluster (GKE), using jx boot, but it always gives me the error trying to execute 'jx boot' from a non requirements repo\nIn fact, I have tried to use jx install, and it works, but this command is already marked as deprecated, but I see it's still the method to use on Jenkins X's GitHub page.\nThen another detail ... I'm in fact creating the cluster using Terraform because I don't like the idea that Jenkins X creates the cluster for me. And I want to use Terraform to install Jenkins X as well but that would be another question. :)\nSo how to install using jx boot and what is a non requirements repo ?\nThanks\n",
  "Answer": "Are you trying to execute jx boot from within an existing git repository? Try changing into an empty, non-git directory run jx boot from there. \njx wants to clone the jenkins-x-boot-config and create your dev repository. It cannot do so from within an existing repository.\n"
},
{
  "Question": "In kubernetes pod yaml specification file, you can set a pod to use the host machine's network using hostNetwork:true.\nI can't find anywhere a good (suitable for a beginner) explanation of what the hostPID:true and hostIPC:true options mean. Please could someone explain this, assuming little knowledge in linux networking and such. Thanks.\nspec:\n  template:\n    metadata:\n      labels:\n        name: podName\n    spec:\n      hostPID: true\n      hostIPC: true\n      hostNetwork: true\n      containers:\n\nSource: github link here\n",
  "Answer": "they're roughly described within the Pod Security Policies\n\nhostPID - Use the host\u2019s pid namespace. Optional: Default to false.\nhostIPC - Use the host\u2019s ipc namespace. Optional: Default to false.\n\nThose are related to the SecurityContext of the Pod. You'll find some more information in the Pod Security design document.\n"
},
{
  "Question": "I have a devops pipeline divided in three steps:\n\nkubectl apply -f configmap.yml\nkubectl apply -f deployment.yml\nkubectl rollout restart deployment/test-service\n\nI think that when the configmap.yml changes the rollout restart step is useful. But when only the deployment.yml changes, I'm worried that the \"extra\" rollout restart step is not useful and should be avoided.\nShould I execute rollout restart only when the configmap.yml changes or should I don't care about?\n",
  "Answer": "This isn't a direct answer, but it ended up being too long for a comment and I think it's relevant. If you were to apply your manifests using kustomize (aka kubectl apply -k), then you get the following behavior:\n\nConfigMaps are generated with a content-based hash appended to their name\nKustomize substitutes the generated name into your Deployment\nThis means the Deployment is only modified when the content of the ConfigMap changes, causing an implicit re-deploy of the pods managed by the Deployment.\n\nThis largely gets you the behavior you want, but it would require some changes to your deployment pipeline.\n"
},
{
  "Question": "I have deployed a java application (backend) in openshift, and i have spun up 3 pods backend-1-abc, backend-1-xyz and backend-1-def. \nHow can I get the list of all the pod names for this service \"backend\"? Is it possible to obtain it programatically or is there any endpoint exposed in openshift to obtain this?\n",
  "Answer": "Are you saying you have actually created three separate Pod definitions with those names? Are you not using a DeploymentConfig or StatefulSet?\nIf you were using StatefulSet the names would be predictable.\nEither way, the Pods would usually be set up with labels and could use a command like:\noc get pods --selector app=myappname\n\nPerhaps have a read of:\n\nhttps://www.openshift.com/deploying-to-openshift/\n\nIt touches on labelling and querying based on labels.\nPlease provide more details about how you are creating the deployment if want more details/options.\n"
},
{
  "Question": "Is it possible to provide environment variables which will be set in all pods instead of configuring in each pods spec?\nIf not natively possible in Kubernetes, what would be an efficient method to accomplish it? We have Helm, but that still requires a lot of duplication.\nThis old answer suggested \"PodPreset\" which is no longer part of Kubernetes: Kubernetes - Shared environment variables for all Pods\n",
  "Answer": "You could do this using a mutating admission webhook to inject the environment variable into the pod manifest.\nThere are more details on implementing webhooks here.\n"
},
{
  "Question": "I refactored my k8s objects to use Kustomization, Components, replacements, patches and got to a good DRY state so that I don't repeat much between 2 apps and between those across dev and test environments. While doing so I am referring to objects outside of the folder (but same repository)\ncomponents:\n  - ../../common-nonprod\n  - ../../common-nonprod-ui\n\nMy question is will ArgoCD be able to detect a change in the app if any of the files inside the common folders change that this application refers to as components.\nIn other words does ArgoCD performs kustomize build to detect what changed ?\n",
  "Answer": "Yes. ArgoCD runs kustomize build to realize your manifests before trying to apply them to the cluster. ArgoCD doesn't care which files have changed; it simply cares that the manifests produced by kustomize build differ (or not) from what is currently deployed in the cluster.\n"
},
{
  "Question": "I created a Kubernetes cluster through Kops. The configuration and the ssh keys were in a machine that I don't have access to anymore. Is it possible to ssh to the nodes through kops even if I have lost the key? I see there is a command - \n\nkops get secrets\n\nThis gives me all the secrets. Can I use this to get ssh access to the nodes and how to do it?\nI see the cluster state is stored in S3. Does it store the secret key as well?\n",
  "Answer": "You can't recover the private key, but you should be able install a new public key following this procedure:\nkops delete secret --name <clustername> sshpublickey admin\nkops create secret --name <clustername> sshpublickey admin -i ~/.ssh/newkey.pub\nkops update cluster --yes to reconfigure the auto-scaling groups\nkops rolling-update cluster --name <clustername> --yes to immediately roll all the machines so they have the new key (optional)\n\nTaken from this document:\nhttps://github.com/kubernetes/kops/blob/master/docs/security.md#ssh-access\n"
},
{
  "Question": "I want to use the ClusterRole edit for some users of my Kubernetes cluster (https://kubernetes.io/docs/reference/access-authn-authz/rbac/#user-facing-roles).\nHowever, it is unfortunate that the user can be accessing and modifying Resource Quotas and Limit Ranges.\nMy question is now: How can I grant Users via a RoleBinding access to a namespace, such that the Role is essentially the CluserRole edit, but without having any access to Resource Quotas and Limit Ranges?\n",
  "Answer": "The edit role gives only read access to resourcequotas and limitranges:\n- apiGroups:\n  - \"\"\n  resources:\n  - bindings\n  - events\n  - limitranges\n  - namespaces/status\n  - pods/log\n  - pods/status\n  - replicationcontrollers/status\n  - resourcequotas\n  - resourcequotas/status\n  verbs:\n  - get\n  - list\n  - watch\n\nIf you want a role that doesn't include read access to these resources, just make a copy of the edit role with those resources excluded.\n"
},
{
  "Question": "When a POD is terminating,  how to get correct status Terminating using Kubernetes REST API. I am not able to figure it out.\nBut kubectl always report correct status,  and it also uses REST API to do that.\nWhat magic am I missing in REST API ? does it call two different API's and accumulate status ?\n",
  "Answer": "You are not the first person to ask this question. The answer appears to be that kubectl inspects metadata.deletionTimestamp; if this exists (and, presumably, has a non-null value) then the pod is in Terminating state.\nFor example, for a running pod:\n$ curl -s localhost:8001/api/v1/namespaces/mynamespace/pods/example | jq .metadata.deletionTimestamp\n<empty output>\n\nAnd then immediately after I kubectl delete the pod:\n$ curl -s localhost:8001/api/v1/namespaces/mynamespace/pods/example | jq .metadata.deletionTimestamp\n\"2022-01-15T15:30:01Z\"\n\n"
},
{
  "Question": "Can I a list all the namespaces using a role and not a clusterrole resource? If yes, can anyone help me with some example\n",
  "Answer": "Namespaces are cluster-wide objects.\nThe only way to access them is if there exists a ClusterRoleBinding and a corresponding ClusterRole that gives your principal (user, service account,etc) permissions to list namespaces.\n"
},
{
  "Question": "I am searching for a solution that enables me to set up a single node K8s cluster and if I needed I add nodes to it later.\nI am aware of solutions such as minikube and microk8s but they are not expandable. I am trying k3s at the moment exactly because it is offering this feature but I have some problems with storage and other stuff that I am working on them. \nNow my questions:\n\nWhat other solution for this exists?\nWhat are the disadvantages if I untaint the master node and run everything there (for a long period and not just for test)?\n\n",
  "Answer": "You can use kubeadm to setup a single node \"cluster\". Then you can use the join command to add more nodes\n"
},
{
  "Question": "Using crictl an containerd, is there an easy way to find to which pod/container belongs a given process, using it's PID` on the host machine?\nFor example, how can I retrieve the name of the pod which runs the process below (1747):\nroot@k8s-worker-node:/# ps -ef | grep mysql\n1000        1747    1723  0 08:58 ?        00:00:01 mysqld\n\n",
  "Answer": "Assuming that you're looking at the primary process in a pod, you could do something like this:\ncrictl ps -q | while read cid; do\n    if crictl inspect -o go-template --template '{{ .info.pid }}' $cid | grep -q $target_pid; then\n        echo $cid\n    fi\ndone\n\nThis walks through all the crictl managed pods and checks the pod pid against the value of the $target_pid value (which you have set beforehand to the host pid in which you are interested).\n"
},
{
  "Question": "I have an overlay kustomization.yaml as following:\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\n\nbases:\n    - ../../base/\n\npatches:\n    - patch.yaml\n\nsecretGenerator:\n    - name: my-secrets\n      env: password.env\n\nWhen applying it with embedded kustomize on kubectl like kubectl -k it works fine, but now I need to generate the final yaml before applying it so when I attempt to use itself through kustomize build devops/kustomize/my-cluster/overlay/local > local.yaml I'm getting this error:\nError: json: unknown field \"env\"\n\nsecretGenerator spec has env parameter so I'm not sure what am I doing wrong.\n",
  "Answer": "Turns out that newer versions of kustomize use envs parameter instead of env\n"
},
{
  "Question": "The field employeeData is of type json.\nConsider the query:\nselect * FROM \"employees\" where employeeData::text like '%someSampleHere%'\n\nWhen I'm running the query inside Postgres it works perfect and I get the rows that I'm asking for.\nHowever when I use it with KubeCTL and running this query outside of the PG App\npsql -d employess_db -U postgres -c 'select * FROM \"employees\" where employeeData::text like 'someSampleHere';'\n\nPG throws\nERROR:  syntax error at or near \"%\"\nLINE 1: select * FROM \"employees\" where employeeData::text like %someSampleHere%;\n\nHow can we fix it ?\n",
  "Answer": "Sounds like a quoting problem to me. You neglected to show your actual kubectl command line in your question, but this works for me without any errors:\nkubectl exec postgres-pod -- psql -U postgres -d employees_db \\\n  -c \"select * from \\\"employees\\\" where employeeData::text like '%someSampleHere%'\"\n\n"
},
{
  "Question": "I have worked with Jenkins X which is Jenkins running in Kubernetes cluster and I am seeing a new feature in the Google Cloud marketplace here, which is offering Jenkins, are these same?  \n",
  "Answer": "The Jenkins currently available in Google Cloud Marketplace is for the Jenkins Server.\nJenkins X is a new project, that is utilizing Kubernetes and ecosystem to provider a Kubernetes native equivalent to the Jenkins server, and provide horizontal scaling with no single point of failure and small footprint.\n"
},
{
  "Question": "I have a executed  command1 | command2  which runs from inside a container.\nI am trying to run the same command by passing it to the running container, but it doesn't work.\nI tried kubectl -n namespace exec -it pod -- 'command1 | command2'\nAny ideas? If pipes are not supported, any alternatives to run these 2 commands in sequence ?\n",
  "Answer": "The arguments to the kubectl exec command are executed directly, without a shell. Because you're passing not a single command but a shell expression, this isn't going to work.\nThe solution is to explicitly invoke the shell:\nkubectl -n namespace exec -it pod -- sh -c 'command1 | command2'\n\nFor example:\n$ kubectl exec -it fedora0 -- sh -c 'echo hello world | sed s/hello/goodbye/'\ngoodbye world\n\n"
},
{
  "Question": "I need to view logs a pod of specific name:\nBasically, I want to see logs for the pod having name infra in it.\nI'm using below command:\nkubectl logs $(kubectl get pods | awk '{print $1}' | grep -e \"infra\")\n\nBut, it's not working.\n",
  "Answer": "The stern command make this very simple:\nstern infra\n\nThat will stream the logs from any pods that have infra in the name.\n\nBut even without stern, you can do something like:\nkubectl logs $(kubectl get pods -o name | grep infra)\n\nThat will work as long as your grep command returns a single line. If the grep command results in multiple matches, you'll need to use a more specific pattern.\nIf you want to see logs from multiple pods with a single command, you can request logs by label using kubectl logs -l <label>. E.g., if I have several pods with the label app=my-app, I can run:\nkubectl logs -l app=my-app\n\n"
},
{
  "Question": "I'm looking for a way to restart all the pods of my service. They should restart one by one so the service is always available. The restart should happen when a Python script from a different service is done. \nI'm doing this because on the pods I want to restart there is a Gunicorn-server running which needs to reload some data. That only works when the server gets restarted.\nThe gunicorn service gets started in a Dockerfile:\nCMD gunicorn -c gunicorn.conf.py -b :$PORT --preload app:app\n\nBut I'm guessing this is not too relevant.\nI imagine the solution to be some kind of kubectl command that I can run in the Python script or a hint for a kubectl endpoint, that I couldn't find.\n",
  "Answer": "kubectl rollout restart has landed in Kubernetes v1.15 [1]. This feature is designed for exactly what you are looking to do - a rolling restart of pods.\n[1] https://github.com/kubernetes/kubernetes/issues/13488\n"
},
{
  "Question": "we have kubernetes deployment controller build using kubebuilder.\nin controller we are updating ConfigMap in controller and then want all pods backed by deployment to restart so latest ConfigMap will be reloaded.\nHow can I trigger this deployment/pods restart from controller.\nNote: the deployment/pod I want to restart is different form the one for which controller would have received event.\nwe are exploring the ways to do this\n",
  "Answer": "You can't \"restart\" a Pod, but if the Pod is managed by a Deployment you can remove the Pod and the Deployment will spawn a new one. If you update the Deployment (e.g., by changing the name of a ConfigMap referenced in the manifest), the Deployment will automatically respawn all associated pods.\nYour controller just needs to make the same API call you would normally use to delete a Pod or update a Deployment. This may require some RBAC configuration so that the ServiceAccount under which your controller is running has the necessary permissions.\n"
},
{
  "Question": "If a Helm deployment's status is failed, what can I do to determine what made it fail?\n",
  "Answer": "helm history <release_name>\nShows the kubernetes errors for the attempted deployment of that release.\n"
},
{
  "Question": "I've been using kubectl -vvvv ... a lot to learn about the different HTTP requests sent to the API server for different commands.\nHowever, I cannot seem to find a way of achieving the same with docker.\nIn particular, I've considered docker --debug ..., but e.g. docker --debug ps just displays the normal output.\nHow can I make docker output the HTTP requests sent to the daemon?\n",
  "Answer": "\nHow can I make docker output the HTTP requests sent to the daemon?\n\nYou can't, but you can set up a proxy server between the client and the docker daemon so that you can see the requests. The socat tool is useful for this. Set up a proxy by running:\nsocat -v unix-listen:/tmp/docker.sock,fork unix-connect:/var/run/docker.sock\n\nAnd then point docker at the proxy:\ndocker -H unix:///tmp/docker.sock ps\n\nAs you make requests with docker, you'll see the requests and replies in the output from the socat command.\n(You can set the DOCKER_HOST environment variable if you get tired of typing the -H ... command line option.)\n"
},
{
  "Question": "For default service account I have creating clusterrolebinding for cluster role=cluster-admin using below kubectl command\nkubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=rbac-test:default\n\ncluster-admin role is bind to default service account. \nHow to unbind it again from service account?\n",
  "Answer": "When you run your kubectl command it creates the following object:\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  creationTimestamp: null\n  name: add-on-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: default\n  namespace: rbac-test\n\nYou should be able to just delete that object:\nkubectl delete clusterrolebinding add-on-cluster-admin\n\n"
},
{
  "Question": "When creating an ingress resource in GCE using the Nginx ingress controller, the ingress resource is stuck on \"Creating ingress\". Any custom annotations appear to be lost, but I can access the URL defined by the ingress.\nWhat could be causing this?\n\n",
  "Answer": "This turned out to be because I was sending the annotation\nnginx.ingress.kubernetes.io/ssl-redirect: false\n\ninstead of \nnginx.ingress.kubernetes.io/ssl-redirect: \"false\"\n\nAccording to https://github.com/kubernetes/ingress-nginx/issues/1990, the Nginx controller only accepts strings containing \"true\" or \"false\". By sending boolean values, GCE was hanging.\nInterestingly there were no errors indicating a problem, and I could access the ingress URL, which made debugging the problem quote painful.\n"
},
{
  "Question": "I'm trying to mount an azureFile volume on a Windows K8S pod, but I get the error \n\nMountVolume.SetUp failed for volume \"azure-file-share\" : azureMount:\n  SmbGlobalMapping failed: fork/exec\n  C:\\Windows\\System32\\WindowsPowerShell\\v1.0\\powershell.exe: The\n  parameter is incorrect., only SMB mount is supported now, output: \"\"\n\nWhat is wrong?\n",
  "Answer": "The issue where was a bad azurestorageaccountkey value in the secret. You can have a secret like:\napiVersion: v1\nkind: Secret\nmetadata:\n  name: volume-azurefile-storage-secret\ntype: Opaque\ndata:\n  azurestorageaccountname: <base 64 encoded account name>\n  azurestorageaccountkey: <base 64 encoded account key>\n\nWhat was throwing me was that Azure already base 64 encodes the account key, and it was not clear if you need to double encode it for this secret file. \nThe answer is yes, you do double encode it. If you do not, you get the error from the question.\n"
},
{
  "Question": "I have a local minikube cluster on hyper-v, when i try to pull images from my private repository i get this error :\nFailed to pull image \"my-repolink/image\": rpc error: code = Unknown desc = Error response from daemon: Get my-repolink/v2/: x509: certificate signed by unknown authority\nWhen running  minikube docker-env  i get:\n$Env:DOCKER_TLS_VERIFY = \"1\"\n$Env:DOCKER_HOST = \"tcp://IP:2376\"\n$Env:DOCKER_CERT_PATH = \"C:\\Users\\myuser\\.minikube\\certs\"\n$Env:MINIKUBE_ACTIVE_DOCKERD = \"minikube\"\n\nI was wanderring if i can change the DOCKER_TLS_VERIFY to \"0\" (if yes how plz?) and if it will have any effect on this error?\n",
  "Answer": "You need to tell minikube which certificates to trust. The official doc mentions this specific issue.\nThe suggestion is to put the appropriate self-signed ceritificate of your private registry into ~/.minikube/files/etc/ssl/certs; then run minikube delete followed by minikube start.\n"
},
{
  "Question": "I want all members of security group sg-a to be able to access several ports, e.g. 6443 (kubernetes api server), on all instances in sg-a: including themselves. \nI create a rule in sg-a that says\n\nType: Custom TCP\nProtocol: TCP\nPort Range: 6443\nSource: sg-a \n\nHowever, instanceA cannot access port 6443 on itself. \nWhen I update \"Source\" to Source: instanceA.public.ip.address , then instanceA can access port 6443 on itself.\nHowever, I now have instance specific rules in my security group. If possible, I would like to find a solution where I do not have to add new rules when I add a new instance to my security group\n",
  "Answer": "For the security group to operate as you describe, the instances will need to connect to each other via a Private IP address.\nThe fact that it works if you allow the Public IP address indicates that the connection is being made by the public IP address.\n"
},
{
  "Question": "I have created an pub/sub topic to which I will publish a message every time an new object is uploaded to the bucket. Now I want to create a subscription to push a notification to an endpoint every time a new object is uploaded to that bucket. Following the documentation, I wanted something like that:\n\ngcloud alpha pubsub subscriptions create orderComplete \\\n        --topic projects/PROJECT-ID/topics/TOPIC \\\n        --push-endpoint http://localhost:5000/ENDPOINT/\n        --ack-deadline=60\n\nHowever my app is running on kubernetes and it seems that pub/sub cannot reach my endpoint. Any suggestions?\n",
  "Answer": "Yeah, so as @jakub-bujny points out you need a SSL endpoint. So one solution, on GKE, to use google's managed certificates with an Ingress resource (link shows you how)\n"
},
{
  "Question": "I have tried to list pods based on labels\n    // Kubernetes client - package kubernetes\n    clientset := kubernetes.NewForConfigOrDie(config)\n\n    // create a temp list for storage \n    var podslice []string\n\n    // Get pods -- package metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n    pods, _ := clientset.CoreV1().Pods(\"\").List(metav1.ListOptions{})\n    for _, p := range pods.Items {\n        fmt.Println(p.GetName())\n    }\n\nthis is equivalent of\nkubectl get po \n\nis there a way to get in golang\nkubectl get po -l app=foo\n\nthanks in advance\n",
  "Answer": "You may just be able to set using the ListOptions parameter.\nlistOptions := metav1.ListOptions{\n        LabelSelector: \"app=foo\",\n    }\npods, _ := clientset.CoreV1().Pods(\"\").List(listOptions)\n\nIf you have multiple labels, you may be able to perform this via the labels library, like below untested code:\nimport \"k8s.io/apimachinery/pkg/labels\"\n\nlabelSelector := metav1.LabelSelector{MatchLabels: map[string]string{\"app\": \"foo\"}}\nlistOptions := metav1.ListOptions{\n    LabelSelector: labels.Set(labelSelector.MatchLabels).String(),\n}\npods, _ := clientset.CoreV1().Pods(\"\").List(listOptions)\n\n"
},
{
  "Question": "I have create cron job in kubernetes and I have ssh key in one of  pod directory. when I am executing from command line its working fine, but when I am manually triggered , cron job is not recognizing .ssh folder .\nscp -i  /srv/batch/source/.ssh/id_rsa   user@server:/home/data/openings.csv  /srv/batch/source\n\n\n",
  "Answer": "When you log into a remote host from your container, the remote host key is  unknown to your SSH client inside the container\nusually, you're asked to confirm its fingerprint:\nThe authenticity of host ***** can't be established.\nRSA key fingerprint is *****.\nAre you sure you want to continue connecting (yes/no)?\n\nBut as there is no interactive shell, the SSH client fails.\nTwo solutions:\n\nadd the host key in the file ~/.ssh/known_hosts in the container\ndisable host key check (Dangerous as no remote host authentication is performed)\nssh -o \"StrictHostKeyChecking=no\" user@host\n\n"
},
{
  "Question": "I want to know How much volume is available to each of EC2 instances in an EKS cluster?\nAccording to this page, There are two types of AMIs:\n\nEBS-backed AMIs with 16 TiB available volume.\ninstance store-backed AMIs with 10 GiB available volume.\n\nWhich one of them does the workers' AMI belong?\nI create my EKS cluster using this terraform module:\nmodule \"eks\" {\n  ...\n  worker_groups = [\n    {\n      name                          = \"worker-group-1\"\n      instance_type                 = \"t2.small\"\n      asg_desired_capacity          = 2\n    }\n  ]\n}\n\n",
  "Answer": "You\u2019re using instances of type T2.small. This instance type is EBS-backed only and doesn\u2019t have an instance store option.\nAccording to the documentation that you mentioned, the size limit for an EBS-backed instance\u2019s root device is 16 TiB. The actual size however depends on the volume sizes that you configure for the instances (I\u2019m not sure but I think it defaults to 20 GiB). You can also add multiple EBS volumes to exceed the 16 TiB limit if needed.\n"
},
{
  "Question": "item.Status.ContainerStatuses.RestartCount doesn't exist. I cannot find the command. Reinstalling the nuget-package or updating it did not work either.\nDown there I added the problem I have and the package I use. Sorry if my english is kinda rusty.\n\n\n",
  "Answer": "ContainerStatuses is a collection of ContainerStatus, not a ContainerStatus itself. You must choose from which container you want the RestartCount, per example:\n int restarts = item.Status.ContainerStatuses[0].RestartCount;\n\n"
},
{
  "Question": "I am trying to understand how we can create circuit breakers for cloud run services,Unlike in GKE we are using istio kind of service mesh how we implement same thing cloud Run ?\n",
  "Answer": "On GKE you'd set up a circuit breaker to prevent overloading your legacy backend systems from a surge in requests.\nTo accomplish the same on Cloud Run or Cloud Functions, you can set a maximum number of instances. From that documentation:\n\nSpecifying maximum instances in Cloud Run allows you to limit the scaling of your service in response to incoming requests, although this maximum setting can be exceeded for a brief period due to circumstances such as traffic spikes. Use this setting as a way to control your costs or to limit the number of connections to a backing service, such as to a database.\n\n"
},
{
  "Question": "I just started to use Rancher and request to correct me for any wrong terminology.\nEarlier I was using minikube on Macbook which provide SSH easily using minikube ssh for troubleshooting. As I am newbie to Rancher Desktop and wanted to do SSH on Rancher Desktop node similar to minikube.\nI googled for same but unfortunately I didn't get any fruitful answer. Thanks in advance.\n",
  "Answer": "On recent versions (1.3 on) you can use the rdctl utility, which ships with Rancher Desktop, and run rdctl shell COMMAND or rdctl shell to ssh into the VM.\n"
},
{
  "Question": "eks server endpoint is xxxxxxxxxxx.xxx.eks.amazonaws.com and I've created a yml file with a deployment and service object.  \n[ec2-user@ip-]$ kubectl get svc\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\nfakeserver   NodePort    10.100.235.246   <none>        6311:30002/TCP   1h\nkubernetes   ClusterIP   10.100.0.1       <none>        443/TCP          1d\n\nWhen I browse xxxxxxxxxxx.xxx.eks.amazonaws.com:30002 returns too long to respond. security groups have all traffic in inbound rules. \n",
  "Answer": "You should be using your Worker Node's IP (one of the nodes if you have more than one), not the EKS server endpoint. The EKS server endpoint is the master plane, meant to process requests pertaining to creating/deleting pods, etc.\nYou also need to make sure that the Security Group of your Node's will allow the traffic.\nWith this in place you should be able to make the request to your NodePort service.\nFor Example:\nhttp://your-workernodeIp:NodePortNumber\n"
},
{
  "Question": "I deployed a EKS cluster and I'd like to add more IAM users to the role. I read this doc https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html and it mentioned how to map IAM users or roles to k8s but it doesn't say how to map IAM group. Is it not supported? or is there a way to do that? I don't want to map many users one by one. When a new user join the team, I just move them to the IAM group without changing anything in EKS.\n",
  "Answer": "You can't.  You can only map roles and users.  Directly from the documentation you linked:\n\n\nAdd your IAM users, roles, or AWS accounts to the configMap. You cannot add IAM groups to the configMap.\n\n\nThe easiest workaround would be to have a different IAM role for each group and only grant that group the ability to assume that role.\n"
},
{
  "Question": "I have an application with multiple services called from a primary application service.  I understand the basics of doing canary and A/B deployments, however all the examples I see show a round robin where each request switches between versions.  \nWhat I'd prefer is that once a given user/session is associated with a certain version it stays that way to avoid giving a confusing experience to the user.  \nHow can this be achieved with Kubernetes or Istio/Envoy?\n",
  "Answer": "You can do this with Istio using Request Routing - Route based on user identity but I don't know how mature the feature is. It may also be possible to route based on cookies or header values.\n"
},
{
  "Question": "I am getting below error in Jenkins while deploying to kubernetes cluster:\n\nERROR: ERROR: java.lang.RuntimeException: io.kubernetes.client.openapi.ApiException: java.net.UnknownHostException: **.azmk8s.io: Name or service not known\n  hudson.remoting.ProxyException: java.lang.RuntimeException: io.kubernetes.client.openapi.ApiException: java.net.UnknownHostException:\n\nTried to deploy with below jenkins pipeline snippet:\nkubernetesDeploy(\n            configs: 'deploymentFile.yaml',\n            kubeconfigId: 'Kubeconfig',\n            enableConfigSubstitution: true\n        )\n\nPlease suggest\n",
  "Answer": "Have you deployed AKS private cluster (document)? If so, jenkins needs to be in the private network to access k8s cluster. \nIf this is not private cluster, check the network setting of jenkins to see it is able to connect to internet. also check the DNS setting of the jenkins box as the error which you have shard is DNS error.\n"
},
{
  "Question": "I am using Azure Kubernetes. I installed Istio 1.6.1. It installed the Istio-ingressgateway with LoadBalancer. I don't want to use Istio ingressgateway because I want to kong ingress. \nI tried to run below command to change istio-ingress services from LoadBalancer to ClusterIP but getting errors.\n$ kubectl patch svc istio-ingressgateway -p '{\"spec\": {\"ports\": \"type\": \"ClusterIP\"}}' -n istio-system\n\nError from server (BadRequest): invalid character ':' after object key:value pair\n\nNot sure if I can make the changes and delete and re-create istio-ingress service?\n",
  "Answer": "The better option would be to reinstall istio without ingress controller. Do not install default profile in istio as it will install ingress controller along with other component. Check the various settings as mentioned in the installation page of istio and disable ingress controller.\nAlso check the documentation of using istio and kong together on k8s page and see what needs to be done on kong installation in order for enble communication between kong and other services.\n"
},
{
  "Question": "I'm reading through https://kubernetes.io/docs/reference/access-authn-authz/authentication/, but it is not giving any concrete commands and it is mostly focusing when we want to create everything from scratch. It's also explaining auth for engineers using Kubernetes.\nI have an existing deployment and service (with exposed external IP) and would like to create the simplest possible authentication (preferably token based) for an external user accessing the exposed IP. I can't add authentication to the services since I don't have access to their code. If somebody could help me with some commands I would be grateful.\n",
  "Answer": "The documentation which referred is for authentication with k8s (for api accesses). This is not for application layer authentication.\nHowever I can suggest one way to implement application layer authentication without changing the service at all. You can redirect the traffic to nginx (or any other reverse proxy) which can perform the authentication and redirect the authenticated user to service directly. It can also perform some kind of authorization too.\nThere are various resources available which can help you choose various authentication mechanism available in nginx such as password file based mechanism (link) or JWT based authentication (link)\n"
},
{
  "Question": "I have a Security-group Apple which is attached to my application EKS worker nodes i.e. EC2 instances.\nI have another Security-group Mango which is attached to the database EC2 instance, also EKS cluster.\nWhen I whitelist Apple Security-group in Mango, the applications in my EKS cannot access the db.\nBut when I explicitely whitelist the IP's of the worker nodes i.e. the EC2 instances, the applications can access the database.\nWhy does this work? Shouldn't whitelisting the attached Security-group solve my use case?\nPlease help.\n",
  "Answer": "When the rules of a Security Group refer to another Security Group, traffic will be permitted but only for the private IP address of instances.\nIf you are referring to the public IP address of the EC2 instance with the database, then the traffic goes out of the Internet Gateway and back 'into' the VPC. The source identity is therefore lost and the Security Group will not permit the inbound traffic.\nYou should communicate within the VPC by using private IP addresses.\n"
},
{
  "Question": "I am running python application on K8s cluster in Stateless mode.\nRight now we are using configmap & secret to store environment variables data. Using configmap and Secrets adding an environment variable to container os and application get it from os.\napp.config['test'] = os.environ.get(\"test\")\n\nTo use best practices we are planning to use vault from hashicrop. SO can i populate the config map ? Or direct add the values to container OS so that from os application can get it. No plan to use the volume to populate the variables as using stateless images.\nCan someone also please share a document or blog. Thanks in advance.\n",
  "Answer": "You can check Vault Agent with Kubernetes. You may have to do some Kubernetes distro specific steps as well. \n"
},
{
  "Question": "I am new to Kubernetes, I am looking to see if its possible to hook into the container execution life cycle events in the orchestration process so that I can call an API to pass the details of the container and see if its allowed to execute this container in the given environment, location etc.\nAn example check could be: container can only be run in a Europe or US data centers. so before someone tries to execute this container, outside this region data centers, it should not be allowed.\nIs this possible and what is the best way to achieve this?\n",
  "Answer": "You can possibly set up an ImagePolicy admission controller in the clusters, were you describes from what registers it is allowed to pull images.\nkube-image-bouncer is an example of an ImagePolicy admission controller\n\nA simple webhook endpoint server that can be used to validate the images being created inside of the kubernetes cluster.\n\n"
},
{
  "Question": "I've build a npm react-app that connects to a REST-backend using a given url.\nTo run the app on kubernetes, I've distributed the app and put it into an nginx container.\nThe app starts nicely, but I want to make the backend url configurable without having to rebuild the container image every time.\nI don't know how to do that or where to search for, any help would be appreciated\n",
  "Answer": "You have several methods to achieve your objective\n\nUse environment variables\n\n    apiVersion: v1\n    kind: Pod\n    metadata:\n     name: pod-name\n    spec:\n      containers:\n      - name: envar-demo-container\n        image: my_image:my_version\n        env:\n         - name: BACKEND_URL\n           value: \"http://my_backend_url\"\n\n\nUsing a configmap as a config file for your service\nIf the service is external, you can use a fixed name and register as a local kubernetes service: https://cloud.google.com/blog/products/gcp/kubernetes-best-practices-mapping-external-services\n\nRegards.\n"
},
{
  "Question": "Any example appsmith yaml for deploying appsmith since I don't want use helm in prod environment?\n",
  "Answer": "There's no official direct YAML files for deploying Appsmith on Kubernetes today. The Helm chart at helm.appsmith.com, and as documented at https://docs.appsmith.com/getting-started/setup/installation-guides/kubernetes, is the recommended way to install Appsmith on your cluster.\nAsking as an engineering team member with Appsmith, can you elaborate a little on why avoid Helm for production please?\n"
},
{
  "Question": "An operator I'm building needs to talk to other Kubernetes clusters, are there any best practices on how to do that from within the operator that runs on Kubernetes?\nUsing k8s.io/client-go/tools/clientcmd package I can call BuildConfigFromFlags method passing masterURL and kubeconfig location. This works fine from outside Kubernetes, but within Kubernetes, can any assumptions be made about kubeconfig location? Or should some other API be used?\nAs a side note: I'm using controller-runtime's Client API for talking to Kubernetes.\n",
  "Answer": "Turns out it's quite easy to do, just call the following with the master URL and the token to access it:\ncfg, err := clientcmd.BuildConfigFromFlags(os.Getenv(\"MASTERURL\"), os.Getenv(\"KUBECONFIG\"))\ncfg.BearerToken = os.Getenv(\"BEARERTOKEN\")\n\nIt might also require:\ncfg.Insecure = true\n\n"
},
{
  "Question": "I am wondering about Kubernetes's secret management. I have a process that generates a lot of secrets that only need to live for a short while.\nI would like for these secrets to come from Vault or a similar service in the future. However, for right now, I don't have the time to implement this. \nIf someone could provide me with the documentation or delineate the secret life cycle, it would be super helpful. Does Kubernetes have the means to garbage collect these secrets as it does with containers?\nLikewise, I am wondering if there is a way to set cascading deletes when this one resource disappears, so does its secrets?\n",
  "Answer": "Kubernetes has no notion of secret lifetime.\n\nyou can implement a CronJob in charge of checking then deleting secret in specific namespace(s) if the secret is older that a specific time.\nyou can create all your secrets in a temporary namespace, destroying the namespace will destroy all the secrets associated with this namespace.\nuse Vault\n\n"
},
{
  "Question": "While running this command k\n\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml\n\nI am getting this error\n\nError from server (NotFound): error when deleting\n  \"samples/bookinfo/networking/bookinfo-gateway.yaml\": the server could\n  not find the requested resource (delete gatewaies.networking.istio.io\n  bookinfo-gateway)\n\nCan someone please tell me how can i accept gatewaies plural ? or how to fix this error\n",
  "Answer": "Upgrading to latest kubectl solved the issue\n"
},
{
  "Question": "Using kubectl command line, is it possible to define the exact pod name?\nI have tried with\nkubectl run $pod-name --image imageX\n\nHowever, the resulted pod name is something like $pod-name-xx-yyy-nnn.\nSo without using a yaml file, can I define the pod name using kubectl CLI?\n",
  "Answer": "kubectl run creates a Deployment by default. A Deployment starts a ReplicaSet that manages the pods/replicas... and therefore has a generated pod name.\nRun pod\nTo run a single pod you can add --restart=Never to the kubectl run command.\nkubectl run mypod --restart=Never --image=imageX\n\n"
},
{
  "Question": "We are on Kubernetes 1.9.0 and wonder if there is way to access an \"ordinal index\" of a pod with in its statefulset configuration file. We like to dynamically assign a value (that's derived from the ordinal index) to the pod's label and later use it for setting pod affinity (or antiaffinity) under spec.\nAlternatively, is the pod's instance name available with in statefulset configfile? If so, we can hopefully extract ordinal index from it and dynamically assign to a label (for later use for affinity).\n",
  "Answer": "Right now the only option is to extract index from host name\nlifecycle:\n  postStart:\n    exec:\n      command: [\"/bin/sh\", \"-c\", \"export INDEX=${HOSTNAME##*-}\"]\n\n"
},
{
  "Question": "In my Kubernetes installation, I can see cAdvisor reports a measurement called \"container_cpu_load_average_10s\" for each pod/container. I get values such as 232, 6512 and so on.\nSo, what is the unit of measure for CPU load here? To me \"CPU Load\" and \"CPU Usage\" are used interchangeably, so I can't understand why its not a value between [0-100] ?\nUPDATE:\nHere I put the related line from cAdvisor log:\n...\ncontainer_cpu_load_average_10s{container_name=\"\",id=\"/system.slice/kubelet.service\",image=\"\",name=\"\",namespace=\"\",pod_name=\"\"} 1598\n...\n\n",
  "Answer": "It is the number of tasks. A very nice explanation can be found here: https://serverfault.com/questions/667078/high-cpu-utilization-but-low-load-average\n"
},
{
  "Question": "Can anybody let me know how to config \nkubernetes pods to send alert to  slack channel ?\nThanks in Advance\nRishabh Gupta\n",
  "Answer": "Kubernetes dosn't provide out of the box slack integration.\nThere are few projects that you can use:\n\nhttps://hub.kubeapps.com/charts/stable/kube-slack - runs on Kubernetes, watches for evnets and sends pod failures notifications to Slac\nhttps://hub.kubeapps.com/charts/stable/kubewatch - similar project. depending on configuration can be quiet noisy\n\nIf you need more complex motoring you can use Prometheus and it's alert manager https://prometheus.io/docs/alerting/notification_examples/ \n"
},
{
  "Question": "I have private Docker registry which works over https with self-signed SSL certificate. I've installed this certificate on local machine and it's working fine (can push and pull).\nIs it possible to configure Kubernetes to use this certificate for deployments (pull images from private registry)?\n",
  "Answer": "Kubernetes it self doesn't support this. you have to deploy certificate to all worker nodes. You can simplify the process using DaemonSet and hostPath  volumes.\n"
},
{
  "Question": "I am wondering if there are examples of full application stack based on Kubernetes, for ex: golang+solr+postgres with all the services and load balancers configured? And is it a good idea to have services like PostgreSQL and Solr on Kubernetes?\n",
  "Answer": "For databases, you can use SaaS, since it relives you of tasks like backup and management. Or if you really want to go all in on Kubernetes, for databases, you can go with operators. Operators manages most lifecycles for you.\nAs far as the other components are concerned, you will have to containerize them and then create rhe deployment artifacts. You can also use tools like Konveyor Move2kube (https://github.com/konveyor/move2kube) to accelerate the process.\n"
},
{
  "Question": "I am new to kubernetes. I have an issue in the pods. When I run the command\n kubectl get pods\n\nResult:\nNAME                   READY     STATUS             RESTARTS   AGE\nmysql-apim-db-1viwg    1/1       Running            1          20h\nmysql-govdb-qioee      1/1       Running            1          20h\nmysql-userdb-l8q8c     1/1       Running            0          20h\nwso2am-default-813fy   0/1       ImagePullBackOff   0          20h\n\nDue to an issue of \"wso2am-default-813fy\" node, I need to restart it. Any suggestion? \n",
  "Answer": "In case of not having the yaml file:\nkubectl get pod PODNAME -n NAMESPACE -o yaml | kubectl replace --force -f -\n"
},
{
  "Question": "The issue is that I would like to persistent one status file(status generated by the service), not the directory, of some service in case the status lost when service restart, how to solve?\n",
  "Answer": "If it's just a status file, you should be able to write it into a config map. See Add ConfigMap data to a Volume. If in volumes you have\nvolumes:\n  - name: status\n    configMap:\n      name: status\n      defaultMode: 420\n      optional: true\n\nand in volumeMounts\nvolumeMounts:\n  - name: status\n    mountPath: /var/service/status\n\nthen you should be able to write in it. See also how kube-dns does it with the kube-dns-config mount from kube-dns config-map.\n"
},
{
  "Question": "Using Spring Boot and managing database changes by Liquibase all changes are executed on application start. This is totally fine for fast running changes.\nSome changes, e.g. adding DB index, can run for a while. If running application on K8s it happens that liveness/readyness checks trigger an application restart. In this case Liquibase causes an end-less loop.\nIs there a pattern how to manage long running scripts with Liquibase? Any examples?\nOne approach might be splitting the the changes in two groups:\n\nExecute before application start.\nOr execute while application is already up and running.\n\n",
  "Answer": "Defer long running script after application start. Expose DB upgrade invocation via private REST API or similar.\nYour code and dev practice have to support n-1 version of DB schema. Eg: hide the feature that needs new column behind feature flag until schema upgrade fully rolled out.\n"
},
{
  "Question": "It's possible to perform an authorization(rule-based like) into Kubernetes ingress(like kong, nginx).\nFor example, i have this:\napiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: foo-bar\nspec:\n  rules:\n  - host: api.foo.bar\n    http:\n      paths:\n      - path: /service\n        backend:\n          serviceName: service.foo.bar\n          servicePort: 80\n\nBut before redirect to /service, I need to perform a call in my authorization api to valid if the request token has the rule to pass for /service.\nOr I really need to use an API gateway behind ingress like a spring zuul to do this?\n",
  "Answer": "Ingress manifest is just input for a controller. You also need an Ingress Controller, an proxy that understand the Ingress object. Kong and Nginx is two examples of implementation.\nNginx  Ingress Controller is provided from the Kubernetes community and it has an example of configuring an external oauth2 proxy using annotations\nannotations:\n  nginx.ingress.kubernetes.io/auth-url: \"https://$host/oauth2/auth\"\n  nginx.ingress.kubernetes.io/auth-signin: \"https://$host/oauth2/start?rd=$escaped_request_uri\"\n\n"
},
{
  "Question": "We have deployed a few pods in  cluster in various namespaces. I would like to inspect and identify all pod which is not in a Ready state.\n master $ k get pod/nginx1401 -n dev1401\n NAME        READY   STATUS    RESTARTS   AGE\n nginx1401   0/1     Running   0          10m\n\nIn above list, Pod are showing in Running status but having some issue. How can we find the list of those pods.  Below command not showing me the desired output:\n kubectl get po -A | grep Pending Looking for pods that have yet to schedule\n\n kubectl get po -A | grep -v Running Looking for pods in a state other than Running     \n\n kubectl get pods --field-selector=status.phase=Failed\n\n",
  "Answer": "There is a long-standing feature request for this. The latest entry suggests\nkubectl get po --all-namespaces | gawk 'match($3, /([0-9])+\\/([0-9])+/, a) {if (a[1] < a[2] && $4 != \"Completed\") print $0}'\n\nfor finding pods that are running but not complete.\nThere are a lot of other suggestions in the thread that might work as well.\n"
},
{
  "Question": "A default Google Kubernetes Engine (GKE) cluster \ngcloud container clusters create [CLUSTER_NAME] \\\n--zone [COMPUTE_ZONE]\n\nstarts with 3 nodes. What's the idea behind that? Shouldn't 2 nodes in the same zone be sufficient for high availability?\n",
  "Answer": "Kubernetes uses etcd for state. Etcd uses Raft for consensus to achieve high availability properties.\nWhen using a consensus protocol like Raft, you need majority in voting. Using 3 nodes you need 2 of 3 nodes to respond for availability. Using 2 nodes, you can not get majority with only 1 of 2 nodes, so you need both 2 nodes to be available.\n"
},
{
  "Question": "How to make load balancing for GRPC services on GKE on L7 (with Ingress over HTTP/2 + TLS)?\nI know that I have the option to use L4 (TCP layer) to configure Service with \"LoadBalancer\" type. But I want to know if I can use Ingress + L7 load balancing over HTTP/2+TLS.\nAlso I see \"HTTP/2 to backends is not supported for GKE.\" (on https://cloud.google.com/load-balancing/docs/backend-service#HTTP2-limitations).   But I don't know it's actual or not.\n",
  "Answer": "GKE Ingress can now load balance with HTTP/2, when you use https.\nTo get HTTP/2 between the load balancer (ingress controller) and your pods, your service need an extra annotation:\napiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    cloud.google.com/app-protocols: '{\"my-port\":\"HTTP2\"}'\n\nIn addition, your pods most use TLS and have ALPN h2 configured. This can be done e.g. with an HAProxy as a sidecar with http2 configuration. I have successfully used this setup on GKE.\n"
},
{
  "Question": "When I run this command\nkubectl get deployments\n\nat my Linux Ubuntu 18 machine, I got different output than expected (according to documentation).\nExpected:\n\nActual:\n\nOf course, I am not talking about values, I am talking about names of labels. \n[EDIT]\nMy k8s version:\n\n",
  "Answer": "This is just an old output format. The newer output you're getting below contains all the same information; the \"READY\" field is a combination of the old \"DESIRED\" and \"CURRENT\".\nIt's showing as 4/5 in your output to indicate 4 pods ready/current, and 5 pods desired.\nHope this helps.\n"
},
{
  "Question": "So, what I'm trying to do is use helm to install an application to my kubernetes cluster. Let's say the image tag is 1.0.0 in the chart.\nThen, as part of a CI/CD build pipeline, I'd like to update the image tag using kubectl, i.e. kubectl set image deployment/myapp... \nThe problem is if I subsequently make any change to the helm chart (e.g. number of replicas), and I helm upgrade myapp this will revert the image tag back to 1.0.0.\nI've tried passing in the --reuse-values flag to the helm upgrade command but that hasn't helped. \nAnyone have any ideas? Do I need to use helm to update the image tag? I'm trying to avoid this, as the chart is not available at this stage in the pipeline.\n",
  "Answer": "When using CI/CD to build and deploy, you should use a single source-of-truth, that means a file versioned in e.g. Git and you do all changes in that file. So if you use Helm charts, they should be stored in e.g. Git and all changes (e.g. new image) should be done in your Git repository.\nYou could have a build pipeline that in the end commit the new image to a Kubernetes config repository. Then a deployment pipeline is triggered that use Helm or Kustomize to apply your changes and possibly execute tests.\n"
},
{
  "Question": "I have multiple pods of the same app deployed using Kubernetes. The app manages multiple 'Project' objects. When Marry is working on 'Project 1' on pod-01, Tom logs on pod-02. Here is the requirement, if Tom tries to open 'Project 1' on pod-02, we need to route him to pod-01 where 'Project 1' is already open by Marry. How would I do that? \nCan I store some unique identifier of pod-01 in the 'Project 1' object? So I can use it to route Tom to pod-01.\nIs this technically feasible?\n",
  "Answer": "What you are describing is stateful workload, where each instance of your application contain state. \nNormal workload in Kubernetes is stateless and deployed with Deployment and ReplicaSet. However, Kubernetes now has some support for stateful workloads by using StatefulSet\nIt may be possible to implement your use case, but it depends. Your users will not have an instance for themself if that is what you need. I would recommend you to architect your service to be stateless workload, and store all state in a database (possibly with statefulSet) since it is much easier to handle stateless workloads.\n"
},
{
  "Question": "I'd like to access cluster deployed Helm charts programmatically to make web interface which will allow manual chart manipulation.\nI found pyhelm but it supports only Helm 2. I looked on npm, but nothing there. I wrote a bash script but if I try to use it's output I get just a string really so it's not really useful.\n",
  "Answer": "\nI'd like to access cluster deployed Helm charts programmatically to make web interface which will allow manual chart manipulation.\n\nHelm 3 is different than previous versions in that it is a client only tool, similar to e.g. Kustomize. This means that helm charts only exists on the client (and in chart repositories) but is then transformed to a kubernetes manifest during deployment. So only Kubernetes objects exists in the cluster.\nKubernetes API is a REST API so you can access and get Kubernetes objects using a http client. Kubernetes object manifests is available in JSON and Yaml formats.\n"
},
{
  "Question": "I want to access to the codes regarding Cgroup in Kubernetes GitHub repository. Where is the exact place?\n",
  "Answer": "The cgroups code is in the container engine selected, not in k8s. K8s take care of running containers and talks with the runtime using CRI. CRI is an API to let any container engine interact with kubelet. Kubelet is a piece of kubernetes that sits on every node and make sure that all pods are running as expected.\nTake a look at libcontainerd for docker as container engine and cgroups package for containerd\nRegards.\n"
},
{
  "Question": "I am working with a minecraft server image to create a cluster of statefulsets that all should have a random external port. I was told using a nodeport would do the job but not exactly how that is done. I was looking at nodeport but it looks like you would need to specify that exact port name. \nI need each replica in a cluster to either have a random external IP or a random external port on the same IP, Is that possible or do I need to create a service for every single port/IP.\n",
  "Answer": "You need to create a NodePort service for each instance of minecraft server.\nA NodePort open a random port up to < 30000 and links it to an internal (set of) based on the selectors.\nFor instance, let's say there is one instance of the minecraft server with the following resource:\napiVersion: v1\nkind: Pod\nmetadata:\n  name: minecraft-instance1\n  labels:\n    instance: minecraft-1\nspec:\n...\n\nThis is the nodePort description to reach it on port 30007 (on every node of the cluster):\napiVersion: v1\nkind: Service\nmetadata:\n  name: service-minecraft-1\nspec:\n  type: NodePort\n  selector:\n    instance: minecraft-1\n  ports:\n    - port: 25565\n      targetPort: 25565\n      nodePort: 30007\n\n"
},
{
  "Question": "I understand the concepts of a Kubernetes service ClusterIP and Headless service but when would I use one over the other?\n",
  "Answer": "The common case is to use ClusterIP for services within your cluster, unless you have a specific reason for another kind of Service.\n\nFor headless Services, a cluster IP is not allocated, kube-proxy does not handle these Services, and there is no load balancing or proxying done by the platform for them. How DNS is automatically configured depends on whether the Service has selectors defined\n\nA specific reason for a headless service may be when you use StatefulSet.\n"
},
{
  "Question": "Is it possible for an InitContainer to change the environment variables of the application container when running inside the same Pod?\nNote that I am looking for a detailed answer that describes the technical reasons why this is or isn't possible. Example: 'Current container technology supports environment variable isolation between containers and Pods cannot bypass that restriction by \"grouping\" containers in the same \"environment variable space\"'.\n",
  "Answer": "Short answer is No, they can't.\nYou can try some hack something using ShareProcessNamespace and gdb but for sure this is not correct solution for problem you are trying to solve.\n"
},
{
  "Question": "I'm currently working on my own custom operator that deploys a fully functional Wordpress. I'm required to implement SSL. Now this is where I'm stuck, I'm not really sure how to implement this using Go.\nIs there a way of adding already existing CRDs, for example cert-manage, into my operator and then create a Kubernetes resource type out of these, using my custom Operator?\n",
  "Answer": "Yes, every Go controller also has clients generated. See e.g. client-go cert-manager.\nIf you import the client-go for cert-manager, you can use it to e.g. create resources or watch for changes.\n"
},
{
  "Question": "Is there any way to setup cross communication with different namespace, say pods of namespace-a to communicate pods of namespace-b with each other in GKE cluster except for setting network policies?\n",
  "Answer": "Networking within a Kubernetes cluster can be done in different ways, but the recommended and most common way is to use DNS names. Pods get their own DNS names, but it is recommended that you access another app in the cluster via the DNS name for the Service.\nDNS names are hierarchical, starting with the Service name, and then the Namespace name.\n\nTo access another app in the same namespace, use <other-app-service-name>, e.g. http://<other-app-service-name>.\n\nTo send a request to an app in a different namespace, also use the namepspace part of the domain name, <another-app-service-name>.<other-namespace-name>, e.g. http://<another-app-service-name>.<other-namespace-name>\n\n\n"
},
{
  "Question": "I have the following deployment config. The test-worker-health and health endpoints are both unreachable as the application is failing due to an error. The startup probe keeps restarting the container after failing as restartPolicy: Always. The pods enter CrashLoopBackoff state. Is there a way to fail such startup probe?\nlivenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 30\nstartupProbe:\n          httpGet:\n            path: /test-worker-health\n            port: 8080\n          failureThreshold: 12\n          periodSeconds: 10\n\n",
  "Answer": "\nThe startup probe keeps restarting the container after failing\n\nThe startupProbe does not restart your container, but the livenessProbe does.\n\nThe pods enter CrashLoopBackoff state. Is there a way to fail such startup probe?\n\nIf you remove the livenessProbe, you will not get this restart-behavior. You may want to use a readinessProbe instead?\n\nIs there a way to fail such startup probe?\n\nWhat do you mean? It is already \"failing\" as you say. You want automatic rollback? That is provided by e.g. Canary Deployment, but is a more advanced topic.\n"
},
{
  "Question": "I use a kubernetes manifest file to deploy my code. My manifest typically has a number of things like Deployment, Service, Ingress, etc.. How can I perform a type of \"rollout\" or \"restart\" of everything that was applied with my manifest?\nI know I can update my deployment say by running\nkubectl rollout restart deployment <deployment name> \n\nbut what if I need to update all resources like ingress/service? Can it all be done together?\n",
  "Answer": "I would recommend you to store your manifests, e.g. Deployment, Service and Ingress in a directory, e.g. <your-directory>\nThen use kubectl apply to \"apply\" those files to Kubernetes, e.g.:\nkubectl apply -f <directory>/\n\nSee more on Declarative Management of Kubernetes Objects Using Configuration Files.\nWhen your Deployment is updated this way, your pods will be replaced with the new version during a rolling deployment (you can configure to use another deployment strategy).\n"
}
]