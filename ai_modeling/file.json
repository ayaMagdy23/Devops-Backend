[
  {
    "Question": "According to the documentation, Kubernetes reserves a significant amount of resources on the nodes in the cluster in order to run itself. Are the numbers in the documentation correct or is Google trying to sell me bigger nodes?\nAside: Taking kube-system pods and other reserved resources into account, am I right in saying it's better resource-wise to rent one machine equiped with 15GB of RAM instead of two with 7.5GB of RAM each?\n",
    "Answer": "Yes, kubernetes reserves a significant amount of resources on the nodes. So better consider that before renting the machine. \nYou can deploy custom machines in GCP. For the pricing you can use this calculator by Google\n"
  },
  {
    "Question": "Is it possible to mount a Persistent Volume Claim on the path of a volume mount. Here is an example of a deployment with said configuration:\n      volumes:\n        - name: tmp\n          emptyDir: {}\n        - name: pvc\n          persistentVolumeClaim:\n            claimName: pvc-claim-name\n\n          volumeMounts:\n            - name: tmp\n              mountPath: /tmp\n            - name: pvc\n              mountPath: /tmp/pvc\n\nWould the PVC be deleted when the mounted tmp volume is deleted with the container?\nWill the PVC remain after being deployed to within a mounted volume, which will be deleted with the container?\n",
    "Answer": "Yes, you can do that.\nWhen the POD (probably you was mentioning the pod, not the container) will be deleted, nothing will happen because the Deployment will re-deploy another pod to meet the replicas requirement.\nWhen the Deployment will be deleted the PVC will remain as well as your data will remain in your filesystem.\nTo remove the PVC, you need to specifically delete the it:\nkubectl delete pvc pvc-name\n\nbut your data persist into the filesystem after the PVC deletion.\nPlease, refer to this page for more information.\n"
  },
  {
    "Question": "My proxy settings (controlled by admin) doesn't allow me to pull busybox image. I want to test a few things on busybox. How can I manually download busybox image, so that I can push and use it?\nOr is there an alternative to busybox that I can download manually. I just want to run simple hello world program.\nEDIT: I know obvious solution is to reach out to admin which I already did and they declined my request so I just need an image with the shell which I can download outside the network and push on kubernetes instance.\n",
    "Answer": "You can manually push the image from your local machine to k8s nodes. This assumes that you've the SSH access to the nodes. Following are the steps\n\nOn your local machine, download the image.\n\nConvert the image into tar file using docker save <image-name> <image-name.tar>\n\nPush the image to remote server using scp <image-name.tar> username@host:/path/to/remote/server/directory\n\nFinally, SSH into the machine & load the image with docker load -i /path/to/remote/server/directory/image-name.tar\n\n\n"
  },
  {
    "Question": "I am pretty novice in Kubernetes.\nFrom the Kubernetes website, on 'host' field in ingress:\n\nAn optional host. In this example, no host is specified, so the rule\napplies to all inbound HTTP traffic through the IP address specified.\nIf a host is provided (for example, foo.bar.com), the rules apply to\nthat host.\n\nSo if I have host as 'x.y.z', path as '/my-service', then only a call like 'x.y.z/my-service/....' would be directed to the configured backend service.\nIf host is not set, then any call with /my-service in its url would be send to configured backend.\n\nx.y.z/my-service/..\na.b.c/my-service/..\n\nBoth would be directed to backend.\nIs my understanding correct?\n",
    "Answer": "Generally speaking, your understanding is correct.\nHost is a optional parameter that indicates to use a certain host. The usage of the host argument is also related to the DNS management of that address, so in order to use it you also need to either:\n\nadd the hostname into the DNS record\nedit the /etc/hosts file (if you're running your environment locally) with the association ip hostname\n\nAfter you set this up, you match any path that starts with http://host/path, e.g. http://host/path/1, http://host/path/2.\nYou don't match any path that either modify the host or the path prefix.\n"
  },
  {
    "Question": "I need to execute a diagnostic program written for my application. The application is running/deployed in a Kubernetes pod. I want to copy a diagnostic executable jar file inside the Kubernetes pod and execute it without stopping the pod. How can this be achieved ? Is there a specific way to run the executable jar using the Kubernetes pod environment in place ?\n",
    "Answer": "You can try the kubectl cp command:\nFrom the docs:\n\nCopy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in\nthe default namespace\n\nkubectl cp /tmp/foo_dir <some-pod>:/tmp/bar_dir\n\n\nCopy /tmp/foo local file to /tmp/bar in a remote pod in a specific\ncontainer\n\nkubectl cp /tmp/foo <some-pod>:/tmp/bar -c <specific-container>\n\n\nCopy /tmp/foo local file to /tmp/bar in a remote pod in namespace\n\nkubectl cp /tmp/foo <some-namespace>/<some-pod>:/tmp/bar\n\n\nCopy /tmp/foo from a remote pod to /tmp/bar locally\n\nkubectl cp <some-namespace>/<some-pod>:/tmp/foo /tmp/bar\n\nOnce the executable is added inside the container, you can exec into the pod and execute the file manually.\n"
  },
  {
    "Question": "Is there a flag/field in kubectl describe pods which tells if the container is running as root or non-root, I have few containers, I need to check if they are running as root\n",
    "Answer": "You can run whoami command inside the pod to see the name of the users. You can also check if runAsUser field is specified in the securityContext of pod's manifest. Containers by default run as root users.\nFor securityContext, check out examples in this link\n"
  },
  {
    "Question": "I have 3 services that are based on the same image, they're basically running the same app in 3 different configurations. 1 service is responsible for running migrations and data updates which the other 2 services will need. So I need this 1 service to be deployed first before the other 2 will be deployed. Is there any way to do this?\n",
    "Answer": "I assume that the two services which are dependent on service 1 are either Pod, Deployment or ReplicaSet. So, make use of Init Containers.\nHere is an example that will make a pod wait until the dependent services are up and running. busybox is most common image used for init containers that supports variety of linux commands that will help you check the status of dependent services.\n"
  },
  {
    "Question": "apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: deny-transactions-authorizationpolicy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: transactions\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/checking-account\"]\n    to:\n    - operation:\n       methods: [\"GET\"]\n       paths: [\"/*\"]\n\nit denies checking-account to access transaction workload.\nIf I change action to DENY. it allows checking-account to access transaction workload.\nCan someone help me why allow is denying and deny is allowing?\n",
    "Answer": "apiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: deny-transactions-authorizationpolicy\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: transactions\n  action: DENY\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/checking-account\"]\n  - to:\n    - operation:\n       methods: [\"GET\"]\n       paths: [\"/*\"]\n\nI needed to put - in front of to:\nThat fixed the issue.\n"
  },
  {
    "Question": "I am brand new to kubernetes. I am working on a Mac M1. Docker is already installed\nI followed the aws instructions here\nI got caught with this error:\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\nThis led me to instructions all over the internet that I need to copy /etc/kubernetes/admin.conf to ~/.kube/config\nThis is where it gets weird. I don't have a etc/kubernetes directory.\nQuestion: Does anyone know why and how to fix?\nIn addition, it turnes out that kubectl was already installed with homebrew. I uninstalled and reinstalled following aws instructions (above). This still brought no success\nAny help deeply appreciated\n",
    "Answer": "/etc/kubernetes directory is present inside the master nodes of the cluster. In case of bare-metal kubernetes , you can SSH into one of the nodes & copy the admin.conf file.\nFor cloud based k8s cluster like AWS's EKS, download the admin.conf file from the dashboard and place it ~/.kube/ folder. This AWS EKS Kubeconfig access may help you\n"
  },
  {
    "Question": "Currently using version 1.11.0 of Kuberenetes.\nClient Version: version.Info{Major:\"1\", Minor:\"11\", GitVersion:\"v1.11.0\", GitCommit:\"91e7b4fd31fcd3d5f436da26c980becec37ceefe\", GitTreeState:\"clean\", BuildDate:\"2018-06-27T20:17:28Z\", GoVersion:\"go1.10.2\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n\nBut Having Trouble changing the version of my Kuberentes to 1.10.5 in Ubuntu.\nAny Ideas how to change the my current version of Kubernetes?\n",
    "Answer": "This solves my problem apt-get install -y kubectl=1.10.5-00\n"
  },
  {
    "Question": "How to patch \"db.password\" in the following cm with kustomize?\ncomfigmap:\napiVersion: v1\ndata:\n  dbp.conf: |-\n    {\n      \"db_properties\": {\n        \"db.driver\": \"com.mysql.jdbc.Driver\",\n        \"db.password\": \"123456\",\n        \"db.user\": \"root\"\n      }\n    }\n\nkind: ConfigMap\nmetadata:\n  labels: {}\n  name: dbcm\n\n",
    "Answer": "create a placeholder in your file and replace it with real data while applying kustomize\nyour code will be like this:\n#!/bin/bash\nsed -i \"s/PLACE-HOLDER/123456/g\" db_config.yaml\nkustomize config.yaml >> kustomizeconfig.yaml\nkubectl apply -f kustomizeconfig.yaml -n foo\n\nAnd the db_config file will be:\napiVersion: v1\ndata:\n  dbp.conf: |-\n    {\n      \"db_properties\": {\n        \"db.driver\": \"com.mysql.jdbc.Driver\",\n        \"db.password\": \"PLACE_HODLER\",\n        \"db.user\": \"root\"\n      }\n    }\n\nkind: ConfigMap\nmetadata:\n  labels: {}\n  name: dbcm\n\nNB: This should be running on the pipeline to have the config file cloned from repo, so the real file won't be updated.\n"
  },
  {
    "Question": "I have a doubt, hope you would help.\nSuppose I have 4 node and created a deployment with ReplicaSet value is 3 with  pod spec having label xyz. Now creating one DaemonSet with pod having label xyz which is same mentioned above. Now finally how many pods will be there 4 or 3 and how does it work?\nPlease reply soon.\n",
    "Answer": "Total pods would be 7, 3 created by deployment, and 4 would be created by DamemonSet on each node.\nA DaemonSet deploys pods to all nodes in the cluster.\nAs per Kubernetes docs :-\nyou should not normally create any pods whose labels match this selector, either directly, with another ReplicationController, or with another controller such as Job. If you do so, the ReplicationController thinks that it created the other pods. Kubernetes does not stop you from doing this.\nIf you do end up with multiple controllers that have overlapping selectors, you will have to manage the deletion yourself\n"
  },
  {
    "Question": "in kubernetes V 1.14.5   with ingress controller nginx-ingress-controller:0.21.0-rancher3 .I have below issue:\nwhen userinfo api is called with internal service address (form inside cluster), it response fine (token is invalid). as below screenshot\n\nbut when that API is called with its ingress name, it responses with list of permission while token is revoked. both requests use same tokens\n\nI think its related to cache inside Ingress and Ingress Controller but I cant disable it vi annotation configuration\n",
    "Answer": "I've found the solution.\nthe root cause was related to wso2is  nginx mosuled, and cache was configured there. I change cache time in nginx configuration inside wso2is\n"
  }
]